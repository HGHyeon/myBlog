<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ObsidianBlog]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>ObsidianBlog</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sat, 15 Feb 2025 05:46:18 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sat, 15 Feb 2025 05:46:12 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[(1) λ¨Έμ‹ λ¬λ‹μ κ°λ…]]></title><description><![CDATA[ 
 <br>λ¨Έμ‹ λ¬λ‹ (Machine Learning)
μΌλ°μ μΌλ΅λ” μ• ν”λ¦¬μΌ€μ΄μ…μ„ μμ •ν•μ§€ μ•κ³ λ„ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ν¨ν„΄μ„ ν•™μµν•κ³  κ²°κ³Όλ¥Ό μμΈ΅ν•λ” μ•κ³ λ¦¬μ¦ κΈ°λ²•μ„ ν†µμΉ­<br>
κ·μΉ™μ„ μ‚¬λμ΄ μ§μ ‘ μ½”λ”©ν•μ§€ μ•κ³ , μ•κ³ λ¦¬μ¦μ΄ ν•™μµμ„ ν†µν•΄ μλ™μΌλ΅ μ°Ύμ•„λƒ„
<br>
<br>λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ€ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ν†µκ³„μ μΈ μ‹ λΆ°λ„λ¥Ό κ°•ν™”ν•κ³  μμΈ΅ μ¤λ¥λ¥Ό μµμ†ν™”ν•κΈ° μ„ν• λ‹¤μ–‘ν• μν•™μ  κΈ°λ²•μ„ μ μ©

<br>λ°μ΄ν„° λ‚΄μ ν¨ν„΄μ„ μ¤μ¤λ΅ μΈμ§€ν•κ³  μ‹ λΆ°λ„ μλ” μμΈ΅ κ²°κ³Όλ¥Ό λ„μ¶


<br><br><img alt="Pasted image 20250207005218.png" src="lib\media\pasted-image-20250207005218.png"><br>Note
μΌλ°μ μΌλ΅ λ¨Έμ‹ λ¬λ‹μ€ μ§€λ„ν•™μµ(Supervised Learning)κ³Ό λΉ„μ§€λ„ν•™μµ(Un-supervised Learning), κ°•ν™”ν•™μµ(Reinforcement Learning)μΌλ΅ λ‚λ‰¨
<br>
<br>μ§€λ„ν•™μµ (Supervised Learning)

<br>μ •λ‹µ(label)μ΄ μλ” λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ν•™μµν•λ©°, λ©ν‘λ” μ£Όμ–΄μ§„ μ…λ ¥μ— λ€ν• μ¶λ ¥μ„ μμΈ΅ν•λ” κ²ƒ
<br>λ¶„λ¥(Classification), νκ·€(Regression), μ¶”μ² μ‹μ¤ν…(Recommendation), μ‹κ°/μμ„± κ°μ§€/μΈμ§€(Computer Vision, Speech Recognition), ν…μ¤νΈ λ¶„μ„ λ° μμ—°μ–΄ μ²λ¦¬(NLP)


<br>λΉ„μ§€λ„ν•™μµ (Un-supervised Learning)

<br>μ •λ‹µ(label)μ΄ μ—†λ” λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ν¨ν„΄μ΄λ‚ κµ¬μ΅°λ¥Ό ν•™μµ
<br>ν΄λ¬μ¤ν„°λ§(Clustering), μ°¨μ› μ¶•μ†(Dimensionality Reduction), μ—°κ΄€ κ·μΉ™ ν•™μµ(Association Rule Learning), μ΄μƒ νƒμ§€(Anomaly Detection)


<br>κ°•ν™”ν•™μµ (Reinforcement Learning)

<br>ν–‰λ™μ— λ€ν• λ³΄μƒμ„ λ°›μΌλ©΄μ„ ν•™μµν•μ—¬, μ–΄λ–¤ ν™κ²½ μ•μ—μ„ μ„ νƒ κ°€λ¥ν• ν–‰λ™λ“¤ μ¤‘ λ³΄μƒμ„ μµλ€ν™”ν•λ” ν–‰λ™ λλ” ν–‰λ™ μμ„λ¥Ό μ„ νƒν•λ” λ°©λ²•
<br>κ²μ„ ν”λ μ΄(μ : μ²΄μ¤, λ°”λ‘‘), λ΅λ΄‡ μ μ–΄, μμ¨ μ£Όλ¦¬


<br><br>
λ¨Έμ‹ λ¬λ‹μ—μ„ λ°μ΄ν„°μ™€ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦ μ¤‘ μ–΄λ κ²ƒμ΄ λ” μ¤‘μ”ν• μ”μ†μΈκ°€?
<br>
<br>μ‚¬μ‹¤ λ°μ΄ν„°μ™€ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦ λ¨λ‘ λ¨Έμ‹ λ¬λ‹μ—μ„λ” μ¤‘μ”ν• μ”μ†

<br>ν•μ§€λ§ μΌλ‹¨ λ¨Έμ‹ λ¬λ‹ μ„Έμƒμ΄ λ³Έκ²©μ μΌλ΅ νΌμ³μ§„λ‹¤λ©΄ λ°μ΄ν„°μ μ¤‘μ”μ„±μ΄ λ¬΄μ—‡λ³΄λ‹¤ μ»¤μ§!


<br>λ¨Έμ‹ λ¬λ‹μ κ°€μ¥ ν° λ‹¨μ μ€ λ°μ΄ν„°μ— λ§¤μ° μμ΅΄μ 

<br>Garbage In, Garbage Out β†’ μΆ‹μ€ ν’μ§μ λ°μ΄ν„°λ¥Ό κ°–μ¶”μ§€ λ»ν•λ‹¤λ©΄ λ¨Έμ‹ λ¬λ‹μ μν–‰ κ²°κ³Όλ„ μΆ‹μ„ μ μ—†μ


<br>λ”°λΌμ„, μµμ μ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦κ³Ό λ¨λΈ νλΌλ―Έν„°λ¥Ό κµ¬μ¶•ν•λ” λ¥λ ¥λ„ μ¤‘μ”ν•μ§€λ§ λ°μ΄ν„°λ¥Ό μ΄ν•΄ν•κ³  ν¨μ¨μ μΌλ΅ κ°€κ³µ, μ²λ¦¬, μ¶”μ¶ν•΄ μµμ μ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ μ•κ³ λ¦¬μ¦μ„ κµ¬λ™ν•  μ μλ„λ΅ μ¤€λΉ„ν•λ” λ¥λ ¥μ΄ λ” μ¤‘μ”ν•  μ μμ!
<br><br>
<br>λ¨Έμ‹ λ¬λ‹ ν”„λ΅κ·Έλ¨μ„ μ‘μ„±ν•  μ μλ” λ€ν‘μ μΈ μ¤ν” μ†μ¤ ν”„λ΅κ·Έλ¨ μ–Έμ–΄λ” νμ΄μ¬κ³Ό R
<br>Rμ€ ν†µκ³„ μ „μ© ν”„λ΅κ·Έλ¨ μ–Έμ–΄μ΄κ³  νμ΄μ¬μ€ λ‹¤μ–‘ν• μμ—­μ—μ„ μ‚¬μ©λλ” κ°λ° μ „λ¬Έ ν”„λ΅κ·Έλ¨ μ–Έμ–΄
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\01.-νμ΄μ¬-κΈ°λ°μ-λ¨Έμ‹ λ¬λ‹κ³Ό-μƒνƒκ³„-μ΄ν•΄\(1)-λ¨Έμ‹ λ¬λ‹μ-κ°λ….html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/01. νμ΄μ¬ κΈ°λ°μ λ¨Έμ‹ λ¬λ‹κ³Ό μƒνƒκ³„ μ΄ν•΄/(1) λ¨Έμ‹ λ¬λ‹μ κ°λ….md</guid><pubDate>Thu, 13 Feb 2025 10:25:23 GMT</pubDate><enclosure url="lib\media\pasted-image-20250207005218.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250207005218.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(2) νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μƒνƒκ³„λ¥Ό κµ¬μ„±ν•λ” μ£Όμ” ν¨ν‚¤μ§€]]></title><description><![CDATA[ 
 ]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\01.-νμ΄μ¬-κΈ°λ°μ-λ¨Έμ‹ λ¬λ‹κ³Ό-μƒνƒκ³„-μ΄ν•΄\(2)-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μƒνƒκ³„λ¥Ό-κµ¬μ„±ν•λ”-μ£Όμ”-ν¨ν‚¤μ§€.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/01. νμ΄μ¬ κΈ°λ°μ λ¨Έμ‹ λ¬λ‹κ³Ό μƒνƒκ³„ μ΄ν•΄/(2) νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μƒνƒκ³„λ¥Ό κµ¬μ„±ν•λ” μ£Όμ” ν¨ν‚¤μ§€.md</guid><pubDate>Thu, 13 Feb 2025 09:56:59 GMT</pubDate></item><item><title><![CDATA[01. νμ΄μ¬ κΈ°λ°μ λ¨Έμ‹ λ¬λ‹κ³Ό μƒνƒκ³„ μ΄ν•΄]]></title><description><![CDATA[ 
 μ΄ μ„Ήμ…μ—μ„λ” λ¨Έμ‹ λ¬λ‹μ κ°λ…κ³Ό νμ΄μ¬ κΈ°λ° μƒνƒκ³„λ¥Ό λ‹¤λ£Ήλ‹λ‹¤.]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\01.-νμ΄μ¬-κΈ°λ°μ-λ¨Έμ‹ λ¬λ‹κ³Ό-μƒνƒκ³„-μ΄ν•΄\index.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/01. νμ΄μ¬ κΈ°λ°μ λ¨Έμ‹ λ¬λ‹κ³Ό μƒνƒκ³„ μ΄ν•΄/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[(1) μ‚¬μ΄ν‚·λ° μ†κ°μ™€ νΉμ§•]]></title><description><![CDATA[ 
 <br>μ‚¬μ΄ν‚·λ° (scikit-learn)
νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ λΌμ΄λΈλ¬λ¦¬

<br>λ¨Έμ‹ λ¬λ‹μ„ μ„ν• λ§¤μ° λ‹¤μ–‘ν• μ•κ³ λ¦¬μ¦κ³Ό κ°λ°μ„ μ„ν• νΈλ¦¬ν• ν”„λ μ„μ›ν¬μ™€ API μ κ³µ

]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(1)-μ‚¬μ΄ν‚·λ°-μ†κ°μ™€-νΉμ§•.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(1) μ‚¬μ΄ν‚·λ° μ†κ°μ™€ νΉμ§•.md</guid><pubDate>Thu, 13 Feb 2025 10:25:27 GMT</pubDate></item><item><title><![CDATA[(2) μ²« λ²μ§Έ λ¨Έμ‹ λ¬λ‹ λ§λ“¤μ–΄ λ³΄κΈ° - λ¶“κ½ƒ ν’μΆ… μμΈ΅ν•κΈ°]]></title><description><![CDATA[ 
 <br>scikit-learnμ„ ν†µν• μ²« λ²μ§Έ λ¨Έμ‹ λ¬λ‹ λ¨λΈ
λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈ -&gt; λ¶“κ½ƒμ ν’μΆ…μ„ λ¶„λ¥(Classification)ν•λ” κ²ƒ<br>
λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈ : κ½ƒμμ κΈΈμ΄μ™€ λ„λΉ„, κ½ƒλ°›μΉ¨μ κΈΈμ΄μ™€ λ„λΉ„ featureλ¥Ό κΈ°λ°μΌλ΅ κ½ƒμ ν’μΆ… μμΈ΅<br>
<img alt="Pasted image 20250207141842.png" src="lib\media\pasted-image-20250207141842.png">
<br>
<br>λ¶„λ¥(Classification)λ” λ€ν‘μ μΈ μ§€λ„ν•™μµ(Supervised Learning) λ°©λ²•
<br>μ§€λ„ν•™μµμ΄λ€?

<br>ν•™μµμ„ μ„ν• λ‹¤μ–‘ν• feature(λ°μ΄ν„°λ¥Ό μ„¤λ…ν•λ” μ…λ ¥λ³€μ)μ™€ λ¶„λ¥ κ²°μ •κ°’μΈ label(μ •λ‹µ) λ°μ΄ν„°λ΅ λ¨λΈμ„ ν•™μµν• λ’¤, λ³„λ„μ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈμ—μ„ λ―Έμ§€μ labelμ„ μμΈ΅ν•λ” λ°©λ²•
<br>μ¦‰, μ§€λ„ν•™μµμ€ λ…ν™•ν• μ •λ‹µμ΄ μ£Όμ–΄μ§„ λ°μ΄ν„°λ¥Ό λ¨Όμ € ν•™μµν• λ’¤ λ―Έμ§€μ μ •λ‹µμ„ μμΈ΅ν•λ” λ°©μ‹
<br>μ΄λ• ν•™μµμ„ μ„ν•΄ μ£Όμ–΄μ§„ λ°μ΄ν„° μ„ΈνΈ -&gt; ν•™μµ λ°μ΄ν„° μ„ΈνΈ, λ¨Έμ‹ λ¬λ‹ λ¨λΈμ μμΈ΅ μ„±λ¥μ„ ν‰κ°€ν•κΈ° μ„ν•΄ λ³„λ„λ΅ μ£Όμ–΄μ§„ λ°μ΄ν„° μ„ΈνΈ -&gt; ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ μ§€μΉ­<br>
<img alt="Pasted image 20250209201811.png" src="lib\media\pasted-image-20250209201811.png">


<br><br><br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
<br>
<br>μ‚¬μ΄ν‚·λ° ν¨ν‚¤μ§€ λ‚΄μ λ¨λ“λ…μ€ sklearnμΌλ΅ μ‹μ‘ν•λ” λ…λ…κ·μΉ™ μ΅΄μ¬

<br>sklearn.datasets : μ‚¬μ΄ν‚·λ°μ—μ„ μμ²΄μ μΌλ΅ μ κ³µν•λ” λ°μ΄ν„° μ„ΈνΈλ¥Ό μƒμ„±ν•λ” λ¨λ“μ λ¨μ„

<br>λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ¥Ό μƒμ„±ν•λ” λ°λ” load_iris()λ¥Ό μ΄μ©


<br>sklearn.tree : tree κΈ°λ° ML μ•κ³ λ¦¬μ¦μ„ κµ¬ν„ν• ν΄λμ¤μ λ¨μ„

<br>ML μ•κ³ λ¦¬μ¦μ€ μμ‚¬ κ²°μ • νΈλ¦¬(Decision Tree) μ•κ³ λ¦¬μ¦μΌλ΅, μ΄λ¥Ό κµ¬ν„ν• DecisionTreeClassifierλ¥Ό μ μ©


<br>sklearn.model_selection : ν•™μµ λ°μ΄ν„°μ™€ κ²€μ¦ λ°μ΄ν„°, μμΈ΅ λ°μ΄ν„°λ΅ λ°μ΄ν„°λ¥Ό λ¶„λ¦¬ν•κ±°λ‚ μµμ μ ν•μ΄νΌ νλΌλ―Έν„°λ΅ ν‰κ°€ν•κΈ° μ„ν• λ‹¤μ–‘ν• λ¨λ“μ λ¨μ„

<br>ν•μ΄νΌ νλΌλ―Έν„°(Hyperparameter) : λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦λ³„λ΅ μµμ μ ν•™μµμ„ μ„ν•΄ μ§μ ‘ μ…λ ¥ν•λ” νλΌλ―Έν„° (λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ μ„±λ¥μ„ νλ‹ν•  μ μμ)
<br>λ°μ΄ν„° μ„ΈνΈλ¥Ό ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ΅ λ¶„λ¦¬ν•λ” λ°λ” train_test_split()ν•¨μλ¥Ό μ‚¬μ©




<br><br><br>import pandas as pd

# λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ΅λ”©ν•©λ‹λ‹¤.
iris = load_iris()

# iris.dataλ” Iris λ°μ΄ν„° μ„ΈνΈμ—μ„ ν”Όμ²(feature)λ§μΌλ΅ λ λ°μ΄ν„°λ¥Ό numpyλ΅ κ°€μ§€κ³  μμµλ‹λ‹¤.
iris_data = iris.data

# iris.targetμ€ λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈμ—μ„ λ μ΄λΈ”(κ²°μ • κ°’) λ°μ΄ν„°λ¥Ό numpyλ΅ κ°€μ§€κ³  μμµλ‹λ‹¤.
iris_label = iris.target
print('iris targetκ°’:', iris_label)
print('iris targetλ…:', iris.target_names)

# λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ¥Ό μμ„Έν λ³΄κΈ° μ„ν•΄ DataFrameμΌλ΅ λ³€ν™ν•©λ‹λ‹¤.
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df.head(3)
<br><img alt="Pasted image 20250209172443.png" src="lib\media\pasted-image-20250209172443.png"><br>
<br>feature : sepal length, sepal width, petal length, petal width
<br>label : 0 (setosa ν’μΆ…), 1 (versicolort ν’μΆ…), 2 (virginica ν’μΆ…)
<br><br><br>X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label,
                                                    test_size=0.2, random_state=11)
<br>
<br>ν•™μµμ© λ°μ΄ν„°μ™€ ν…μ¤νΈμ© λ°μ΄ν„°λ” λ°λ“μ‹ λ¶„λ¦¬ν•΄μ•Ό ν•¨

<br>ν•™μµ λ°μ΄ν„°λ΅ ν•™μµλ λ¨λΈμ΄ μ–Όλ§λ‚ λ›°μ–΄λ‚ μ„±λ¥μ„ κ°€μ§€λ”μ§€ ν‰κ°€ν•λ ¤λ©΄ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈκ°€ ν•„μ”ν•κΈ° λ•λ¬Έ!
<br>μ΄λ¥Ό μ„ν•΄μ„ μ‚¬μ΄ν‚·λ°μ€ train_test_split() APIλ¥Ό μ κ³µ
<br>train_test_split() : μ‚¬μ΄ν‚·λ°μ—μ„ μ κ³µν•λ” λ°μ΄ν„° λ¶„ν•  ν•¨μ, train setμ™€ test setλ¥Ό λ‚λ λ• μ‚¬μ©

<br>test_size νλΌλ―Έν„° : μ…λ ¥κ°’μ λΉ„μ¨λ΅ μ‰½κ² λ¶„ν• 

<br>test_size = 0.2 : μ „μ²΄ λ°μ΄ν„° μ¤‘ ν…μ¤νΈ λ°μ΄ν„°κ°€ 20%, ν•™μµ λ°μ΄ν„°κ°€ 80%λ΅ λ°μ΄ν„° λ¶„ν• 




<br>μ²« λ²μ§Έ νλΌλ―Έν„°μΈ iris_dataλ” feature λ°μ΄ν„° μ„ΈνΈ, λ‘ λ²μ§Έ νλΌλ―Έν„°μΈ iris_labelμ€ label λ°μ΄ν„° μ„ΈνΈ


<br><br><br># DecisionTreeClassifier κ°μ²΄ μƒμ„±
dt_clf = DecisionTreeClassifier(random_state=11)

# ν•™μµ μν–‰
dt_clf.fit(X_train, y_train)
<br>
<br>λ¨Όμ € μ‚¬μ΄ν‚·λ°μ μμ‚¬ κ²°μ • νΈλ¦¬ ν΄λμ¤μΈ DecisionTreeClassifier λ¥Ό κ°μ²΄λ΅ μƒμ„±

<br>random_state=11 μ€ ν•­μƒ λ™μΌν• λ¨λΈμ΄ μƒμ„±λλ„λ΅ λ³΄μ¥ν•΄μ£Όλ” κ²ƒ


<br>μƒμ„±λ DecisionTreeClassifier κ°μ²΄μ fit() λ©”μ„λ“μ— ν•™μµμ© feature λ°μ΄ν„° μ†μ„±κ³Ό label κ°’ λ°μ΄ν„° μ„ΈνΈλ¥Ό μ…λ ¥ν•΄ νΈμ¶ν•λ©΄ ν•™μµμ„ μν–‰
<br># ν•™μµμ΄ μ™„λ£λ DecisionTreeClassifier κ°μ²΄μ—μ„ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ μμΈ΅ μν–‰.
pred = dt_clf.predict(X_test)
<br>
<br>μμΈ΅μ€ λ°λ“μ‹ ν•™μµ λ°μ΄ν„°κ°€ μ•„λ‹ λ‹¤λ¥Έ λ°μ΄ν„°λ¥Ό μ΄μ©ν•΄μ•Ό ν•λ©°, μΌλ°μ μΌλ΅ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό μ΄μ©
<br>DecisionTreeclassifier κ°μ²΄μ predict() λ©”μ„λ“μ— ν…μ¤νΈμ© feature λ°μ΄ν„° μ„ΈνΈλ¥Ό μ…λ ¥ν•΄ νΈμ¶ -&gt; ν•™μµλ λ¨λΈ κΈ°λ°μ—μ„ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈμ— λ€ν• μμΈ΅κ°’μ„ λ°ν™
<br><br><br>from sklearn.metrics import accuracy_score

print(f'μμΈ΅ μ •ν™•λ„: {accuracy_score(y_test, pred):.4f}')

&gt;&gt;&gt; μμΈ΅ μ •ν™•λ„: 0.9333
<br>
<br>μΌλ°μ μΌλ΅ λ¨Έμ‹ λ¬λ‹ λ¨λΈμ μ„±λ¥ ν‰κ°€ λ°©λ²•μ€ μ—¬λ¬ κ°€μ§€κ°€ μμΌλ‚, μ—¬κΈ°μ„λ” μ •ν™•λ„λ¥Ό μΈ΅μ •ν•΄λ΄„
<br>μ •ν™•λ„ (accuracy) : μμΈ΅ κ²°κ³Όκ°€ μ‹¤μ  label κ°’κ³Ό μ–Όλ§λ‚ μ •ν™•ν•κ² λ§λ”μ§€λ¥Ό ν‰κ°€ν•λ” μ§€ν‘<br>
- μ‚¬μ΄ν‚·λ°μ€ μ •ν™•λ„ μΈ΅μ •μ„ μ„ν•΄ accuracy_score() ν•¨μ μ κ³µ<br>
- μ²« λ²μ§Έ νλΌλ―Έν„°λ΅ μ‹¤μ  label λ°μ΄ν„° μ„ΈνΈ, λ‘ λ²μ§Έ νλΌλ―Έν„°λ΅ μμΈ΅ label λ°μ΄ν„° μ„ΈνΈ μ…λ ¥
λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ΅ λ¶„λ¥λ¥Ό μμΈ΅ν• process

<br>λ°μ΄ν„° μ„ΈνΈ λ¶„λ¦¬ : λ°μ΄ν„°λ¥Ό ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ΅ λ¶„λ¦¬
<br>λ¨λΈ ν•™μµ : ν•™μµ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ML μ•κ³ λ¦¬μ¦μ„ μ μ©ν•΄ λ¨λΈμ„ ν•™μµ
<br>μμΈ΅ μν–‰ : ν•™μµλ ML λ¨λΈμ„ μ΄μ©ν•΄ ν…μ¤νΈ λ°μ΄ν„°μ λ¶„λ¥ (μ¦‰, λ¶“κ½ƒ μΆ…λ¥)λ¥Ό μμΈ΅
<br>ν‰κ°€ : μμΈ΅λ κ²°κ΄κ°’κ³Ό ν…μ¤νΈ λ°μ΄ν„°μ μ‹¤μ  κ²°κ΄κ°’μ„ λΉ„κµν•΄ ML λ¨λΈ μ„±λ¥μ„ ν‰κ°€<br>
<img alt="Pasted image 20250209202115.png" src="lib\media\pasted-image-20250209202115.png">



]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(2)-μ²«-λ²μ§Έ-λ¨Έμ‹ λ¬λ‹-λ§λ“¤μ–΄-λ³΄κΈ°-λ¶“κ½ƒ-ν’μΆ…-μμΈ΅ν•κΈ°.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(2) μ²« λ²μ§Έ λ¨Έμ‹ λ¬λ‹ λ§λ“¤μ–΄ λ³΄κΈ° - λ¶“κ½ƒ ν’μΆ… μμΈ΅ν•κΈ°.md</guid><pubDate>Thu, 13 Feb 2025 10:25:31 GMT</pubDate><enclosure url="lib\media\pasted-image-20250207141842.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250207141842.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(3) μ‚¬μ΄ν‚·λ°μ κΈ°λ° ν”„λ μ„μ›ν¬ μµνκΈ°]]></title><description><![CDATA[ 
 <br><br>Note

<br>μ‚¬μ΄ν‚·λ°μ€ ML λ¨λΈ ν•™μµμ„ μ„ν•΄μ„ fit() , μμΈ΅μ„ μ„ν•΄ predict() λ©”μ„λ“λ¥Ό μ κ³µ
<br>μ§€λ„ν•™μµμ μ£Όμ” μ ν•μΈ λ¶„λ¥(Classification)μ™€ νκ·€(Regression)λ¥Ό μ„ν• λ‹¤μ–‘ν• μ•κ³ λ¦¬μ¦μ„ μ§€μ›
<br>λ¶„λ¥ μ•κ³ λ¦¬μ¦μ„ κµ¬ν„ν• ν΄λμ¤λ¥Όλ” Classifierλ΅, νκ·€ μ•κ³ λ¦¬μ¦μ„ κµ¬ν„ν• ν΄λμ¤λ” Regressor λ΅ μ§€μΉ­
<br>μ΄λ¬ν• Classifierμ™€ Regressor λ¥Ό ν¬ν•¨ν• λ¨λ“  λ¨λΈ ν΄λμ¤λ” Estimator ν΄λμ¤λ΅ ν†µμΉ­λ¨
<br>λ¨λ“  Estimatorλ” fit()μ„ ν†µν•΄ ν•™μµν•κ³ , predict()λ¥Ό ν†µν•΄ μμΈ΅μ„ μν–‰

<br>
<br>cross_val_score()μ™€ κ°™μ€ evaluation ν•¨μ, GridSearchCVμ™€ κ°™μ€ ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ μ§€μ›ν•λ” ν΄λμ¤μ κ²½μ° μ΄ Estimatorλ¥Ό μΈμλ΅ λ°›μ
<br>μΈμλ΅ λ°›μ€ Estimatorμ— λ€ν•΄μ„ cross_val_score(), GridSearchCV.fit() ν•¨μ λ‚΄μ—μ„ μ΄ Estimatorμ fit()κ³Ό predict()λ¥Ό νΈμ¶ν•΄μ„ ν‰κ°€λ¥Ό ν•κ±°λ‚ ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ μν–‰ν•λ” κ²ƒ<br>
<img alt="Pasted image 20250209212336.png" src="lib\media\pasted-image-20250209212336.png">
<br><br><br><img alt="Pasted image 20250209221724.png" src="lib\media\pasted-image-20250209221724.png"><br>
<img alt="Pasted image 20250209221754.png" src="lib\media\pasted-image-20250209221754.png"><br>
<img alt="Pasted image 20250209221813.png" src="lib\media\pasted-image-20250209221813.png"><br>Check
μΌλ°μ μΌλ΅ λ¨Έμ‹ λ¬λ‹ λ¨λΈμ„ κµ¬μ¶•ν•λ” μ£Όμ” ν”„λ΅μ„Έμ¤λ” featureμ κ°€κ³µ, λ³€κ²½, μ¶”μ¶μ„ μν–‰ν•λ” feature processing, ML μ•κ³ λ¦¬μ¦ ν•™μµ/μμΈ΅ μν–‰, κ·Έλ¦¬κ³  λ¨λΈ ν‰κ°€μ λ‹¨κ³„λ¥Ό λ°λ³µμ μΌλ΅ μν–‰ν•λ” κ²ƒ
<br><br><br>Note
μ‚¬μ΄ν‚·λ°μ—λ” λ³„λ„μ μ™Έλ¶€ μ›Ήμ‚¬μ΄νΈμ—μ„ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ‚΄λ ¤λ°›μ„ ν•„μ” μ—†μ΄ μμ λ΅ ν™μ©ν•  μ μλ” κ°„λ‹¨ν•λ©΄μ„λ„ μΆ‹μ€ λ°μ΄ν„° μ„ΈνΈκ°€ λ‚΄μ¥λΌ μμ
<br>
<br>μ‚¬μ΄ν‚·λ°μ— λ‚΄μ¥ λμ–΄ μλ” λ°μ΄ν„° μ„ΈνΈλ” λ¶„λ¥λ‚ νκ·€λ¥Ό μ—°μµν•κΈ° μ„ν• μμ  μ©λ„μ λ°μ΄ν„° μ„ΈνΈμ™€ λ¶„λ¥λ‚ ν΄λ¬μ¤ν„°λ§μ„ μ„ν•΄ ν‘λ³Έ λ°μ΄ν„°λ΅ μƒμ„±λ  μ μλ” λ°μ΄ν„° μ„ΈνΈλ΅ λ‚λ‰μ–΄ μ§
<br><br><br>
<br>fetch κ³„μ—΄μ λ…λ Ήμ€ λ°μ΄ν„°μ ν¬κΈ°κ°€ μ»¤μ„ ν¨ν‚¤μ§€μ— μ²μλ¶€ν„° μ €μ¥λΌ μμ§€ μ•κ³  μΈν„°λ„·μ—μ„ λ‚΄λ ¤λ°›μ•„ ν™ λ””λ ‰ν„°λ¦¬ μ•„λμ scikit_learn_dataλΌλ” μ„λΈ λ””λ ‰ν„°λ¦¬μ— μ €μ¥ν• ν›„ μ¶”ν›„ λ¶λ¬λ“¤μ΄λ” λ°μ΄ν„° (μµμ΄ μ‚¬μ© μ‹μ— μΈν„°λ„·μ— μ—°κ²°λΌ μμ§€ μ•μΌλ©΄ μ‚¬μ© λ¶κ°€)

<br>fetch_covtype() : νκ·€ λ¶„μ„μ© ν† μ§€ μ΅°μ‚¬ μλ£
<br>fetch_20newsgroups() : λ‰΄μ¤ κ·Έλ£Ή ν…μ¤νΈ μλ£
<br>fetch_olivetti_faces() : μ–Όκµ΄ μ΄λ―Έμ§€ μλ£
<br>fetch_lfw_people() : μ–Όκµ΄ μ΄λ―Έμ§€ μλ£
<br>fetch_lfw_pairs() : μ–Όκµ΄ μ΄λ―Έμ§€ μλ£
<br>fetch_rcv1() : λ΅μ΄ν„° λ‰΄μ¤ λ§λ­‰μΉ
<br>fetch_mldata() : ML μ›Ήμ‚¬μ΄νΈμ—μ„ λ‹¤μ΄λ΅λ“


<br><br><br><br>
<br>μ‚¬μ΄ν‚·λ°μ— λ‚΄μ¥λ μ΄ λ°μ΄ν„° μ„ΈνΈλ” μΌλ°μ μΌλ΅ λ”•μ…”λ„λ¦¬ ν•νƒλ΅ λμ–΄ μμ

<br>keyλ” λ³΄ν†µ data, target, target_name, feature_names, DESCRλ΅ κµ¬μ„±λΌ μμ
<br>data(ndarray) : featureμ λ°μ΄ν„° μ„ΈνΈ
<br>target(ndarray) : λ¶„λ¥ μ‹ label κ°’, νκ·€μΌ λ•λ” μ«μ κ²°κ΄κ°’ λ°μ΄ν„° μ„ΈνΈ
<br>target_names(ndarray or list) : κ°λ³„ labelμ μ΄λ¦„
<br>feature_names(ndarray or list) : featureμ μ΄λ¦„
<br>DESCR(string) : λ°μ΄ν„° μ„ΈνΈμ— λ€ν• μ„¤λ…κ³Ό κ° featureμ μ„¤λ…


<br><br>from sklearn.datasets import load_iris

iris_data = load_iris()

print(type(iris_data))
<br>
<br>load_iris()μ API λ°ν™ κ²°κ³Όλ” sklearn.utils.Bunchν΄λμ¤λ΅ νμ΄μ¬ λ”•μ…”λ„λ¦¬ μλ£ν•κ³Ό μ μ‚¬
<br>keys = iris_data.keys()
print('λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈμ ν‚¤λ“¤:', keys)

&gt;&gt;&gt; λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈμ ν‚¤λ“¤: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
<br>
<br>
λ‹¤μ κ·Έλ¦Όμ€ load_iris()κ°€ λ°ν™ν•λ” λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈμ κ° ν‚¤κ°€ μλ―Έν•λ” κ°’<br>
<img alt="Pasted image 20250210191359.png" src="lib\media\pasted-image-20250210191359.png">

<br>
feature_names, target_name, data, targetμ values ν™•μΈ

<br>print('\n feature_names μ type:',type(iris_data.feature_names))
print(' feature_names μ shape:',len(iris_data.feature_names))
print(iris_data.feature_names)

print('\n target_names μ type:',type(iris_data.target_names))
print(' feature_names μ shape:',len(iris_data.target_names))
print(iris_data.target_names)

print('\n data μ type:',type(iris_data.data))
print(' data μ shape:',iris_data.data.shape)
print(iris_data['data'])

print('\n target μ type:',type(iris_data.target))
print(' target μ shape:',iris_data.target.shape)
print(iris_data.target)
<br><img alt="Pasted image 20250210193511.png" src="lib\media\pasted-image-20250210193511.png">]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(3)-μ‚¬μ΄ν‚·λ°μ-κΈ°λ°-ν”„λ μ„μ›ν¬-μµνκΈ°.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(3) μ‚¬μ΄ν‚·λ°μ κΈ°λ° ν”„λ μ„μ›ν¬ μµνκΈ°.md</guid><pubDate>Thu, 13 Feb 2025 10:25:34 GMT</pubDate><enclosure url="lib\media\pasted-image-20250209212336.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250209212336.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(4) Model Selection λ¨λ“ μ†κ°]]></title><description><![CDATA[ 
 <br>Note
μ‚¬μ΄ν‚·λ°μ model_selection λ¨λ“μ€ ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ¶„λ¦¬ν•κ±°λ‚ κµμ°¨ κ²€μ¦ λ¶„ν•  λ° ν‰κ°€, κ·Έλ¦¬κ³  Estimatorμ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό νλ‹ν•κΈ° μ„ν• λ‹¤μ–‘ν• ν•¨μμ™€ ν΄λμ¤λ¥Ό μ κ³µ
<br><br>
ν•™μµ λ°μ΄ν„° μ„ΈνΈλ΅λ§ ν•™μµν•κ³  μμΈ΅ν•λ©΄ λ¬΄μ—‡μ΄ λ¬Έμ μΌκΉ?
<br>
<br>λ‹¤μ μμ λ” ν•™μµκ³Ό μμΈ΅μ„ λ™μΌν• λ°μ΄ν„° μ„ΈνΈλ΅ μν–‰ν• κ²°κ³Ό
<br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
dt_clf = DecisionTreeClassifier()
train_data = iris.data
train_label = iris.target
dt_clf.fit(train_data, train_label)

# ν•™μµ λ°μ΄ν„° μ…‹μΌλ΅ μμΈ΅ μν–‰
pred = dt_clf.predict(train_data)
print('μμΈ΅ μ •ν™•λ„:',accuracy_score(train_label,pred))

&gt;&gt;&gt; μμΈ΅ μ •ν™•λ„: 1.0
<br>
<br>μμΈ΅ μ •ν™•λ„κ°€ 1.0μ΄λΌλ” λ»μ€ μ •ν™•λ„κ°€ 100%
<br>μ¦‰, λ¬Έμ μ μ •λ‹µμ„ μ•κ³  μλ” μƒνƒμ—μ„ κ°™μ€ λ¬Έμ λ¥Ό ν…μ¤νΈ ν• κ²ƒ!
<br>λ”°λΌμ„, μμΈ΅μ„ μν–‰ν•λ” λ°μ΄ν„° μ„ΈνΈλ” ν•™μµμ„ μν–‰ν• ν•™μµμ© λ°μ΄ν„° μ„ΈνΈκ°€ μ•„λ‹ μ „μ©μ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈμ—¬μ•Ό ν•¨
<br>μ‚¬μ΄ν‚·λ°μ train_test_split()

<br>μ›λ³Έ λ°μ΄ν„° μ„ΈνΈμ—μ„ ν•™μµ λ° ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό μ‰½κ² λ¶„λ¦¬ κ°€λ¥
<br>train_test_split()λ” μ²« λ²μ§Έ νλΌλ―Έν„°λ΅ feature λ°μ΄ν„° μ„ΈνΈ, λ‘ λ²μ§Έ νλΌλ―Έν„°λ΅ label λ°μ΄ν„° μ„ΈνΈλ¥Ό μ…λ ¥λ°›κ³ , μ„ νƒμ μΌλ΅ λ‹¤μ νλΌλ―Έν„°λ¥Ό μ…λ ¥ λ°›μ

<br>test_size : μ „μ²΄ λ°μ΄ν„°μ—μ„ test λ°μ΄ν„° μ„ΈνΈ ν¬κΈ°λ¥Ό μ–Όλ§λ΅ μƒν”λ§ν•  κ²ƒμΈκ°€λ¥Ό κ²°μ • (default : 0.25, μ¦‰ 25%)
<br>train_size : μ „μ²΄ λ°μ΄ν„°μ—μ„ train λ°μ΄ν„° μ„ΈνΈ ν¬κΈ°λ¥Ό μ–Όλ§λ΅ μƒν”λ§ν•  κ²ƒμΈκ°€λ¥Ό κ²°μ • (test_size parameterλ¥Ό ν†µμƒμ μΌλ΅ μ‚¬μ©ν•κΈ° λ•λ¬Έμ— train_sizeλ” μ μ‚¬μ©λμ§€ μ•μ)
<br>shuffle : λ°μ΄ν„°λ¥Ό λ¶„λ¦¬ν•κΈ° μ „μ— λ°μ΄ν„°λ¥Ό λ―Έλ¦¬ μ„μ„μ§€λ¥Ό κ²°μ • (default : True), λ°μ΄ν„°λ¥Ό λ¶„μ‚°μ‹μΌμ„ μΆ€ λ” ν¨μ¨μ μΈ ν•™μµ λ° ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ§λ“λ” λ° μ‚¬μ©
<br>random_state : νΈμ¶ν•  λ•λ§λ‹¤ λ™μΌν• ν•™μµ/ν…μ¤νΈμ© λ°μ΄ν„° μ„ΈνΈλ¥Ό μƒμ„±ν•κΈ° μ„ν•΄ μ£Όμ–΄μ§€λ” λ‚μ κ°’ (train_test_split()λ” νΈμ¶ μ‹ λ¬΄μ‘μ„λ΅ λ°μ΄ν„°λ¥Ό λ¶„λ¦¬ν•λ―€λ΅ random_stateλ¥Ό μ§€μ •ν•μ§€ μ•μΌλ©΄ μν–‰ν•  λ•λ§λ‹¤ λ‹¤λ¥Έ ν•™μµ/ν…μ¤νΈ μ© λ°μ΄ν„°λ¥Ό μƒμ„±)
<br>train_test_split()μ λ°ν™κ°’μ€ tuple ν•νƒλ΅, μμ°¨μ μΌλ΅ train-feature, test-feature, train-label, test-label λ°μ΄ν„° μ„ΈνΈ λ°ν™



<br>
<br>λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ¥Ό train_test_split()μ„ μ΄μ©ν•΄ test λ°μ΄ν„° μ„ΈνΈλ¥Ό μ „μ²΄μ 30%, train λ°μ΄ν„° μ„ΈνΈλ¥Ό 70%λ΅ λ¶„λ¦¬
<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target,
                 test_size=0.3, random_state=121)
<br>
<br>train λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ DecisionTreeClassfierλ¥Ό ν•™μµν•κ³  μμΈ΅ μ •ν™•λ„ μΈ΅μ •
<br>dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('μμΈ΅ μ •ν™•λ„: {0:.4f}'.format(accuracy_score(y_test,pred)))

&gt;&gt;&gt; μμΈ΅ μ •ν™•λ„: 0.9556
<br>
λ¶“κ½ƒ λ°μ΄ν„°λ” 150κ°μ λ°μ΄ν„°λ΅ λ°μ΄ν„°μ μ–‘μ΄ ν¬μ§€ μ•μ•„ μ „μ²΄μ 30% μ •λ„μΈ ν…μ¤νΈ λ°μ΄ν„°λ” 45κ° μ •λ„λ°–μ— λμ§€ μ•μΌλ―€λ΅ μ•κ³ λ¦¬μ¦μ μμΈ΅ μ„±λ¥μ„ νλ‹¨ν•κΈ°μ—λ” κ·Έλ¦¬ μ μ ν•μ§€ μ•μ<br>
-&gt; ν•™μµμ„ μ„ν• λ°μ΄ν„°μ μ–‘μ„ μΌμ • μμ¤€ μ΄μƒμΌλ΅ λ³΄μ¥ν•λ” κ²ƒλ„ μ¤‘μ”ν•μ§€λ§, ν•™μµλ λ¨λΈμ— λ€ν•΄ λ‹¤μ–‘ν• λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ μμΈ΅ μ„±λ¥μ„ ν‰κ°€ν•΄ λ³΄λ” κ²ƒλ„ λ§¤μ° μ¤‘μ”!
<br><br><br>κ³Όμ ν•© (Overfitting)
λ¨λΈμ΄ ν•™μµ λ°μ΄ν„°μ—λ§ κ³Όλ„ν•κ² μµμ ν™”λμ–΄, μ‹¤μ  μμΈ΅μ„ λ‹¤λ¥Έ λ°μ΄ν„°λ΅ μν–‰ν•  κ²½μ°μ—λ” μμΈ΅ μ„±λ¥μ΄ κ³Όλ„ν•κ² λ–¨μ–΄μ§€λ” κ²ƒ
<br>
<br>κ³ μ •λ ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ΅ ν‰κ°€λ¥Ό ν•λ‹¤ λ³΄λ©΄ ν…μ¤νΈ λ°μ΄ν„°μ—λ§ μµμ μ μ„±λ¥μ„ λ°νν•  μ μλ„λ΅ νΈν–¥λκ² λ¨λΈμ„ μ λ„ν•λ” κ²½ν–¥μ΄ μƒκΈΈ μ μμ
<br>κ²°κµ­μ€ ν•΄λ‹Ή ν…μ¤νΈ λ°μ΄ν„°μ—λ§ κ³Όμ ν•©λλ” ν•™μµ λ¨λΈμ΄ λ§λ“¤μ–΄μ Έ λ‹¤λ¥Έ ν…μ¤νΈμ© λ°μ΄ν„°κ°€ λ“¤μ–΄μ¬ κ²½μ°μ—λ” μ„±λ¥μ΄ μ €ν•λ¨<br>
-&gt; μ΄λ¬ν• λ¬Έμ μ μ„ κ°μ„ ν•κΈ° μ„ν•΄ κµμ°¨ κ²€μ¦μ„ μ΄μ©ν•΄ λ” λ‹¤μ–‘ν• ν•™μµκ³Ό ν‰κ°€λ¥Ό μν–‰!
<br>κµμ°¨ κ²€μ¦ (Cross-Validation, CV)
μ£Όμ–΄μ§„ λ°μ΄ν„°λ¥Ό ν›λ ¨ λ°μ΄ν„°μ™€ κ²€μ¦ λ°μ΄ν„°λ΅ λ‚λ„μ–΄ λ¨λΈμ μΌλ°ν™” μ„±λ¥μ„ ν‰κ°€ν•λ” λ°©λ²•

<br>λ°μ΄ν„° νΈμ¤‘μ„ λ§‰κΈ° μ„ν•΄μ„ λ³„λ„μ μ—¬λ¬ μ„ΈνΈλ΅ κµ¬μ„±λ ν•™μµ λ°μ΄ν„° μ„ΈνΈμ™€ κ²€μ¦ λ°μ΄ν„° μ„ΈνΈμ—μ„ ν•™μµκ³Ό ν‰κ°€λ¥Ό μν–‰ν•λ” κ²ƒ
<br>κ° μ„ΈνΈμ—μ„ μν–‰ν• ν‰κ°€ κ²°κ³Όμ— λ”°λΌ ν•μ΄νΌ νλΌλ―Έν„° νλ‹ λ“±μ λ¨λΈ μµμ ν™”λ¥Ό λ”μ± μ†μ‰½κ² ν•  μ μμ
<br>λ€λ¶€λ¶„μ ML λ¨λΈμ μ„±λ¥ ν‰κ°€λ” κµμ°¨ κ²€μ¦ κΈ°λ°μΌλ΅ 1μ°¨ ν‰κ°€λ¥Ό ν• λ’¤μ— μµμΆ…μ μΌλ΅ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈμ— μ μ©ν•΄ ν‰κ°€ν•λ” ν”„λ΅μ„Έμ¤<br>
<img alt="Pasted image 20250210210414.png" src="lib\media\pasted-image-20250210210414.png">

<br><br>k ν΄λ“ κµμ°¨ κ²€μ¦ (K-Fold Cross-Validation)
kκ°μ λ°μ΄ν„° fold(μ΅°κ°) μ„ΈνΈλ¥Ό λ§λ“¤μ–΄μ„ kλ²λ§νΌ κ° fold μ„ΈνΈμ— ν•™μµκ³Ό κ²€μ¦ ν‰κ°€λ¥Ό λ°λ³µμ μΌλ΅ μν–‰ν•λ” λ°©λ²•
<br>
<br>
λ‹¤μ κ·Έλ¦Όμ€ 5 fold κµμ°¨ κ²€μ¦ μν–‰ (k=5)<br>
<img alt="Pasted image 20250210211232.png" src="lib\media\pasted-image-20250210211232.png">

<br>5κ°μ foldλ λ°μ΄ν„° μ„ΈνΈλ¥Ό ν•™μµκ³Ό κ²€μ¦μ„ μ„ν• λ°μ΄ν„° μ„ΈνΈλ΅ λ³€κ²½ν•λ©΄μ„ 5λ² ν‰κ°€λ¥Ό μν–‰ν• λ’¤, μ΄ 5κ°μ ν‰κ°€λ¥Ό ν‰κ· ν• κ²°κ³Όλ¥Ό κ°€μ§€κ³  μμΈ΅ μ„±λ¥μ„ ν‰κ°€
<br>μ΄λ ‡κ² ν•™μµ λ°μ΄ν„° μ„ΈνΈμ™€ κ²€μ¦ λ°μ΄ν„° μ„ΈνΈλ¥Ό μ μ§„μ μΌλ΅ λ³€κ²½ν•λ©΄μ„ λ§μ§€λ§‰ 5λ²μ§Έ(kλ²μ§Έ)κΉμ§€ ν•™μµκ³Ό κ²€μ¦μ„ μν–‰ν•λ” κ²ƒμ΄ λ°”λ΅ k fold κµμ°¨ κ²€μ¦


<br>
μ‚¬μ΄ν‚·λ°μ—μ„λ” k fold κµμ°¨ κ²€μ¦ ν”„λ΅μ„Έμ¤λ¥Ό κµ¬ν„ν•κΈ° μ„ν•΄ KFoldμ™€ StratifiedKFoldλ¥Ό μ κ³µ

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5κ°μ ν΄λ“ μ„ΈνΈλ΅ λ¶„λ¦¬ν•λ” KFold κ°μ²΄μ™€ ν΄λ“ μ„ΈνΈλ³„ μ •ν™•λ„λ¥Ό λ‹΄μ„ λ¦¬μ¤νΈ κ°μ²΄ μƒμ„±.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈ ν¬κΈ°:',features.shape[0])

&gt;&gt;&gt; λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈ ν¬κΈ°: 150
<br>
<br>KFold(n_splits=5)λ΅ KFold κ°μ²΄λ¥Ό μƒμ„±ν–μΌλ‹, μ΄μ  μƒμ„±λ KFold κ°μ²΄μ split()μ„ νΈμ¶ν•΄ μ „μ²΄ λ¶“κ½ƒ λ°μ΄ν„°λ¥Ό 5κ°μ fold λ°μ΄ν„° μ„ΈνΈλ΅ λ¶„λ¦¬
<br>n_iter = 0

# KFoldκ°μ²΄μ split( ) νΈμ¶ν•λ©΄ ν΄λ“ λ³„ ν•™μµμ©, κ²€μ¦μ© ν…μ¤νΈμ λ΅μ° μΈλ±μ¤λ¥Ό arrayλ΅ λ°ν™  
for train_index, test_index  in kfold.split(features):
    # kfold.split( )μΌλ΅ λ°ν™λ μΈλ±μ¤λ¥Ό μ΄μ©ν•μ—¬ ν•™μµμ©, κ²€μ¦μ© ν…μ¤νΈ λ°μ΄ν„° μ¶”μ¶
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    #ν•™μµ λ° μμΈ΅
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1

    # λ°λ³µ μ‹ λ§λ‹¤ μ •ν™•λ„ μΈ΅μ •
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]

    print('\n#{0} κµμ°¨ κ²€μ¦ μ •ν™•λ„ :{1}, ν•™μµ λ°μ΄ν„° ν¬κΈ°: {2}, κ²€μ¦ λ°μ΄ν„° ν¬κΈ°: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} κ²€μ¦ μ„ΈνΈ μΈλ±μ¤:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)

# κ°λ³„ iterationλ³„ μ •ν™•λ„λ¥Ό ν•©ν•μ—¬ ν‰κ·  μ •ν™•λ„ κ³„μ‚°
print('\n## ν‰κ·  κ²€μ¦ μ •ν™•λ„:', np.mean(cv_accuracy))
<br><img alt="Pasted image 20250211162001.png" src="lib\media\pasted-image-20250211162001.png"><br>
<br>5λ² κµμ°¨ κ²€μ¦ κ²°κ³Ό ν‰κ·  κ²€μ¦ μ •ν™•λ„λ” 0.9μ΄κ³ , κµμ°¨ κ²€μ¦ μ‹λ§λ‹¤ κ²€μ¦ μ„ΈνΈμ μΈλ±μ¤κ°€ λ‹¬λΌμ§μ„ μ• μ μμ!
<br><br>Stratified K Fold
λ¶κ· ν•ν•(imbalanced) λ¶„ν¬λ„λ¥Ό κ°€μ§„ label(κ²°μ • ν΄λμ¤) λ°μ΄ν„° μ§‘ν•©μ„ μ„ν• K fold λ°©μ‹

<br>λ¶κ· ν•ν• λ¶„ν¬λ„λ¥Ό κ°€μ§„ label λ°μ΄ν„° μ§‘ν•©μ€ νΉμ • label κ°’μ΄ νΉμ΄ν•κ² λ§κ±°λ‚ λ§¤μ° μ μ–΄μ„ κ°’μ λ¶„ν¬κ°€ ν•μ½μΌλ΅ μΉμ°μΉλ” κ²ƒμ„ μλ―Έ
<br>K Foldκ°€ label λ°μ΄ν„° μ§‘ν•©μ΄ μ›λ³Έ λ°μ΄ν„° μ§‘ν•©μ label λ¶„ν¬λ¥Ό ν•™μµ λ° ν…μ¤νΈ μ„ΈνΈμ— μ λ€λ΅ λ¶„λ°°ν•μ§€ λ»ν•λ” κ²½μ°μ λ¬Έμ λ¥Ό ν•΄κ²°!

<br>μ΄λ¥Ό μ„ν•΄ μ›λ³Έ λ°μ΄ν„°μ label λ¶„ν¬λ¥Ό λ¨Όμ € κ³ λ ¤ν• λ’¤ μ΄ λ¶„ν¬μ™€ λ™μΌν•κ² ν•™μµκ³Ό κ²€μ¦ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ¶„λ°°



<br>
λ€μ¶ μ‚¬κΈ° λ°μ΄ν„°λ¥Ό μμΈ΅ν•λ‹¤κ³  κ°€μ •!
<br>
<br>μ΄ λ°μ΄ν„° μ„ΈνΈλ” 1μ–µ κ±΄μ΄κ³ , μμ‹­ κ°μ featureμ™€ λ€μ¶ μ‚¬κΈ° μ—¬λ¶€λ¥Ό λ»ν•λ” label(μ‚¬κΈ°:1, μ •μƒ:0)λ΅ κµ¬μ„±λμ–΄ μμ
<br>κ·Έλ°λ° λ€λ¶€λ¶„μ λ°μ΄ν„°λ” μ •μƒ λ€μ¶μΌ κ²ƒ!
<br>λ€μ¶ μ‚¬κΈ°κ°€ μ•½ 1000κ±΄μ΄ μλ‹¤κ³  ν•λ‹¤λ©΄ μ „μ²΄μ 0.0001%μ μ•„μ£Ό μ‘μ€ ν™•λ¥ λ΅ λ€μ¶ μ‚¬κΈ° labelμ΄ μ΅΄μ¬
<br>μ΄λ ‡κ² λλ‹¤λ©΄ K Foldλ΅ λλ¤ν•κ² ν•™μµ λ° ν…μ¤νΈ μ„ΈνΈμ μΈλ±μ¤λ¥Ό κ³ λ¥΄λ”λΌκ³  label κ°’μΈ 0κ³Ό 1μ λΉ„μ¨μ„ μ λ€λ΅ λ°μν•μ§€ λ»ν•λ” κ²½μ°κ°€ μ‰½κ² λ°μƒ!
<br>λ”°λΌμ„ μ›λ³Έ λ°μ΄ν„°μ™€ μ μ‚¬ν• λ€μ¶ μ‚¬κΈ° λ μ΄λΈ” κ°’μ λ¶„ν¬λ¥Ό ν•™μµ/ν…μ¤νΈ μ„ΈνΈμ—λ„ μ μ§€ν•λ” κ² λ§¤μ° μ¤‘μ”!
<br>Note
λ¨Όμ € K Foldκ°€ μ–΄λ–¤ λ¬Έμ λ¥Ό κ°€μ§€κ³  μλ”μ§€ ν™•μΈν•΄ λ³΄κ³  μ΄λ¥Ό μ‚¬μ΄ν‚·λ°μ StratifiedKFold ν΄λμ¤λ¥Ό μ΄μ©ν•΄ κ°μ„ !
<br>
<br>λ¶“κ½ƒ λ°μ΄ν„° μ„ΈνΈλ¥Ό κ°„λ‹¨ν•κ² DataFrameμΌλ΅ μƒμ„±ν•κ³  label κ°’μ λ¶„ν¬λ„ ν™•μΈ
<br>import pandas as pd

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label']=iris.target
iris_df['label'].value_counts()
<br><img alt="Pasted image 20250211164900.png" src="lib\media\pasted-image-20250211164900.png"><br>
<br>κ° κµμ°¨ κ²€μ¦ μ‹λ§λ‹¤ μƒμ„±λλ” ν•™μµ/κ²€μ¦ label λ°μ΄ν„° κ°’μ λ¶„ν¬λ„ ν™•μΈ
<br>kfold = KFold(n_splits=3)
# kfold.split(X)λ” ν΄λ“ μ„ΈνΈλ¥Ό 3λ² λ°λ³µν•  λ•λ§λ‹¤ λ‹¬λΌμ§€λ” ν•™μµ/ν…μ¤νΈ μ© λ°μ΄ν„° λ΅μ° μΈλ±μ¤ λ²νΈ λ°ν™.
n_iter =0
for train_index, test_index  in kfold.split(iris_df):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]

    print('## κµμ°¨ κ²€μ¦: {0}'.format(n_iter))
    print('ν•™μµ λ μ΄λΈ” λ°μ΄ν„° λ¶„ν¬:\n', label_train.value_counts())
    print('κ²€μ¦ λ μ΄λΈ” λ°μ΄ν„° λ¶„ν¬:\n', label_test.value_counts())
<br><img alt="0211170424950648.jpg" src="lib\media\0211170424950648.jpg"><br>
<br>κµμ°¨ κ²€μ¦ μ‹λ§λ‹¤ 3κ°μ fold μ„ΈνΈλ΅ λ§λ“¤μ–΄μ§€λ” ν•™μµ labelκ³Ό κ²€μ¦ labelμ΄ μ™„μ „ν λ‹¤λ¥Έ κ°’μΌλ΅ μ¶”μ¶λ¨
<br>μ΄λ° μ ν•μΌλ΅ κµμ°¨ κ²€μ¦ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ¶„ν• ν•λ©΄ κ²€μ¦ μμΈ΅ μ •ν™•λ„λ” 0μ΄ λ  μλ°–μ— μ—†μ
<br>
<br>λ™μΌν• λ°μ΄ν„° λ¶„ν• μ„ StratifiedKFoldλ΅ μν–‰ν•κ³  ν•™μµ/κ²€μ¦ label λ°μ΄ν„°μ λ¶„ν¬λ„ ν™•μΈ
<br>
StratifiedKFoldλ¥Ό μ‚¬μ©ν•λ” λ°©λ²•μ€ KFoldλ¥Ό μ‚¬μ©ν•λ” λ°©λ²•κ³Ό κ±°μ λΉ„μ·ν•μ§€λ§, λ‹¨ ν•λ‚ ν° μ°¨μ΄λ” StratifiedKFoldλ” label λ°μ΄ν„° λ¶„ν¬λ„μ— λ”°λΌ ν•™μµ/κ²€μ¦ λ°μ΄ν„°λ¥Ό λ‚λ„κΈ° λ•λ¬Έμ— split()λ©”μ„λ“μ— μΈμλ΅ feature λ°μ΄ν„° μ„ΈνΈλΏλ§ μ•„λ‹λΌ label λ°μ΄ν„° μ„ΈνΈλ„ λ°λ“μ‹ ν•„μ”!
<br>from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
n_iter=0

for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]

    print('## κµμ°¨ κ²€μ¦: {0}'.format(n_iter))
    print('ν•™μµ λ μ΄λΈ” λ°μ΄ν„° λ¶„ν¬:\n', label_train.value_counts())
    print('κ²€μ¦ λ μ΄λΈ” λ°μ΄ν„° λ¶„ν¬:\n', label_test.value_counts())
<br><img alt="021117243817062.jpg" src="lib\media\021117243817062.jpg"><br>
<br>μ¶λ ¥ κ²°κ³Όλ¥Ό λ³΄λ©΄ ν•™μµ labelκ³Ό κ²€μ¦ label λ°μ΄ν„° κ°’μ λ¶„ν¬λ„κ°€ κ±°μ λ™μΌν•κ² ν• λ‹Ήλμμ„ μ• μ μμ
<br>μ΄λ ‡κ² λ¶„ν• μ΄ λμ–΄μ•Ό label κ°’ 0, 1, 2λ¥Ό λ¨λ‘ ν•™μµν•  μ μκ³ , μ΄μ— κΈ°λ°ν•΄ κ²€μ¦μ„ μν–‰ν•  μ μμ
<br>
<br>StratifiedKFoldλ¥Ό μ΄μ©ν•΄ λ¶“κ½ƒ λ°μ΄ν„° κµμ°¨ κ²€μ¦
<br>dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFoldμ split( ) νΈμ¶μ‹ λ°λ“μ‹ λ μ΄λΈ” λ°μ΄ν„° μ…‹λ„ μ¶”κ°€ μ…λ ¥ ν•„μ”  
for train_index, test_index  in skfold.split(features, label):
    # split( )μΌλ΅ λ°ν™λ μΈλ±μ¤λ¥Ό μ΄μ©ν•μ—¬ ν•™μµμ©, κ²€μ¦μ© ν…μ¤νΈ λ°μ΄ν„° μ¶”μ¶
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    #ν•™μµ λ° μμΈ΅
    dt_clf.fit(X_train, y_train)    
    pred = dt_clf.predict(X_test)

    # λ°λ³µ μ‹ λ§λ‹¤ μ •ν™•λ„ μΈ΅μ •
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]

    print('\n#{0} κµμ°¨ κ²€μ¦ μ •ν™•λ„ :{1}, ν•™μµ λ°μ΄ν„° ν¬κΈ°: {2}, κ²€μ¦ λ°μ΄ν„° ν¬κΈ°: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} κ²€μ¦ μ„ΈνΈ μΈλ±μ¤:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)

# κµμ°¨ κ²€μ¦λ³„ μ •ν™•λ„ λ° ν‰κ·  μ •ν™•λ„ κ³„μ‚°
print('\n## κµμ°¨ κ²€μ¦λ³„ μ •ν™•λ„:', np.round(cv_accuracy, 4))
print('## ν‰κ·  κ²€μ¦ μ •ν™•λ„:', np.round(np.mean(cv_accuracy), 4))
<br><img alt="Pasted image 20250211172934.png" src="lib\media\pasted-image-20250211172934.png"><br>Note
Stratified K Fold μ κ²½μ° μ›λ³Έ λ°μ΄ν„°μ label λ¶„ν¬λ„ νΉμ„±μ„ λ°μν• ν•™μµ λ° κ²€μ¦ λ°μ΄ν„° μ„ΈνΈλ¥Ό λ§λ“¤ μ μμΌλ―€λ΅ μ™κ³΅λ label λ°μ΄ν„° μ„ΈνΈμ—μ„λ” λ°λ“μ‹ Stratified K Foldλ¥Ό μ΄μ©ν•΄ κµμ°¨ κ²€μ¦ν•΄μ•Ό ν•¨!
<br><br>
<br>μ‚¬μ΄ν‚·λ°μ€ κµμ°¨ κ²€μ¦μ„ μΆ€ λ” νΈλ¦¬ν•κ² μν–‰ν•  μ μκ² ν•΄μ£Όλ” API μ κ³µ
<br>KFoldλ΅ λ°μ΄ν„°λ¥Ό ν•™μµν•κ³  μμΈ΅ ν•λ” μ½”λ“ μμ„

<br>

<br>fold μ„ΈνΈλ¥Ό μ„¤μ •


<br>

<br>for λ£¨ν”„μ—μ„ λ°λ³µμΌλ΅ ν•™μµ λ° ν…μ¤νΈ λ°μ΄ν„°μ μΈλ±μ¤λ¥Ό μ¶”μ¶


<br>

<br>λ°λ³µμ μΌλ΅ ν•™μµκ³Ό μμΈ΅μ„ μν–‰ν•κ³  μμΈ΅ μ„±λ¥ λ°ν™




<br>cross_val_score()λ” μ΄λ° μΌλ ¨μ κ³Όμ •μ„ ν•κΊΌλ²μ— μν–‰ν•΄μ£Όλ” API
<br>cross_val_score()
cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs') 

<br>μ£Όμ” parameter : estimator, X, y, scoring, cv

<br>estimator : Classifier ν΄λμ¤ or Regressor ν΄λμ¤λ¥Ό μλ―Έ
<br>X : feature λ°μ΄ν„° μ„ΈνΈ
<br>y : label λ°μ΄ν„° μ„ΈνΈ
<br>scoring : μμΈ΅ μ„±λ¥ ν‰κ°€ μ§€ν‘
<br>cv : κµμ°¨ κ²€μ¦ fold μ

<br>KFold κ°μ²΄λ‚ StratifiedKFold κ°μ²΄λ¥Ό μ…λ ¥ν•  μλ„ μμ




<br>λ°ν™ κ°’μ€ scoring νλΌλ―Έν„°λ΅ μ§€μ •λ μ„±λ¥ μ§€ν‘ μΈ΅μ •κ°’μ„ λ°°μ—΄ ν•νƒλ΅ λ°ν™
<br>μ¦‰, classifierκ°€ μ…λ ¥λλ©΄ Stratified K fold λ°©μ‹μΌλ΅ label κ°’μ λ¶„ν¬μ— λ”°λΌ ν•™μµ/ν…μ¤νΈ μ„ΈνΈ λ¶„ν•  (νκ·€μΈ κ²½μ°λ” Stratified K fold λ°©μ‹μΌλ΅ λ¶„ν•  ν•  μ μ—†μΌλ―€λ΅ K fold λ°©μ‹μΌλ΅ λ¶„ν• )

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score , cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# μ„±λ¥ μ§€ν‘λ” μ •ν™•λ„(accuracy) , κµμ°¨ κ²€μ¦ μ„ΈνΈλ” 3κ°
scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)
print('κµμ°¨ κ²€μ¦λ³„ μ •ν™•λ„:',np.round(scores, 4))
print('ν‰κ·  κ²€μ¦ μ •ν™•λ„:', np.round(np.mean(scores), 4))

&gt;&gt;&gt; κµμ°¨ κ²€μ¦λ³„ μ •ν™•λ„: [0.98 0.94 0.98]
    ν‰κ·  κ²€μ¦ μ •ν™•λ„: 0.9667
<br>
<br>cross_val_score() APIλ” λ‚΄λ¶€μ—μ„ Estimatorλ¥Ό ν•™μµ(fit), μμΈ΅(predict), ν‰κ°€(evaluation)μ‹μΌμ£Όλ―€λ΅ κ°„λ‹¨ν•κ² κµμ°¨ κ²€μ¦μ„ μν–‰ν•  μ μμ!
<br>cv νλΌλ―Έν„°μ— μ •μκ°’(fold μ)λ¥Ό μ…λ ¥ν•λ©΄ λ‚΄λ¶€μ μΌλ΅ StratifiedKFoldλ¥Ό μ΄μ©
<br>λΉ„μ·ν• APIλ΅ cross_validate() μ΅΄μ¬

<br>μ—¬λ¬ κ°μ ν‰κ°€ μ§€ν‘ λ°ν™ κ°€λ¥
<br>ν•™μµ λ°μ΄ν„°μ— λ€ν• μ„±λ¥ ν‰κ°€ μ§€ν‘μ™€ μν–‰ μ‹κ°„λ„ κ°™μ΄ μ κ³µ


<br><br>Note
ν•μ΄νΌ νλΌλ―Έν„° (Hyperparameter)

<br>λ¨λΈμ„ ν•™μµν•κΈ° μ „μ— μ‚¬μ©μκ°€ μ§μ ‘ μ„¤μ •ν•λ” κ°’μΌλ΅, μ΄ κ°’μ„ μ΅°μ •ν•΄ μ•κ³ λ¦¬μ¦μ μμΈ΅ μ„±λ¥ κ°μ„ 

GridSearchCV

<br>μ‚¬μ΄ν‚·λ°μ—μ„ μ κ³µν•λ” ν•μ΄νΌ νλΌλ―Έν„° μµμ ν™” API
<br>Classifierλ‚ Regressorμ™€ κ°™μ€ μ•κ³ λ¦¬μ¦μ— μ‚¬μ©λλ” ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ μ…λ ¥ν•λ©΄μ„ νΈλ¦¬ν•κ² μµμ μ νλΌλ―Έν„°λ¥Ό λ„μ¶ν•  μ μλ” λ°©μ• μ κ³µ
<br>Gridλ” κ²©μλΌλ” λ»μΌλ΅, μ΄μ΄ν•κ² νλΌλ―Έν„°λ¥Ό μ…λ ¥ν•λ©΄μ„ ν…μ¤νΈλ¥Ό ν•λ” λ°©μ‹

<br>
<br>κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦μ μ—¬λ¬ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ λ³€κ²½ν•λ©΄μ„ μµκ³  μ„±λ¥μ„ κ°€μ§€λ” νλΌλ―Έν„° μ΅°ν•©μ„ μ°Ύκ³ μ ν•λ‹¤λ©΄ 
<br>λ‹¤μκ³Ό κ°™μ΄ νλΌλ―Έν„°μ μ§‘ν•©μ„ λ§λ“¤κ³  μ΄λ¥Ό μμ°¨μ μΌλ΅ μ μ©ν•λ©΄μ„ μµμ ν™” μν–‰ κ°€λ¥
<br>grid_parameters = {'max_depth': [1, 2, 3],
				   'min_samples_split': [2, 3]}
<br>
<br>GridSearchCVλ” κµμ°¨ κ²€μ¦μ„ κΈ°λ°μΌλ΅ μ΄ ν•μ΄νΌ νλΌλ―Έν„°μ μµμ κ°’μ„ μ°Ύκ² ν•΄μ¤!

<br>

<br>λ°μ΄ν„° μ„ΈνΈλ¥Ό cross-validationμ„ μ„ν• ν•™μµ/ν…μ¤νΈ μ„ΈνΈλ΅ μλ™μΌλ΅ λ¶„ν• 


<br>

<br>ν•μ΄νΌ νλΌλ―Έν„° gridμ— κΈ°μ λ λ¨λ“  νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ μ μ©ν•΄ μµμ μ νλΌλ―Έν„°λ¥Ό μ°Ύμ„ μ μκ² ν•΄μ¤




<br>λ‹¨, λ™μ‹μ— μμ°¨μ μΌλ΅ νλΌλ―Έν„°λ¥Ό ν…μ¤νΈν•λ―€λ΅ μν–‰μ‹κ°„μ΄ μƒλ€μ μΌλ΅ μ¤λ κ±Έλ¦¬λ” λ‹¨μ  μ΅΄μ¬!

<br>μ„μ κ²½μ° CVκ°€ 3νλΌλ©΄ CV 3ν x 6κ° νλΌλ―Έν„° μ΅°ν•© = 18νμ ν•™μµ/ν‰κ°€ μ΄λ£¨μ–΄μ§


<br>GridSearchCV ν΄λμ¤

<br>estimator : classifier, regressor, pipelineμ΄ μ‚¬μ©λ  μ μμ
<br>param_grid : key + λ¦¬μ¤νΈ κ°’μ„ κ°–λ” λ”•μ…”λ„λ¦¬κ°€ μ£Όμ–΄μ§ (estimatorμ νλ‹μ„ μ„ν•΄ νλΌλ―Έν„°λ…κ³Ό μ‚¬μ©λ  μ—¬λ¬ νλΌλ―Έν„° κ°’ μ§€μ •)
<br>scoring : μμΈ΅ μ„±λ¥μ„ μΈ΅μ •ν•  ν‰κ°€ λ°©λ²• μ§€μ •, λ³΄ν†µμ€ μ‚¬μ΄ν‚·λ°μ μ„±λ¥ ν‰κ°€ μ§€ν‘λ¥Ό μ§€μ •ν•λ” λ¬Έμμ—΄(ex:'accuracy')λ΅ μ§€μ •ν•λ‚ λ³„λ„μ μ„±λ¥ ν‰κ°€ μ§€ν‘ ν•¨μλ„ μ§€μ •ν•  μ μμ
<br>cv : κµμ°¨ κ²€μ¦μ„ μ„ν•΄ λ¶„ν• λλ” ν•™μµ/ν…μ¤νΈ μ„ΈνΈμ κ°μ μ§€μ •
<br>refit : default=Trueμ΄λ©° Trueλ΅ μƒμ„± μ‹ κ°€μ¥ μµμ μ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό μ°Ύμ€ λ’¤ μ…λ ¥λ estimator κ°μ²΄λ¥Ό ν•΄λ‹Ή ν•μ΄νΌ νλΌλ―Έν„°λ΅ μ¬ν•™μµ

<br>
&lt;μμ &gt;<br>
κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦μ μ—¬λ¬ κ°€μ§€ μµμ ν™” νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ μ μ©ν•΄ λ¶“κ½ƒ λ°μ΄ν„°λ¥Ό μμΈ΅ λ¶„μ„ν•λ” λ° GridSearchCV μ΄μ©
<br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# λ°μ΄ν„°λ¥Ό λ΅λ”©ν•κ³  ν•™μµλ°μ΄νƒ€μ™€ ν…μ¤νΈ λ°μ΄ν„° λ¶„λ¦¬
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,
                                                    test_size=0.2, random_state=121)
dtree = DecisionTreeClassifier()

### parameter λ“¤μ„ dictionary ν•νƒλ΅ μ„¤μ •
parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}
<br>
<br>train_test_split()μ„ μ΄μ©ν•΄ ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ¥Ό λ¨Όμ € λ¶„λ¦¬
<br>ν…μ¤νΈν•  ν•μ΄νΌ νλΌλ―Έν„°λ“¤μ„ dictionary ν•νƒλ΅ μ„¤μ •
<br>import pandas as pd

# param_gridμ ν•μ΄νΌ νλΌλ―Έν„°λ“¤μ„ 3κ°μ train, test set fold λ΅ λ‚λ„μ–΄μ„ ν…μ¤νΈ μν–‰ μ„¤μ •.  
### refit=True κ°€ default μ„. Trueμ΄λ©΄ κ°€μ¥ μΆ‹μ€ νλΌλ―Έν„° μ„¤μ •μΌλ΅ μ¬ ν•™μµ μ‹ν‚΄.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# λ¶“κ½ƒ Train λ°μ΄ν„°λ΅ param_gridμ ν•μ΄νΌ νλΌλ―Έν„°λ“¤μ„ μμ°¨μ μΌλ΅ ν•™μµ/ν‰κ°€ .
grid_dtree.fit(X_train, y_train)

# GridSearchCV κ²°κ³Ό μ¶”μ¶ν•μ—¬ DataFrameμΌλ΅ λ³€ν™
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score',
           'split0_test_score', 'split1_test_score', 'split2_test_score']]
<br>
<br>ν•™μµ λ°μ΄ν„° μ„ΈνΈλ¥Ό GridSearchCV κ°μ²΄μ fit() λ©”μ„λ“μ— μΈμλ΅ μ…λ ¥
<br>ν•™μµ λ°μ΄ν„°λ¥Ό cvμ— κΈ°μ λ ν΄λ”© μ„ΈνΈλ΅ λ¶„ν• ν•΄ param_gridμ— κΈ°μ λ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό μμ°¨μ μΌλ΅ λ³€κ²½ν•λ©΄μ„ ν•™μµ/ν‰κ°€λ¥Ό μν–‰ν•κ³  κ·Έ κ²°κ³Όλ¥Ό cv_results_ μ†μ„±μ— κΈ°λ΅
<br>
&lt;κ²°κ³Ό&gt;<img alt="Pasted image 20250211213803.png" src="lib\media\pasted-image-20250211213803.png">

<br>params μ»¬λΌμ—λ” μν–‰ν•  λ•λ§λ‹¤ μ μ©λ κ°λ³„ ν•μ΄νΌ νλΌλ―Έν„°κ°’μ„ λ‚νƒ€λƒ„
<br>rank_test_scoreλ” ν•μ΄νΌ νλΌλ―Έν„°λ³„λ΅d μ„±λ¥μ΄ μΆ‹μ€ score μμ„λ¥Ό λ‚νƒ€λƒ„ (1μ΄ κ°€μ¥ λ›°μ–΄λ‚ μμ„μ΄λ©° μ΄λ•μ νλΌλ―Έν„°κ°€ μµμ μ ν•μ΄νΌ νλΌλ―Έν„°)
<br>mean_test_scoreλ” κ°λ³„ ν•μ΄νΌ νλΌλ―Έν„°λ³„λ΅ CVμ ν΄λ”© ν…μ¤νΈ μ„ΈνΈμ— λ€ν•΄ μ΄ μν–‰ν• ν‰κ°€ ν‰κ· κ°’

<br>print('GridSearchCV μµμ  νλΌλ―Έν„°:', grid_dtree.best_params_)
print('GridSearchCV μµκ³  μ •ν™•λ„: {0:.4f}'.format(grid_dtree.best_score_))
<br><img alt="Pasted image 20250211214549.png" src="lib\media\pasted-image-20250211214549.png"><br>
<br>GridSearchCV κ°μ²΄μ fit()μ„ μν–‰ν•λ©΄ μµκ³  μ„±λ¥μ„ λ‚νƒ€λ‚Έ ν•μ΄νΌ νλΌλ―Έν„°μ κ°’κ³Ό κ·Έλ•μ ν‰κ°€ κ²°κ³Ό κ°’μ΄ κ°κ° best_params_, best_score_ μ†μ„±μ— κΈ°λ΅
<br># GridSearchCVμ refitμΌλ΅ μ΄λ―Έ ν•™μµμ΄ λ estimator λ°ν™
estimator = grid_dtree.best_estimator_

# GridSearchCVμ best_estimator_λ” μ΄λ―Έ μµμ  ν•μ΄νΌ νλΌλ―Έν„°λ΅ ν•™μµμ΄ λ¨
pred = estimator.predict(X_test)
print('ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ μ •ν™•λ„: {0:.4f}'.format(accuracy_score(y_test,pred)))

&gt;&gt;&gt; ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ μ •ν™•λ„: 0.9667
<br>
<br>refit=Trueμ΄λ©΄ GridSearchCVκ°€ μµμ  μ„±λ¥μ„ λ‚νƒ€λ‚΄λ” ν•μ΄νΌ νλΌλ―Έν„°λ΅ Estimatorλ¥Ό λ‹¤μ‹ ν•™μµν•΄ best_estimator_λ΅ μ €μ¥

<br>refit=FalseμΈ κ²½μ° μµμ μ λ¨λΈ(best_estimator_)μ„ μλ™μΌλ΅ λ‹¤μ‹ ν•™μµν•μ§€ μ•μ!
<br>μ¦‰, best_estimator_ μ†μ„±μ΄ μ΅΄μ¬ν•μ§€ μ•μΌλ©°, μ¤μ§ κµμ°¨ κ²€μ¦ κ²°κ³Ό(cv_results_)λ§ μ κ³µλ¨


<br>μ΄λ―Έ ν•™μµλ best_estimator_λ¥Ό μ΄μ©ν•΄ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ μ •ν™•λ„λ¥Ό μΈ΅μ •ν• κ²°κ³Ό μ•½ 96.67%μ κ²°κ³Ό λ„μ¶
<br>Tip
ν•™μµ λ°μ΄ν„°λ¥Ό GridSearchCVλ¥Ό μ΄μ©ν•΄ μµμ  ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ μν–‰ν• λ’¤μ— λ³„λ„μ ν…μ¤νΈ μ„ΈνΈμ—μ„ μ΄λ¥Ό ν‰κ°€ν•λ” κ²ƒμ΄ μΌλ°μ μΈ λ¨Έμ‹ λ¬λ‹ λ¨λΈ μ μ© λ°©λ²•
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(4)-model-selection-λ¨λ“-μ†κ°.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(4) Model Selection λ¨λ“ μ†κ°.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate><enclosure url="lib\media\pasted-image-20250210210414.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250210210414.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(5) λ°μ΄ν„° μ „μ²λ¦¬]]></title><description><![CDATA[ 
 <br>λ°μ΄ν„° μ „μ²λ¦¬ (Data Preprocessing)
"Garbage In, Garbage Out"<br>
λ¶„μ„μ΄λ‚ λ¨Έμ‹ λ¬λ‹ λ¨λΈλ§ μ „μ— λ°μ΄ν„°λ¥Ό μ •μ ν•κ³  λ³€ν•ν•μ—¬ μµμ μ μƒνƒλ΅ λ§λ“λ” κ³Όμ •
<br>
<br>λ¨Έμ‹ λ¬λ‹ λ¨λΈλ§μ—μ„ κ²°μ†κ°’, μ¦‰ NaN, Null κ°’μ€ ν—μ©λμ§€ μ•μ

<br>λ”°λΌμ„ μ΄λ¬ν• Null κ°’μ€ κ³ μ •λ λ‹¤λ¥Έ κ°’μΌλ΅ λ³€ν™ν•΄μ•Ό ν•¨!


<br>Null κ°’μ„ μ–΄λ–»κ² μ²λ¦¬ν•΄μ•Ό ν• μ§€λ” κ²½μ°μ— λ”°λΌ λ‹¤λ¦„

<br>feature κ°’ μ¤‘ Null κ°’μ΄ μ–Όλ§ λμ§€ μ•λ”λ‹¤λ©΄ featureμ ν‰κ· κ°’ λ“±μΌλ΅ κ°„λ‹¨ν λ€μ²΄ν•  μ μμ
<br>ν•μ§€λ§, Null κ°’μ΄ λ€λ¶€λ¶„μ΄λΌλ©΄ μ¤νλ ¤ ν•΄λ‹Ή featureλ” drop ν•λ” κ²ƒμ΄ λ” μΆ‹μ!


<br>κ°€μ¥ κ²°μ •μ΄ νλ“  λ¶€λ¶„μ΄ Null κ°’μ΄ μΌμ • μμ¤€ μ΄μƒ λλ” κ²½μ°!

<br>μ •ν™•ν λ‡ νΌμ„ΌνΈκΉμ§€λ¥Ό μΌμ • μμ¤€ μ΄μƒμ΄λΌκ³  ν•λ‹¤λ” κΈ°μ¤€μ€ μ—†μ§€λ§, ν•΄λ‹Ή featureκ°€ μ¤‘μ”λ„κ°€ λ†’μ€ featureμ΄κ³  Nullμ„ λ‹¨μν featureμ ν‰κ· κ°’μΌλ΅ λ€μ²΄ν•  κ²½μ° μμΈ΅ μ™κ³΅μ΄ μ‹¬ν•  μ μλ‹¤λ©΄ μ—…λ¬΄ λ΅μ§ λ“±μ„ μƒμ„Έν κ²€ν† ν•΄ λ” μ •λ°€ν• λ€μ²΄ κ°’μ„ μ„ μ •ν•΄μ•Ό ν•¨


<br>μ‚¬μ΄ν‚·λ°μ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ€ λ¬Έμμ—΄ κ°’μ„ μ…λ ¥κ°’μΌλ΅ ν—μ©ν•μ§€ μ•μ

<br>λ¨λ“  λ¬Έμμ—΄ κ°’μ€ μΈμ½”λ”©λΌμ„ μ«μν•μΌλ΅ λ³€ν™ν•΄μ•Ό ν•¨!
<br>λ¬Έμμ—΄ featureλ” μΌλ°μ μΌλ΅ μΉ΄ν…κ³ λ¦¬ν• featureμ™€ ν…μ¤νΈν• featureλ¥Ό μλ―Έ


<br><br><br>λ°μ΄ν„° μΈμ½”λ”© (Data Encoding)
λ¨Έμ‹ λ¬λ‹ λ¨λΈμ΄ μ΄ν•΄ν•  μ μλ„λ΅ λ²”μ£Όν•(category) λ°μ΄ν„°λ¥Ό μ«μλ΅ λ³€ν™ν•λ” κ³Όμ •<br>
λ¨Έμ‹ λ¬λ‹μ„ μ„ν• λ€ν‘μ μΈ μΈμ½”λ”© λ°©μ‹μ€ λ μ΄λΈ” μΈμ½”λ”©(Label encoding)κ³Ό μ›-ν•« μΈμ½”λ”©(One-Hot encoding)
<br><br>λ μ΄λΈ” μΈμ½”λ”© (Label Encoding)
κ° λ²”μ£Ό(category)λ¥Ό μ •μ(0, 1, 2...)λ΅ λ³€ν™ν•λ” λ°©μ‹

<br>μ¥μ  : κ°„λ‹¨ν•κ² λ¬Έμμ—΄ κ°’μ„ μ«μν• μΉ΄ν…κ³ λ¦¬ κ°’μΌλ΅ λ³€ν™
<br>λ‹¨μ  : μ«μ κ°’μ κ²½μ° ν¬κ³  μ‘μμ— λ€ν• νΉμ„±μ΄ μ‘μ©ν•κΈ° λ•λ¬Έμ— λ‡λ‡ ML μ•κ³ λ¦¬μ¦μ—λ” μ΄λ¥Ό μ μ©ν•  κ²½μ° μμΈ΅ μ„±λ¥μ΄ λ–¨μ–΄μ§€λ” κ²½μ°κ°€ λ°μƒν•  μ μμ

<br>μ«μ λ³€ν™ κ°’μ€ λ‹¨μ μ½”λ“μ΄μ§€ μ«μ κ°’μ— λ”°λ¥Έ μμ„λ‚ μ¤‘μ”λ„λ΅ μΈμ‹λΌμ„λ” μ• λ¨
<br>μ΄λ¬ν• νΉμ„± λ•λ¬Έμ— λ μ΄λΈ” μΈμ½”λ”©μ€ μ„ ν• νκ·€μ™€ κ°™μ€ ML μ•κ³ λ¦¬μ¦μ—λ” μ μ©ν•μ§€ μ•μ•„μ•Ό ν•¨
<br>tree κ³„μ—΄μ ML μ•κ³ λ¦¬μ¦μ€ μ μ© κ°€λ¥



<br>
<br>μ‚¬μ΄ν‚·λ°μ λ μ΄λΈ” μΈμ½”λ”©μ€ LabelEncoder ν΄λμ¤λ΅ κµ¬ν„
<br>LabelEncoderλ¥Ό κ°μ²΄λ΅ μƒμ„±ν• ν›„ fit()κ³Ό transform()μ„ νΈμ¶ν•΄ λ μ΄λΈ” μΈμ½”λ”© μν–‰
<br>from sklearn.preprocessing import LabelEncoder

items=['TV','λƒ‰μ¥κ³ ','μ „μλ μΈμ§€','μ»΄ν“¨ν„°','μ„ ν’κΈ°','μ„ ν’κΈ°','λ―Ήμ„','λ―Ήμ„']

# LabelEncoderλ¥Ό κ°μ²΄λ΅ μƒμ„±ν• ν›„ , fit( ) κ³Ό transform( ) μΌλ΅ label μΈμ½”λ”© μν–‰.
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
print('μΈμ½”λ”© λ³€ν™κ°’:',labels)

&gt;&gt;&gt; μΈμ½”λ”© λ³€ν™κ°’: [0 1 4 5 3 3 2 2]
<br>
<br>μ„ μμ λ” λ°μ΄ν„°κ°€ μ‘μ•„μ„ λ¬Έμμ—΄ κ°’μ΄ μ–΄λ–¤ μ«μ κ°’μΌλ΅ μΈμ½”λ”©λλ”μ§€ μ§κ΄€μ μΌλ΅ μ• μ μμ§€λ§, λ§μ€ κ²½μ°μ— μ΄λ¥Ό μ•μ§€ λ»ν•¨
<br>μ΄ κ²½μ°μ—λ” LabelEncoder κ°μ²΄μ classes_ μ†μ„±κ°’μΌλ΅ ν™•μΈ!<br>
print('μΈμ½”λ”© ν΄λμ¤:',encoder.classes_)<br>
μΈμ½”λ”© ν΄λμ¤: ['TV' 'λƒ‰μ¥κ³ ' 'λ―Ήμ„' 'μ„ ν’κΈ°' 'μ „μλ μΈμ§€' 'μ»΄ν“¨ν„°']

<br>classes_ μ†μ„±μ€ 0λ²λ¶€ν„° μμ„λ€λ΅ λ³€ν™λ μΈμ½”λ”© κ°’μ— λ€ν• μ›λ³Έκ°’μ„ κ°€μ§€κ³  μμ


<br>inverse_transform()μ„ ν†µν•΄ μΈμ½”λ”©λ κ°’μ„ λ‹¤μ‹ λ””μ½”λ”©ν•  μ μμ<br>
print('λ””μ½”λ”© μ›λ³Έ κ°’:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))<br>
λ””μ½”λ”© μ›λ³Έ κ°’: ['μ „μλ μΈμ§€' 'μ»΄ν“¨ν„°' 'λ―Ήμ„' 'TV' 'λƒ‰μ¥κ³ ' 'λƒ‰μ¥κ³ ' 'μ„ ν’κΈ°' 'μ„ ν’κΈ°']
<br><br>μ›-ν•« μΈμ½”λ”© (One-Hot Encoding)
feature κ°’μ μ ν•μ— λ”°λΌ μƒλ΅μ΄ featureλ¥Ό μ¶”κ°€ν•΄ κ³ μ  κ°’μ— ν•΄λ‹Ήν•λ” μΉΌλΌμ—λ§ 1μ„ ν‘μ‹ν•κ³  λ‚λ¨Έμ§€ μΉΌλΌμ—λ” 0μ„ ν‘μ‹ν•λ” λ°©μ‹

<br>μ¦‰, row ν•νƒλ΅ λμ–΄ μλ” featureμ κ³ μ  κ°’μ„ column ν•νƒλ΅ μ°¨μ›μ„ λ³€ν™ν• λ’¤, κ³ μ  κ°’μ— ν•΄λ‹Ήν•λ” μΉΌλΌμ—λ§ 1μ„ ν‘μ‹ν•κ³  λ‚λ¨Έμ§€ columnμ—λ” 0μ„ ν‘μ‹

<br><img alt="Pasted image 20250212151043.png" src="lib\media\pasted-image-20250212151043.png"><br>
<br>μ›-ν•« μΈμ½”λ”©μ€ μ‚¬μ΄ν‚·λ°μ—μ„ OneHotEncoder ν΄λμ¤λ΅ λ³€ν™ κ°€λ¥
<br>λ‹¨, LabelEncoderμ™€ λ‹¤λ¥΄κ² μ•½κ°„ μ£Όμν•  μ μ€ μ…λ ¥κ°’μΌλ΅ 2μ°¨μ› λ°μ΄ν„°κ°€ ν•„μ”ν•λ‹¤λ” κ²ƒκ³Ό, OneHotEncoderλ¥Ό μ΄μ©ν•΄ λ³€ν™ν• κ°’μ΄ ν¬μ† ν–‰λ ¬(Sparse Matrix) ν•νƒμ΄λ―€λ΅ μ΄λ¥Ό λ‹¤μ‹ toarray() λ©”μ„λ“λ¥Ό μ΄μ©ν•΄ λ°€μ§‘ ν–‰λ ¬(Dense Matrix)λ΅ λ³€ν™ν•΄μ•Ό ν•λ‹¤λ” κ²ƒ

<br>μ—¬κΈ°μ„μ λ°€μ§‘ ν–‰λ ¬μ€ λ¨λ“  κ°’μ„ λ…μ‹μ μΌλ΅ λ©”λ¨λ¦¬μ— μ €μ¥ν•λ” ν–‰λ ¬μ„ μλ―Έ


<br>from sklearn.preprocessing import OneHotEncoder
import numpy as np

items=['TV','λƒ‰μ¥κ³ ','μ „μλ μΈμ§€','μ»΄ν“¨ν„°','μ„ ν’κΈ°','μ„ ν’κΈ°','λ―Ήμ„','λ―Ήμ„']

# 2μ°¨μ› ndarrayλ΅ λ³€ν™ν•©λ‹λ‹¤.
items = np.array(items).reshape(-1, 1)

# μ›-ν•« μΈμ½”λ”©μ„ μ μ©ν•©λ‹λ‹¤.
oh_encoder = OneHotEncoder()
oh_encoder.fit(items)
oh_labels = oh_encoder.transform(items)

# OneHotEncoderλ΅ λ³€ν™ν• κ²°κ³Όλ” ν¬μ†ν–‰λ ¬μ΄λ―€λ΅ toarray()λ¥Ό μ΄μ©ν•΄ λ°€μ§‘ ν–‰λ ¬λ΅ λ³€ν™.
print('μ›-ν•« μΈμ½”λ”© λ°μ΄ν„°')
print(oh_labels.toarray())
print('μ›-ν•« μΈμ½”λ”© λ°μ΄ν„° μ°¨μ›')
print(oh_labels.shape)
<br><img alt="Pasted image 20250212151601.png" src="lib\media\pasted-image-20250212151601.png"><br>
<br>
μ„ μμ  μ½”λ“μ λ³€ν™ μ μ°¨<br>
<img alt="Pasted image 20250212153523.png" src="lib\media\pasted-image-20250212153523.png">

<br>
νλ‹¤μ¤μ—λ” μ›-ν•« μΈμ½”λ”©μ„ λ” μ‰½κ² μ§€μ›ν•λ” API μ΅΄μ¬!

<br>pd.get_dummies(df)
<br>μ‚¬μ΄ν‚·λ°μ OneHotEncoderμ™€ λ‹¤λ¥΄κ² λ¬Έμμ—΄ μΉ΄ν…κ³ λ¦¬ κ°’μ„ μ«μν•μΌλ΅ λ³€ν™ν•  ν•„μ” μ—†μ΄ λ°”λ΅ λ³€ν™ κ°€λ¥!


<br>import pandas as pd
from tabulate import tabulate

df = pd.DataFrame({'item':['TV','λƒ‰μ¥κ³ ','μ „μλ μΈμ§€','μ»΄ν“¨ν„°','μ„ ν’κΈ°','μ„ ν’κΈ°','λ―Ήμ„','λ―Ήμ„'] })
print(tabulate(pd.get_dummies(df).astype(int), headers='keys', tablefmt='fancy_outline'))
<br><img alt="Pasted image 20250212154912.png" src="lib\media\pasted-image-20250212154912.png"><br><br><br>ν”Όμ² μ¤μΌ€μΌλ§ (feature scaling)
μ„λ΅ λ‹¤λ¥Έ λ³€μμ κ°’ λ²”μ„λ¥Ό μΌμ •ν• μμ¤€μΌλ΅ λ§μ¶”λ” μ‘μ—…

<br>λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ€ μ…λ ¥ κ°’μ ν¬κΈ°μ— μν–¥μ„ λ°›μ„ μ μκΈ° λ•λ¬Έμ—, νΉμ„±μ΄ μ„λ΅ λ‹¤λ¥Έ λ‹¨μ„λ‚ λ²”μ„λ¥Ό κ°€μ§€λ©΄ λ¨λΈ μ„±λ¥μ΄ μ €ν•λ  μ μμ!
<br>λ€ν‘μ μΈ λ°©λ²•μΌλ΅ ν‘μ¤€ν™”(Standardization)μ™€ μ •κ·ν™”(Normalization)

<br>ν‘μ¤€ν™” (Standardization)
λ°μ΄ν„°μ feature κ°κ°μ΄ ν‰κ· μ΄ 0μ΄κ³  λ¶„μ‚°μ΄ 1μΈ κ°€μ°μ‹μ• μ •κ· λ¶„ν¬λ¥Ό κ°€μ§„ κ°’μΌλ΅ λ³€ν™ν•λ” κ²ƒ
<br><br>μ •κ·ν™” (Normalization)
μ„λ΅ λ‹¤λ¥Έ feature ν¬κΈ°λ¥Ό ν†µμΌν•κΈ° μ„ν•΄ ν¬κΈ°λ¥Ό λ³€ν™ν•΄μ£Όλ” κ°λ…<br>
λ°μ΄ν„°μ κ°’μ„ λ¨λ‘ μµμ† 0 ~ μµλ€ 1μ κ°’μΌλ΅ λ³€ν™ν•λ” κ²ƒ
<br><br>
<br>λ‹¨, μ‚¬μ΄ν‚·λ°μ μ „μ²λ¦¬μ—μ„ μ κ³µν•λ” Normalizer λ¨λ“κ³Ό μΌλ°μ μΈ μ •κ·ν™”λ” μ•½κ°„μ μ°¨μ΄κ°€ μ΅΄μ¬!
<br>μ‚¬μ΄ν‚·λ°μ Normalizer λ¨λ“μ€ μ„ ν•λ€μμ—μ„μ μ •κ·ν™” κ°λ…μ΄ μ μ©λμΌλ©°, κ°λ³„ λ²΅ν„°μ ν¬κΈ°λ¥Ό λ§μ¶”κΈ° μ„ν•΄ λ³€ν™ν•λ” κ²ƒμ„ μλ―Έ!
<br>μ¦‰, κ°λ³„ λ²΅ν„°λ¥Ό λ¨λ“  feature λ²΅ν„°μ ν¬κΈ°λ΅ λ‚λ μ¤


<br>Note
νΌμ„ μ„ λ°©μ§€ν•κΈ° μ„ν•΄ μΌλ°μ μΈ μλ―Έμ ν‘μ¤€ν™”μ™€ μ •κ·ν™”λ¥Ό ν”Όμ² μ¤μΌ€μΌλ§μΌλ΅ ν†µμΉ­ν•κ³  μ„ ν•λ€μ κ°λ…μ μ •κ·ν™”λ¥Ό λ²΅ν„° μ •κ·ν™”λ΅ μ§€μΉ­
<br><br><br>μ‚¬μ΄ν‚·λ°μ—μ„ μ κ³µν•λ” λ€ν‘μ μΈ Feature Scaling ν΄λμ¤
StandardScalerμ™€ MinMaxScaler
<br>StandardScaler

<br>ν‘μ¤€ν™”λ¥Ό μ‰½κ² μ§€μ›ν•κΈ° μ„ν• ν΄λμ¤
<br>μ¦‰, κ°λ³„ featureλ¥Ό ν‰κ· μ΄ 0μ΄κ³ , λ¶„μ‚°μ΄ 1μΈ κ°’μΌλ΅ λ³€ν™
<br>κ°€μ°μ‹μ• μ •κ· λ¶„ν¬λ¥Ό κ°€μ§ μ μλ„λ΅ λ°μ΄ν„°λ¥Ό λ³€ν™ν•λ” κ²ƒμ€ λ‡λ‡ μ•κ³ λ¦¬μ¦μ—μ„ λ§¤μ° μ¤‘μ”!

<br>SVM, μ„ ν•νκ·€, λ΅μ§€μ¤ν‹± νκ·€λ” λ°μ΄ν„°κ°€ κ°€μ°μ‹μ• λ¶„ν¬λ¥Ό κ°€μ§€κ³  μλ‹¤κ³  κ°€μ •ν•κ³  κµ¬ν„λκΈ° λ•λ¬Έμ— μ‚¬μ „μ— ν‘μ¤€ν™”λ¥Ό μ μ©ν•λ” κ²ƒμ€ μμΈ΅ μ„±λ¥ ν–¥μƒμ— μ¤‘μ”ν• μ”μ†κ°€ λ  μ μμ



<br>from sklearn.datasets import load_iris
import pandas as pd

# λ¶“κ½ƒ λ°μ΄ν„° μ…‹μ„ λ΅λ”©ν•κ³  DataFrameμΌλ΅ λ³€ν™ν•©λ‹λ‹¤.
iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print('feature λ“¤μ ν‰κ·  κ°’')
print(iris_df.mean())
print('\nfeature λ“¤μ λ¶„μ‚° κ°’')
print(iris_df.var())
<br><img alt="0212171840210174.jpg" src="lib\media\0212171840210174.jpg"><br>
<br>μ΄μ  StandardScalerλ¥Ό μ΄μ©ν•΄ κ° featureλ¥Ό ν• λ²μ— ν‘μ¤€ν™”
<br>StandardScaler κ°μ²΄λ¥Ό μƒμ„±ν• ν›„μ— fit()κ³Ό transform() λ©”μ„λ“μ— λ³€ν™ λ€μƒ feature λ°μ΄ν„° μ„ΈνΈλ¥Ό μ…λ ¥ν•κ³  νΈμ¶ν•λ©΄ κ°„λ‹¨ν•κ² λ³€ν™
<br>transform()μ„ νΈμ¶ν•  λ• μ¤μΌ€μΌ λ³€ν™λ λ°μ΄ν„° μ„ΈνΈλ” λ„νμ΄μ ndarrayμ΄λ―€λ΅ μ΄λ¥Ό DataFrameμΌλ΅ λ³€ν™ν•΄ ν‰κ· κ°’κ³Ό λ¶„μ‚° κ°’μ„ λ‹¤μ‹ ν™•μΈ
<br>from sklearn.preprocessing import StandardScaler

# StandardScalerκ°μ²΄ μƒμ„±
scaler = StandardScaler()

# StandardScaler λ΅ λ°μ΄ν„° μ…‹ λ³€ν™. fit( ) κ³Ό transform( ) νΈμ¶.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

#transform( )μ‹ scale λ³€ν™λ λ°μ΄ν„° μ…‹μ΄ numpy ndarryλ΅ λ°ν™λμ–΄ μ΄λ¥Ό DataFrameμΌλ΅ λ³€ν™
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

print('feature λ“¤μ ν‰κ·  κ°’')
print(iris_df_scaled.mean())
print('\nfeature λ“¤μ λ¶„μ‚° κ°’')
print(iris_df_scaled.var())
<br><img alt="0212172238464823.jpg" src="lib\media\0212172238464823.jpg"><br>
<br>λ¨λ“  column κ°’μ ν‰κ· μ΄ 0μ— μ•„μ£Ό κ°€κΉμ΄ κ°’μΌλ΅, λ¶„μ‚°μ€ 1μ— μ•„μ£Ό κ°€κΉμ΄ κ°’μΌλ΅ λ³€ν™λ¨
<br><br><br>MinMaxScaler

<br>μ •κ·ν™”λ¥Ό μ‰½κ² μ§€μ›ν•κΈ° μ„ν• ν΄λμ¤
<br>λ°μ΄ν„°κ°’μ„ 0κ³Ό 1 μ‚¬μ΄μ λ²”μ„ κ°’μΌλ΅ λ³€ν™

<br>μμ κ°’μ΄ μμΌλ©΄ -1μ—μ„ 1κ°’μΌλ΅ λ³€ν™


<br>λ°μ΄ν„°μ λ¶„ν¬κ°€ κ°€μ°μ‹μ• λ¶„ν¬κ°€ μ•„λ‹ κ²½μ°μ— Min, Max Scaleμ„ μ μ©ν•΄ λ³Ό μ μμ

<br>from sklearn.preprocessing import MinMaxScaler

# MinMaxScalerκ°μ²΄ μƒμ„±
scaler = MinMaxScaler()

# MinMaxScaler λ΅ λ°μ΄ν„° μ…‹ λ³€ν™. fit() κ³Ό transform() νΈμ¶.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

# transform()μ‹ scale λ³€ν™λ λ°μ΄ν„° μ…‹μ΄ numpy ndarryλ΅ λ°ν™λμ–΄ μ΄λ¥Ό DataFrameμΌλ΅ λ³€ν™
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

print('featureλ“¤μ μµμ†κ°’')
print(iris_df_scaled.min())
print('\nfeatureλ“¤μ μµλ“κ°’')
print(iris_df_scaled.max())
<br><img alt="0212173412428698.jpg" src="lib\media\0212173412428698.jpg"><br>
<br>λ¨λ“  featureμ— 0μ—μ„ 1 μ‚¬μ΄μ κ°’μΌλ΅ λ³€ν™λλ” μ¤μΌ€μΌλ§μ΄ μ μ©λμμ„ μ• μ μμ
<br>μ™ fit()κ³Ό transform()μ„ λ”°λ΅ μ‹¤ν–‰ν• κΉ?
ν›λ ¨ λ°μ΄ν„°λ¥Ό κΈ°μ¤€μΌλ΅ λ³€ν™ κΈ°μ¤€(scaling parameter)μ„ ν•™μµν•κ³ , μ΄λ¥Ό λ‚μ¤‘μ— ν…μ¤νΈ λ°μ΄ν„°μ—λ„ μ μ©ν•κΈ° μ„ν•΄μ„!

<br>ν›λ ¨ λ°μ΄ν„°μ—μ„ fit -&gt; ν•™μµλ κΈ°μ¤€μ„ ν›λ ¨ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°μ— μ μ©
<br>ν›λ ¨ λ°μ΄ν„°μ— λ€ν•΄μ„λ§ λ³€ν™ν•  κ²½μ°, fit_transform()μ„ μ‚¬μ©ν•  μ μμ§€λ§, ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°κ°€ λ¶„λ¦¬λμ–΄ μμ„ λ•λ” λ°λ“μ‹ fit()κ³Ό transform()μ„ λ”°λ΅ μ‚¬μ©ν•΄μ•Ό ν•¨!

<br><br><br>
<br>Scaler κ°μ²΄λ¥Ό μ΄μ©ν•΄ λ°μ΄ν„°μ μ¤μΌ€μΌλ§ λ³€ν™ κ³Όμ •

<br>fit()μ€ λ°μ΄ν„° λ³€ν™μ„ μ„ν• κΈ°μ¤€ μ •λ³΄ μ„¤μ •μ„ μ μ©
<br>transform()μ€ μ΄λ ‡κ² μ„¤μ •λ μ •λ³΄λ¥Ό μ΄μ©ν•΄ λ°μ΄ν„° λ³€ν™
<br>κ·Έλ¦¬κ³  fit_transform()μ€ fit()κ³Ό transform()μ„ ν• λ²μ— μ μ©ν•λ” κΈ°λ¥ μν–‰


<br>fit()κ³Ό transform()μ„ μ μ©ν•  λ• μ£Όμν•  μ !
ν•™μµ λ°μ΄ν„° μ„ΈνΈλ΅ fit()μ„ μ μ©ν•λ©΄ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅λ” λ‹¤μ‹ fit()μ„ μν–‰ν•μ§€ μ•κ³  ν•™μµ λ°μ΄ν„° μ„ΈνΈλ΅ fit()μ„ μν–‰ν• κ²°κ³Όλ¥Ό μ΄μ©ν•΄ transform() λ³€ν™μ„ μ μ©ν•΄μ•Ό ν•¨!<br>
μ¦‰, ν•™μµ λ°μ΄ν„°λ΅ fit()μ΄ μ μ©λ μ¤μΌ€μΌλ§ κΈ°μ¤€ μ •λ³΄λ¥Ό κ·Έλ€λ΅ ν…μ¤νΈ λ°μ΄ν„°μ— μ μ©
<br>
<br>λ¨λΈμ„ ν•™μµν•  λ•λ” ν›λ ¨ λ°μ΄ν„°μ—μ„ ν¨ν„΄μ„ ν•™μµν• ν›„, ν…μ¤νΈ λ°μ΄ν„°μ—μ„ κ·Έ ν¨ν„΄μ΄ μ μ μ©λλ” μ§€λ¥Ό ν‰κ°€ν•΄μ•Ό ν•¨
<br>λ”°λΌμ„ ν›λ ¨ λ°μ΄ν„°μ—μ„ ν•™μµν• κΈ°μ¤€μ„ κ·Έλ€λ΅ ν…μ¤νΈ λ°μ΄ν„°μ—λ„ μ μ©ν•΄μ•Ό μΌκ΄€μ„±μ΄ μ μ§€λ¨!
<br>
ν…μ¤νΈ λ°μ΄ν„°μ— fit()μ„ μ μ©ν•  λ• μ–΄λ– ν• λ¬Έμ  λ°μƒ?
<br>from sklearn.preprocessing import MinMaxScaler
import numpy as np

# ν•™μµ λ°μ΄ν„°λ” 0 λ¶€ν„° 10κΉμ§€, ν…μ¤νΈ λ°μ΄ν„°λ” 0 λ¶€ν„° 5κΉμ§€ κ°’μ„ κ°€μ§€λ” λ°μ΄ν„° μ„ΈνΈλ΅ μƒμ„±
# Scalerν΄λμ¤μ fit(), transform()μ€ 2μ°¨μ› μ΄μƒ λ°μ΄ν„°λ§ κ°€λ¥ν•λ―€λ΅ reshape(-1, 1)λ΅ μ°¨μ› λ³€κ²½

train_array = np.arange(0, 11).reshape(-1, 1)
test_array =  np.arange(0, 6).reshape(-1, 1)
<br>
<br>ν•™μµ λ°μ΄ν„°λ¥Ό 0λ¶€ν„° 10κΉμ§€, ν…μ¤νΈ λ°μ΄ν„°λ¥Ό 0λ¶€ν„° 5κΉμ§€ κ°’μ„ κ°€μ§€λ” ndarray
<br># MinMaxScaler κ°μ²΄μ— λ³„λ„μ feature_range νλΌλ―Έν„° κ°’μ„ μ§€μ •ν•μ§€ μ•μΌλ©΄ 0~1 κ°’μΌλ΅ λ³€ν™
scaler = MinMaxScaler()

# fit()ν•κ² λλ©΄ train_array λ°μ΄ν„°μ μµμ†κ°’μ΄ 0, μµλ“κ°’μ΄ 10μΌλ΅ μ„¤μ •.
scaler.fit(train_array)

# 1/10 scaleλ΅ train_array λ°μ΄ν„° λ³€ν™ν•¨. μ›λ³Έ 10-&gt; 1λ΅ λ³€ν™λ¨.
train_scaled = scaler.transform(train_array)

print('μ›λ³Έ train_array λ°μ΄ν„°:', np.round(train_array.reshape(-1), 2))
print('Scaleλ train_array λ°μ΄ν„°:', np.round(train_scaled.reshape(-1), 2))

&gt;&gt;&gt; μ›λ³Έ train_array λ°μ΄ν„°: [ 0 1 2 3 4 5 6 7 8 9 10]
    Scaleλ train_array λ°μ΄ν„°: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]
<br>
<br>ν•™μµ λ°μ΄ν„°μΈ train_arrayλ¶€ν„° MinMaxScalerλ¥Ό μ΄μ©ν•΄ λ³€ν™
<br># MinMaxScalerμ— test_arrayλ¥Ό fit()ν•κ² λλ©΄ μ›λ³Έ λ°μ΄ν„°μ μµμ†κ°’μ΄ 0, μµλ“κ°’μ΄ 5λ΅ μ„¤μ •λ¨
scaler.fit(test_array)

# 1/5 scaleλ΅ test_array λ°μ΄ν„° λ³€ν™ν•¨. μ›λ³Έ 5-&gt;1λ΅ λ³€ν™.
test_scaled = scaler.transform(test_array)

# test_arrayμ scale λ³€ν™ μ¶λ ¥.
print('μ›λ³Έ test_array λ°μ΄ν„°:', np.round(test_array.reshape(-1), 2))
print('Scaleλ test_array λ°μ΄ν„°:', np.round(test_scaled.reshape(-1), 2))

&gt;&gt;&gt; μ›λ³Έ test_array λ°μ΄ν„°: [0 1 2 3 4 5]
    Scaleλ test_array λ°μ΄ν„°: [0. 0.2 0.4 0.6 0.8 1. ]
<br>
<br>ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ λ³€ν™
<br>fit()μ„ νΈμ¶ν•΄ μ¤μΌ€μΌλ§ κΈ°μ¤€ μ •λ³΄λ¥Ό λ‹¤μ‹ μ μ©ν• λ’¤ transform()μ„ μν–‰ν• κ²°κ³Ό ν™•μΈ

<br>μ¶λ ¥ κ²°κ³Όλ¥Ό ν™•μΈν•λ©΄ ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°μ μ¤μΌ€μΌλ§μ΄ λ§μ§€ μ•μ!
<br>μ΄λ ‡κ² λλ©΄ ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°μ μ„λ΅ λ‹¤λ¥Έ μ›λ³Έκ°’μ΄ λ™μΌν• κ°’μΌλ΅ λ°ν™λλ” κ²°κ³Ό μ΄λ


<br>Tip
λ¨Έμ‹ λ¬λ‹ λ¨λΈμ€ ν•™μµ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ ν•™μµλκΈ° λ•λ¬Έμ— λ°λ“μ‹ ν…μ¤νΈ λ°μ΄ν„°λ” ν•™μµ λ°μ΄ν„°μ μ¤μΌ€μΌλ§ κΈ°μ¤€μ— λ”°λΌμ•Ό ν•λ©°, ν…μ¤νΈ λ°μ΄ν„°μ 1 κ°’μ€ ν•™μµ λ°μ΄ν„°μ™€ λ™μΌν•κ² 0.1 κ°’μΌλ΅ λ³€ν™λμ–΄μ•Ό ν•¨!

<br>λ”°λΌμ„, ν…μ¤νΈ λ°μ΄ν„°μ— λ‹¤μ‹ fit()μ„ μ μ©ν•΄μ„λ” μ• λλ©° ν•™μµ λ°μ΄ν„°λ΅ μ΄λ―Έ fit()μ΄ μ μ©λ Scaler κ°μ²΄λ¥Ό μ΄μ©ν•΄ transform()μΌλ΅ λ³€ν™ν•΄μ•Ό ν•¨

<br>
<br>λ‹¤μ μ½”λ“λ” ν…μ¤νΈ λ°μ΄ν„°μ— fit()μ„ νΈμ¶ν•μ§€ μ•κ³  ν•™μµ λ°μ΄ν„°λ΅ fit()μ„ μν–‰ν• MinMaxScaler κ°μ²΄μ transform()μ„ μ΄μ©ν•΄ λ°μ΄ν„° λ³€ν™
<br>scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)

print('μ›λ³Έ train_array λ°μ΄ν„°:', np.round(train_array.reshape(-1), 2))
print('Scaleλ train_array λ°μ΄ν„°:', np.round(train_scaled.reshape(-1), 2))

# test_arrayμ— Scale λ³€ν™μ„ ν•  λ•λ” λ°λ“μ‹ fit()μ„ νΈμ¶ν•μ§€ μ•κ³  transform() λ§μΌλ΅ λ³€ν™ν•΄μ•Ό ν•¨.
test_scaled = scaler.transform(test_array)

print('\nμ›λ³Έ test_array λ°μ΄ν„°:', np.round(test_array.reshape(-1), 2))
print('Scaleλ test_array λ°μ΄ν„°:', np.round(test_scaled.reshape(-1), 2))
<br><img alt="Pasted image 20250212204025.png" src="lib\media\pasted-image-20250212204025.png"><br>
<br>fit_transform()μ€ fit()κ³Ό transform()μ„ μμ°¨μ μΌλ΅ μν–‰ν•λ” λ©”μ„λ“μ΄λ―€λ΅ ν•™μµ λ°μ΄ν„°μ—μ„λ” μƒκ΄€μ—†μ§€λ§ ν…μ¤νΈ λ°μ΄ν„°μ—μ„λ” μ λ€ μ‚¬μ©ν•΄μ„λ” μ•λ¨!
<br>Tip
ν•™μµκ³Ό ν…μ¤νΈ λ°μ΄ν„°μ— fit()κ³Ό transform()μ„ μ μ©ν•  λ• μ£Όμ μ‚¬ν•­μ΄ λ°μƒν•λ―€λ΅ ν•™μµκ³Ό ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ λ¶„λ¦¬ν•κΈ° μ „μ— λ¨Όμ € μ „μ²΄ λ°μ΄ν„° μ„ΈνΈμ— μ¤μΌ€μΌλ§μ„ μ μ©ν• λ’¤ ν•™μµκ³Ό ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ λ¶„λ¦¬ν•λ” κ²ƒμ΄ λ” λ°”λμ§!
<br>Summary
1. κ°€λ¥ν•λ‹¤λ©΄ μ „μ²΄ λ°μ΄ν„°μ μ¤μΌ€μΌλ§ λ³€ν™μ„ μ μ©ν• λ’¤ ν•™μµκ³Ό ν…μ¤νΈ λ°μ΄ν„°λ΅ λ¶„λ¦¬
2. 1μ΄ μ—¬μμΉ μ•λ‹¤λ©΄ ν…μ¤νΈ λ°μ΄ν„° λ³€ν™ μ‹μ—λ” fit()μ΄λ‚ fit_transform()μ„ μ μ©ν•μ§€ μ•κ³  ν•™μµ λ°μ΄ν„°λ΅ μ΄λ―Έ fit()λ Scaler κ°μ²΄λ¥Ό μ΄μ©ν•΄ transform()μΌλ΅ λ³€ν™
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(5)-λ°μ΄ν„°-μ „μ²λ¦¬.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(5) λ°μ΄ν„° μ „μ²λ¦¬.md</guid><pubDate>Thu, 13 Feb 2025 10:26:00 GMT</pubDate><enclosure url="lib\media\pasted-image-20250212151043.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250212151043.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(6) μ‚¬μ΄ν‚·λ°μΌλ΅ μν–‰ν•λ” νƒ€μ΄νƒ€λ‹‰ μƒμ΅΄μ μμΈ΅]]></title><description><![CDATA[ 
 <br>μΊκΈ€ νƒ€μ΄νƒ€λ‹‰ νƒ‘μΉμ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ μƒμ΅΄μ μμΈ΅

<br>Passengerid : νƒ‘μΉμ λ°μ΄ν„° μΌλ ¨λ²νΈ
<br>survived : μƒμ΅΄ μ—¬λ¶€ (0:μ‚¬λ§, 1:μƒμ΅΄)
<br>pclass : ν‹°μΌ“μ μ„ μ‹¤ λ“±κΈ‰ (1:μΌλ“±μ„, 2:μ΄λ“±μ„, 3:μ‚Όλ“±μ„)
<br>sex : νƒ‘μΉμ μ„±λ³„
<br>name : νƒ‘μΉμ μ΄λ¦„
<br>Age : νƒ‘μΉμ λ‚μ΄
<br>sibsp : κ°™μ΄ νƒ‘μΉν• ν•μ μλ§¤ λλ” λ°°μ°μ μΈμ›μ
<br>parch : κ°™μ΄ νƒ‘μΉν• λ¶€λ¨λ‹ λλ” μ–΄λ¦°μ΄ μΈμ›μ
<br>ticket : ν‹°μΌ“ λ²νΈ
<br>fare : μ”κΈ
<br>cabin : μ„ μ‹¤ λ²νΈ
<br>embarked : μ¤‘κ°„ μ •μ°© ν•­κµ¬ (C:Cherbourg, Q:Queenstown, S:Southhampton)

<br><br><br>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

titanic_df = pd.read_csv('train.csv')
titanic_df.head(3)
<br><img alt="Pasted image 20250213151504.png" src="lib\media\pasted-image-20250213151504.png"><br><br>print('\n ### train λ°μ΄ν„° μ •λ³΄ ###  \n')
print(titanic_df.info())
<br><img alt="Pasted image 20250213151614.png" src="lib\media\pasted-image-20250213151614.png"><br>
<br>RangeIndexλ” DataFrame μΈλ±μ¤ λ²”μ„λ¥Ό λ‚νƒ€λ‚΄λ―€λ΅ μ „μ²΄ row μλ¥Ό μ• μ μμ

<br>ν„μ¬ λ°μ΄ν„°λ” 891κ° rowλ΅ κµ¬μ„±λμ–΄ μμ
<br>column μλ” 12κ°
<br>2κ°μ columnμ΄ float64
<br>5κ°μ columnμ΄ int64
<br>5κ°μ columnμ΄ object (νλ‹¤μ¤μ object νƒ€μ…μ€ stringνƒ€μ…μΌλ΅ λ΄λ„ λ¬΄λ°©)


<br>Age, Cabin, Embarked μΉΌλΌμ€ κ°κ° 714κ°, 204κ°, 889κ°μ Not Null κ°’μ„ κ°€μ§€κ³  μμΌλ―€λ΅ κ°κ° 117κ°, 608κ°, 2κ°μ Null κ°’(NaN)μ„ κ°€μ§€κ³  μμ
<br><br><br>Note
μ‚¬μ΄ν‚·λ° λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ€ Null κ°’μ„ ν—μ©ν•μ§€ μ•μΌλ―€λ΅ Null κ°’μ„ μ–΄λ–»κ² μ²λ¦¬ν• μ§€ κ²°μ •ν•΄μ•Ό ν•¨
<br>
<br>μ—¬κΈ°μ„λ” DataFrameμ fillna() ν•¨μλ¥Ό μ‚¬μ©ν•΄ κ°„λ‹¨ν•κ² Null κ°’μ„ ν‰κ·  λλ” κ³ μ • κ°’μΌλ΅ λ³€κ²½
<br>titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].mean())
titanic_df['Cabin'] = titanic_df['Cabin'].fillna('N')
titanic_df['Embarked'] = titanic_df['Embarked'].fillna('N')
print('λ°μ΄ν„° μ„ΈνΈ Null κ°’ κ°―μ ',titanic_df.isnull().sum().sum())

&gt;&gt;&gt; λ°μ΄ν„° μ„ΈνΈ Null κ°’ κ°―μ 0
<br>
<br>Cabinκ³Ό Embardedλ” λ²”μ£Όν• λ°μ΄ν„°λ΅ κ²°μΈ΅μΉλ¥Ό λ‹¨μν μ κ±°ν•κ±°λ‚ ν‰κ· μΌλ΅ λ€μ²΄ν•  μ μ—†κΈ° λ•λ¬Έμ— ν•΄λ‹Ή κ°’μ΄ μ›λ μ—†μ—λ‹¤λ” κ²ƒμ„ λ…ν™•ν ν•κΈ° μ„ν•μ—¬ NμΌλ΅ λ€μ²΄
<br><br><br>print(' Sex κ°’ λ¶„ν¬ :\n',titanic_df['Sex'].value_counts())
print('\n Cabin κ°’ λ¶„ν¬ :\n',titanic_df['Cabin'].value_counts())
print('\n Embarked κ°’ λ¶„ν¬ :\n',titanic_df['Embarked'].value_counts())
<br><img alt="Pasted image 20250213153150.png" src="lib\media\pasted-image-20250213153150.png"><br>
<br>Sex, Embarked κ°’μ€ λ³„ λ¬Έμ κ°€ μ—†μΌλ‚, Cabin(μ„ μ‹¤)μ κ²½μ° Nμ΄ 687κ±΄μΌλ΅ κ°€μ¥ λ§μ€ κ²ƒλ„ νΉμ΄ν•μ§€λ§, μ†μ„± κ°’μ΄ μ λ€λ΅ μ •λ¦¬κ°€ λμ§€ μ•μ€ κ²ƒμ²λΌ λ³΄μ„<br>
- C23 C25 C27κ³Ό κ°™μ΄ μ—¬λ¬ Cabinμ΄ ν•κΊΌλ²μ— ν‘κΈ°λ κ°’μ΄ 4κ±΄μ΄λ‚ λκ³ , Cabinμ κ²½μ° μ„ μ‹¤ λ²νΈ μ¤‘ μ„ μ‹¤ λ“±κΈ‰μ„ λ‚νƒ€λ‚΄λ” μ²« λ²μ§Έ μ•νλ²³μ΄ μ¤‘μ”ν•΄ λ³΄μ„
Tip
μ™λƒν•λ©΄ μ΄ μ‹μ μ—λ” μ§€κΈλ³΄λ‹¤λ„ λ¶€μμ™€ κ°€λ‚ν• μ‚¬λμ— λ€ν• μ°¨λ³„μ΄ λ” μλ μ‹μ μ΄μ—κΈ°μ— μΌλ“±μ‹¤μ— ν¬μ™ν• μ‚¬λμ΄ μ‚Όλ“±μ‹¤μ— ν¬μ™ν• μ‚¬λλ³΄λ‹¤ λ” μ‚΄μ•„λ‚  ν™•λ¥ μ΄ λ†’μ•μ„ κ²ƒ!


<br>λ”°λΌμ„ Cabinμ κ²½μ° μ• λ¬Έμλ§ μ¶”μ¶!
<br>titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
print(titanic_df['Cabin'].head(3))

&gt;&gt;&gt; 0  N
    1  C
    2  N
    Name: Cabin, dtype: object
<br><br><br><br>Tip
λ°”λ‹¤μ—μ„ μ‚¬κ³ κ°€ λ‚  κ²½μ° μ—¬μ„±κ³Ό μ•„μ΄λ“¤ κ·Έλ¦¬κ³  λ…Έμ•½μκ°€ μ μΌ λ¨Όμ € κµ¬μ΅° λ€μƒ!<br>
κ·Έλ¦¬κ³  μ•„λ§λ„ λ¶€μλ‚ μ λ…μΈμ΄ λ‹¤μ κµ¬μ΅° λ€μƒμ΄μ—μ„ κ²ƒ<br>
μμƒμ»¨λ° μ‚Όλ“±μ‹¤μ— νƒ„ λ§μ€ κ°€λ‚ν• μ΄λ” μƒμ΅΄ν•μ§€ λ»ν•  ν™•λ¥ μ΄ λ†’μ„ κ²ƒ
<br><br>titanic_df.groupby(['Sex','Survived'])['Survived'].count()
<br><img alt="Pasted image 20250213155100.png" src="lib\media\pasted-image-20250213155100.png"><br>
<br>Survived μΉΌλΌμ€ labelλ΅μ„ κ²°μ • ν΄λμ¤ κ°’
<br>νƒ‘μΉκ°μ€ λ‚¨μκ°€ 577λ…, μ—¬μκ°€ 314λ…μΌλ΅ λ‚¨μκ°€ λ” λ§μ•μ
<br>μ—¬μλ” 314λ… μ¤‘ 233λ…μΌλ΅ μ•½ 74.2%κ°€ μƒμ΅΄ν–μ§€λ§, λ‚¨μμ κ²½μ°μ—λ” 577λ… μ¤‘ 468λ…μ΄ μ£½κ³  109λ…λ§ μ‚΄μ•„λ‚¨μ•„ μ•½ 18.8%κ°€ μƒμ΅΄
<br>κ·Έλν”„λ΅ ν™•μΈ<br>
sns.barplot(x='Sex', y='Survived', data=titanic_df)
<br>κΈ°λ³Έμ μΈ μ‘λ™ λ°©μ‹

<br>x='Sex' : Sex λ³„λ΅ κ·Έλ£Ήν™” (male, female)
<br>y='Survived' : Survived κ°’μ ν‰κ·  κ³„μ‚°
<br>λ§‰λ€ λ†’μ΄ : κ·Έλ£Ήλ³„ Survived ν‰κ· <br>
- Survived μ»¬λΌμ€ 0 λλ” 1μ΄λ―€λ΅, ν‰κ· κ°’μ΄ κ³§ μƒμ΅΄ ν™•λ¥ <br>
<img alt="Pasted image 20250213163711.png" src="lib\media\pasted-image-20250213163711.png">


<br><br>
<br>λ¶€λ¥Ό μΈ΅μ •ν•  μ μλ” μ†μ„±μΌλ΅ μ λ‹Ήν• κ²ƒμ€ κ°μ‹¤ λ“±κΈ‰
<br>κ°μ‹¤ λ“±κΈ‰ λ³„ μ„±λ³„μ— λ”°λ¥Έ μƒμ΅΄ ν™•λ¥  λΉ„κµ
<br>sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)<br>
<img alt="Pasted image 20250213164010.png" src="lib\media\pasted-image-20250213164010.png">

<br>hue νλΌλ―Έν„° : λ°μ΄ν„°λ¥Ό λ λ‹¤λ¥Έ κΈ°μ¤€μΌλ΅ κ·Έλ£Ήν™”ν•μ—¬ μƒ‰μƒλ³„λ΅ λ‚λ„μ–΄ ν‘ν„

<br>μ¦‰, κ°™μ€ xκ°’μ— λ€ν•΄ μ„λΈ κ·Έλ£Ήμ„ λ§λ“¤μ–΄μ„ μ—¬λ¬ κ°μ λ§‰λ€λ¥Ό λ‚λ€ν ν‘μ‹ν•λ” κΈ°λ¥




<br>μ—¬μ„±μ κ²½μ° μΌ, μ΄λ“±μ‹¤μ— λ”°λ¥Έ μƒμ΅΄ ν™•λ¥ μ μ°¨μ΄λ” ν¬μ§€ μ•μΌλ‚, μ‚Όλ“±μ‹¤μ κ²½μ° μƒμ΅΄ ν™•λ¥ μ΄ μƒλ€μ μΌλ΅ λ§μ΄ λ–¨μ–΄μ§
<br>λ‚¨μ„±μ κ²½μ° μΌλ“±μ‹¤μ μƒμ΅΄ ν™•λ¥ μ΄ μ΄, μ‚Όλ“±μ‹¤μ μƒμ΅΄ ν™•λ¥ λ³΄λ‹¤ μ›”λ“±ν λ†’μ
<br><br>Tip
Ageμ κ²½μ° κ°’ μΆ…λ¥κ°€ λ§κΈ° λ•λ¬Έμ— λ²”μ„λ³„λ΅ λ¶„λ¥ν•΄ μΉ΄ν…κ³ λ¦¬ κ°’ ν• λ‹Ή

<br>0~5μ„Έ : Baby
<br>6~12μ„Έ : Child
<br>13~18μ„Έ : Teenager
<br>19~25μ„Έ : Student
<br>26~35μ„Έ : Yiung Adult
<br>36~60μ„Έ : Adult
<br>61μ„Έ μ΄μƒ : Elderly
<br>-1 μ΄ν•μ μ¤λ¥ κ°’μ€ UnknownμΌλ΅ λ¶„λ¥

<br># μ…λ ¥ ageμ— λ”°λΌ κµ¬λ¶„κ°’μ„ λ°ν™ν•λ” ν•¨μ μ„¤μ •. DataFrameμ apply lambdaμ‹μ— μ‚¬μ©.
def get_category(age):
    cat = ''
    if age &lt;= -1: cat = 'Unknown'
    elif age &lt;= 5: cat = 'Baby'
    elif age &lt;= 12: cat = 'Child'
    elif age &lt;= 18: cat = 'Teenager'
    elif age &lt;= 25: cat = 'Student'
    elif age &lt;= 35: cat = 'Young Adult'
    elif age &lt;= 60: cat = 'Adult'
    else : cat = 'Elderly'
    return cat

# λ§‰λ€κ·Έλν”„μ ν¬κΈ° figureλ¥Ό λ” ν¬κ² μ„¤μ •
plt.figure(figsize=(10,6))

# Xμ¶•μ κ°’μ„ μμ°¨μ μΌλ΅ ν‘μ‹ν•κΈ° μ„ν• μ„¤μ •
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']

# lambda μ‹μ— μ„μ—μ„ μƒμ„±ν• get_category( ) ν•¨μλ¥Ό λ°ν™κ°’μΌλ΅ μ§€μ •.
# get_category(X)λ” μ…λ ¥κ°’μΌλ΅ 'Age' μ»¬λΌκ°’μ„ λ°›μ•„μ„ ν•΄λ‹Ήν•λ” cat λ°ν™
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
sns.barplot(x='Age_cat', y = 'Survived', hue='Sex', data=titanic_df, order=group_names)
titanic_df.drop('Age_cat', axis=1, inplace=True)
<br><img alt="Pasted image 20250213165046.png" src="lib\media\pasted-image-20250213165046.png"><br>
<br>μ—¬μ Babyμ κ²½μ° λΉ„κµμ  μƒμ΅΄ ν™•λ¥ μ΄ λ†’μ•μ§€λ§, μ—¬μ Childμ κ²½μ° λ‹¤λ¥Έ μ—°λ Ήλ€μ— λΉ„ν•΄ μƒμ΅΄ ν™•λ¥ μ΄ λ‚®μ
<br>μ—¬μ Elderlyμ κ²½μ°λ” λ§¤μ° μƒμ΅΄ ν™•λ¥ μ΄ λ†’μ•μ
<br>Summary
μ΄μ κΉμ§€  λ¶„μ„ν• κ²°κ³Ό Sex, Age, PClass λ“±μ΄ μ¤‘μ”ν•κ² μƒμ΅΄μ„ μΆμ°ν•λ” featureμ„μ„ μ–΄λ μ •λ„ ν™•μΈ κ°€λ¥
<br><br><br>Note

<br>μΈμ½”λ”©μ€ μ‚¬μ΄ν‚·λ°μ LabelEncoder ν΄λμ¤λ¥Ό μ΄μ”ν•΄ λ μ΄λΈ” μΈμ½”λ”© μ μ©

<br>λ μ΄λΈ” μΈμ½”λ”©μ€ κ° λ²”μ£Όλ¥Ό μ •μν•μΌλ΅ λ³€ν™ν•λ” λ°©μ‹


<br>LabelEncoder κ°μ²΄λ” μΉ΄ν…κ³ λ¦¬ κ°’μ μ ν• μμ— λ”°λΌ 0 ~ (μΉ΄ν…κ³ λ¦¬ μ ν• μ -1)κΉμ§€μ μ«μ κ°’μΌλ΅ λ³€ν™
<br>μ‚¬μ΄ν‚·λ°μ μ „μ²λ¦¬ λ¨λ“μ λ€λ¶€λ¶„ μΈμ½”λ”© APIλ” μ‚¬μ΄ν‚·λ°μ κΈ°λ³Έ ν”„λ μ„μ›ν¬ APIμΈ fit(), transform()μΌλ΅ λ°μ΄ν„° λ³€ν™

<br>
<br>μ—¬λ¬ μΉΌλΌμ„ encode_features() ν•¨μλ¥Ό μƒλ΅ μƒμ„±ν•΄ ν• λ²μ— λ³€ν™
<br>from sklearn.preprocessing import LabelEncoder

def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
    return dataDF

titanic_df = encode_features(titanic_df)
titanic_df.head()
<br><img alt="Pasted image 20250213171432.png" src="lib\media\pasted-image-20250213171432.png"><br>
<br>Sex, Cabin, Embarked μ†μ„±μ΄ μ«μν•μΌλ΅ λ°”λ€
<br><br><br>Note
μ§€κΈκΉμ§€ featureλ¥Ό κ°€κ³µν• λ‚΄μ—­μ„ μ •λ¦¬ν•κ³  μ΄λ¥Ό ν•¨μλ΅ λ§λ“¤μ–΄ μ‰½κ² μ¬μ‚¬μ©ν•  μ μλ„λ΅ κµ¬μ„±<br>
transform_features() 

<br>Null μ²λ¦¬, λ¶ν•„μ”ν• feature μ κ±°, μΈμ½”λ”©μ„ μν–‰ν•λ” λ‚΄λ¶€ ν•¨μλ΅ κµ¬μ„±
<br>λ¶ν•„μ”ν• feature μ κ±°λ” drop_features(df)λ΅ μν–‰ν•λ©° λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ— λ¶ν•„μ”ν•, λ‹¨μν• μ‹λ³„μ μμ¤€μ featureμΈ PassengerId, Name, Ticket feature μ κ±°

<br>from sklearn.preprocessing import LabelEncoder

# Null μ²λ¦¬ ν•¨μ
def fillna(df):
    df['Age'] = df['Age'].fillna(df['Age'].mean())
    df['Cabin'] = df['Cabin'].fillna('N')
    df['Embarked'] = df['Embarked'].fillna('N')
    df['Fare'] = df['Fare'].fillna(0)
    return df

# λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ— λ¶ν•„μ”ν• ν”Όμ² μ κ±°
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

# λ μ΄λΈ” μΈμ½”λ”© μν–‰.
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# μ•μ—μ„ μ„¤μ •ν• λ°μ΄ν„° μ „μ²λ¦¬ ν•¨μ νΈμ¶
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
<br><br><br>Todo

<br>μ›λ³Έ CSV νμΌμ„ λ‹¤μ‹ λ΅λ”©ν•κ³  label κ°’μΈ Survived μ†μ„±λ§ λ³„λ„ λ¶„λ¦¬ν•΄ ν΄λμ¤ κ²°μ •κ°’ λ°μ΄ν„° μ„ΈνΈλ΅ λ§λ“¤κΈ°
<br>κ·Έλ¦¬κ³  Survived μ†μ„±μ„ drop ν•΄ feature λ°μ΄ν„° μ„ΈνΈ λ§λ“¤κΈ°
<br>μ΄λ ‡κ² μƒμ„±λ feature λ°μ΄ν„° μ„ΈνΈμ— transform_features()λ¥Ό μ μ©ν•΄ λ°μ΄ν„° κ°€κ³µ

<br># μ›λ³Έ λ°μ΄ν„°λ¥Ό μ¬λ΅λ”© ν•κ³ , featureλ°μ΄ν„° μ…‹κ³Ό Label λ°μ΄ν„° μ…‹ μ¶”μ¶.
titanic_df = pd.read_csv('train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived',axis=1)

X_titanic_df = transform_features(X_titanic_df)
<br>
<br>λ‚΄λ ¤λ°›μ€ ν•™μµ λ°μ΄ν„° μ„ΈνΈλ¥Ό κΈ°λ°μΌλ΅ train_test_split()λ¥Ό μ΄μ©ν•΄ λ³„λ„μ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ μ¶”μ¶
<br>ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈ ν¬κΈ°λ” μ „μ²΄μ 20%
<br>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df,
                                                  test_size=0.2, random_state=11)
<br><br><br>Note
ML μ•κ³ λ¦¬μ¦μΈ κ²°μ • νΈλ¦¬, λλ¤ ν¬λ μ¤νΈ, λ΅μ§€μ¤ν‹± νκ·€λ¥Ό μ΄μ©ν•΄ νƒ€μ΄νƒ€λ‹‰ μƒμ΅΄μ μμΈ΅<br>
μ—¬κΈ°μ„λ” μ‚¬μ΄ν‚·λ° κΈ°λ°μ λ¨Έμ‹ λ¬λ‹ μ½”λ“μ— μµμ™ν•΄μ§€λ” κ²ƒμ„ λ©ν‘!

<br>κ²°μ • νΈλ¦¬ : DecisionTreeClassifier
<br>λλ¤ ν¬λ μ¤νΈ : RandomForestClassifier
<br>λ΅μ§€μ¤ν‹± νκ·€ : LogisticRegression

ν•µμ‹¬ κ³Όμ •

<br>μ‚¬μ΄ν‚·λ° ν΄λμ¤λ¥Ό μ΄μ©ν•΄ train_test_split()μΌλ΅ λ¶„λ¦¬ν• ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ λ¨Έμ‹ λ¬λ‹ λ¨λΈμ„ ν•™μµν•κ³ (fit), μ—μΈ΅(predict)
<br>μμΈ΅ μ„±λ¥ ν‰κ°€λ” μ •ν™•λ„λ΅ ν•  κ²ƒμ΄λ©° μ΄λ¥Ό μ„ν•΄ accuracy_score() API μ‚¬μ©

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# κ²°μ •νΈλ¦¬, Random Forest, λ΅μ§€μ¤ν‹± νκ·€λ¥Ό μ„ν• μ‚¬μ΄ν‚·λ° Classifier ν΄λμ¤ μƒμ„±
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression(solver='liblinear')

# DecisionTreeClassifier ν•™μµ/μμΈ΅/ν‰κ°€
dt_clf.fit(X_train , y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier μ •ν™•λ„: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

# RandomForestClassifier ν•™μµ/μμΈ΅/ν‰κ°€
rf_clf.fit(X_train , y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier μ •ν™•λ„:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))

# LogisticRegression ν•™μµ/μμΈ΅/ν‰κ°€
lr_clf.fit(X_train , y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression μ •ν™•λ„: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))
<br><img alt="Pasted image 20250213175649.png" src="lib\media\pasted-image-20250213175649.png"><br>
<br>3κ°μ μ•κ³ λ¦¬μ¦ μ¤‘ LogisticRegressionμ΄ νƒ€ μ•κ³ λ¦¬μ¦μ— λΉ„ν•΄ λ†’μ€ μ •ν™•λ„λ¥Ό λ‚νƒ€λ‚΄κ³  μμ
<br>ν•μ§€λ§ μ•„μ§ μµμ ν™” μ‘μ—…μ„ μν–‰ν•μ§€ μ•μ•κ³ , λ°μ΄ν„° μ–‘λ„ μ¶©λ¶„ν•μ§€ μ•κΈ° λ•λ¬Έμ— μ–΄λ–¤ μ•κ³ λ¦¬μ¦μ΄ κ°€μ¥ μ„±λ¥μ΄ μΆ‹λ‹¤κ³  ν‰κ°€ν•  μλ” μ—†μ
<br><br>Note
μ‚¬μ΄ν‚·λ° model_selection ν¨ν‚¤μ§€μ KFold ν΄λμ¤, cross_val_score(), GridSearchCV ν΄λμ¤λ¥Ό λ¨λ‘ μ‚¬μ©!
<br><br>
<br>fold κ°μλ” 5κ°λ΅ μ„¤μ •
<br>from sklearn.model_selection import KFold

def exec_kfold(clf, folds=5):
    # ν΄λ“ μ„ΈνΈλ¥Ό 5κ°μΈ KFoldκ°μ²΄λ¥Ό μƒμ„±, ν΄λ“ μλ§νΌ μμΈ΅κ²°κ³Ό μ €μ¥μ„ μ„ν•  λ¦¬μ¤νΈ κ°μ²΄ μƒμ„±.
    kfold = KFold(n_splits=folds)
    scores = []

    # KFold κµμ°¨ κ²€μ¦ μν–‰.
    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        # X_titanic_df λ°μ΄ν„°μ—μ„ κµμ°¨ κ²€μ¦λ³„λ΅ ν•™μµκ³Ό κ²€μ¦ λ°μ΄ν„°λ¥Ό κ°€λ¦¬ν‚¤λ” index μƒμ„±
        X_train, X_test = X_titanic_df.values[train_index],          
                          X_titanic_df.values[test_index]

        y_train, y_test = y_titanic_df.values[train_index],  
			              y_titanic_df.values[test_index]

        # Classifier ν•™μµ, μμΈ΅, μ •ν™•λ„ κ³„μ‚°
        clf.fit(X_train, y_train)
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        scores.append(accuracy)
        print("κµμ°¨ κ²€μ¦ {0} μ •ν™•λ„: {1:.4f}".format(iter_count, accuracy))    

    # 5κ° foldμ—μ„μ ν‰κ·  μ •ν™•λ„ κ³„μ‚°.
    mean_score = np.mean(scores)
    print("ν‰κ·  μ •ν™•λ„: {0:.4f}".format(mean_score))

# exec_kfold νΈμ¶
exec_kfold(dt_clf , folds=5)
<br>β€Pasted image 20250214163848.pngβ€ μ„ μ°Ύμ§€ λ»ν–μµλ‹λ‹¤.<br>enumerate()
λ°λ³µ(iterable) κ°μ²΄λ¥Ό μνν•  λ•, ν„μ¬ λ‡ λ²μ§Έ λ°λ³µμΈμ§€(index)λ¥Ό ν•¨κ» λ°ν™ν•λ” λ‚΄μ¥ ν•¨μ
<br>
<br>ν‰κ·  μ •ν™•λ„λ” μ•½ 78.23%
<br><br>from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)

for iter_count, accuracy in enumerate(scores):
	print("κµμ°¨ κ²€μ¦ {0} μ •ν™•λ„: {1:.4f}".format(iter_count, accuracy))

print("ν‰κ·  μ •ν™•λ„: {0:.4f}".format(np.mean(scores)))
<br>β€Pasted image 20250214165844.pngβ€ μ„ μ°Ύμ§€ λ»ν–μµλ‹λ‹¤.<br>cross_val_score()μ™€ λ°©κΈ μ „ k foldμ ν‰κ·  μ •ν™•λ„λ” μ™ λ‹¤λ¥ΌκΉ?
cross_val_score()κ°€ StratifiedKFoldλ¥Ό μ΄μ©ν•΄ fold μ„ΈνΈλ¥Ό λ¶„ν• ν•κΈ° λ•λ¬Έ!
<br><br>from sklearn.model_selection import GridSearchCV

parameters = {'max_depth':[2,3,5,10],
             'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}

grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_dclf.fit(X_train , y_train)

print('GridSearchCV μµμ  ν•μ΄νΌ νλΌλ―Έν„° :',grid_dclf.best_params_)
print('GridSearchCV μµκ³  μ •ν™•λ„: {0:.4f}'.format(grid_dclf.best_score_))
best_dclf = grid_dclf.best_estimator_

# GridSearchCVμ μµμ  ν•μ΄νΌ νλΌλ―Έν„°λ΅ ν•™μµλ Estimatorλ΅ μμΈ΅ λ° ν‰κ°€ μν–‰.
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('ν…μ¤νΈ μ„ΈνΈμ—μ„μ DecisionTreeClassifier μ •ν™•λ„ : {0:.4f}'.format(accuracy))
<br>β€Pasted image 20250214165142.pngβ€ μ„ μ°Ύμ§€ λ»ν–μµλ‹λ‹¤.<br>
<br>μµμ ν™”λ ν•μ΄νΌ νλΌλ―Έν„°μΈ max_depth=3, min_samples_leaf=5, min_samples_split=2λ΅ DecisionTreeClassifierλ¥Ό ν•™μµμ‹ν‚¨ λ’¤ μμΈ΅ μ •ν™•λ„κ°€ μ•½ 87.15%λ΅ ν–¥μƒλ¨
<br>ν•μ§€λ§, μΌλ°μ μΌλ΅ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό νλ‹ν•λ”λΌλ„ μ΄ μ •λ„ μμ¤€μΌλ΅ μ¦κ°€ν•κΈ°λ” λ§¤μ° μ–΄λ ¤μ›€
<br>ν…μ¤νΈμ© λ°μ΄ν„° μ„ΈνΈκ°€ μ‘κΈ° λ•λ¬Έμ— μμΉμƒμΌλ΅ μμΈ΅ μ„±λ¥μ΄ λ§μ΄ μ¦κ°€ν• κ²ƒμ²λΌ λ³΄μ„
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(6)-μ‚¬μ΄ν‚·λ°μΌλ΅-μν–‰ν•λ”-νƒ€μ΄νƒ€λ‹‰-μƒμ΅΄μ-μμΈ΅.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(6) μ‚¬μ΄ν‚·λ°μΌλ΅ μν–‰ν•λ” νƒ€μ΄νƒ€λ‹‰ μƒμ΅΄μ μμΈ΅.md</guid><pubDate>Fri, 14 Feb 2025 09:40:45 GMT</pubDate><enclosure url="lib\media\pasted-image-20250213151504.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250213151504.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(7) μ •λ¦¬]]></title><description><![CDATA[ 
 <br>
<br>
μ‚¬μ΄ν‚·λ°μ€ λ§¤μ° λ§μ€ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ„ μ κ³µν•  λΏλ§ μ•„λ‹λΌ, μ‰½κ³  μ§κ΄€μ μΈ API ν”„λ μ„μ›ν¬, νΈλ¦¬ν•κ³  λ‹¤μ–‘ν• λ¨λ“ μ§€μ› λ“±μΌλ΅ νμ΄μ¬ κ³„μ—΄μ λ€ν‘μ μΈ λ¨Έμ‹ λ¬λ‹ ν¨ν‚¤μ§€

<br>
λ¨Έμ‹ λ¬λ‹ μ• ν”λ¦¬μΌ€μ΄μ…μ€ λ°μ΄ν„°μ κ°€κ³µ λ° λ³€ν™ κ³Όμ •μ μ „μ²λ¦¬ μ‘μ—…, λ°μ΄ν„°λ¥Ό ν•™μµ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„°λ΅ λ¶„λ¦¬ν•λ” λ°μ΄ν„° μ„ΈνΈ λ¶„λ¦¬ μ‘μ—…μ„ κ±°μΉ ν›„μ— ν•™μµ λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ„ μ μ©ν•΄ λ¨λΈμ„ ν•™μµ

<br>κ·Έλ¦¬κ³  ν•™μµλ λ¨λΈμ„ κΈ°λ°μΌλ΅ ν…μ¤νΈ λ°μ΄ν„°μ— λ€ν• μμΈ΅μ„ μν–‰ν•κ³ , μ΄λ ‡κ² μμΈ΅λ κ²°κ΄κ°’μ„ μ‹¤μ  κ²°κ΄κ°’κ³Ό λΉ„κµν•΄ λ¨Έμ‹ λ¬λ‹ λ¨λΈμ— λ€ν• ν‰κ°€λ¥Ό μν–‰ν•λ” λ°©μ‹μΌλ΅ κµ¬μ„±


<br>
λ°μ΄ν„° μ „μ²λ¦¬λ” μ¤λ¥ λ°μ΄ν„°μ λ³΄μ •μ΄λ‚ κ²°μ†κ°’(Null) μ²λ¦¬ λ“±μ λ‹¤μ–‘ν• λ°μ΄ν„° ν΄λ μ§• μ‘μ—…, λ μ΄λΈ” μΈμ½”λ”©μ΄λ‚ μ›-ν•« μΈμ½”λ”©κ³Ό κ°™μ€ μΈμ½”λ”© μ‘μ—…, κ·Έλ¦¬κ³  λ°μ΄ν„°μ μ¤μΌ€μΌλ§/μ •κ·ν™” μ‘μ—… λ“±μΌλ΅ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ΄ μµμ μΌλ΅ μν–‰λ  μ μκ² λ°μ΄ν„°λ¥Ό μ‚¬μ „ μ²λ¦¬ν•λ” κ²ƒ

<br>
λ¨Έμ‹ λ¬λ‹ λ¨λΈμ€ ν•™μµ λ°μ΄ν„° μ„ΈνΈλ΅ ν•™μµν• λ’¤ λ°λ“μ‹ λ³„λ„μ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ΅ ν‰κ°€λμ–΄μ•Ό ν•¨

<br>λν• ν…μ¤νΈ λ°μ΄ν„°μ κ±΄μ λ¶€μ΅±μ΄λ‚ κ³ μ •λ ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈλ¥Ό μ΄μ©ν• λ°λ³µμ μΈ λ¨λΈμΌ ν•™μµκ³Ό ν‰κ°€λ” ν•΄λ‹Ή ν…μ¤νΈ λ°μ΄ν„° μ„ΈνΈμ—λ§ μΉμ°μΉ λΉμ•½ν• λ¨Έμ‹ λ¬λ‹ λ¨λΈμ„ λ§λ“¤ κ°€λ¥μ„±μ΄ λ†’μ
<br>μ΄λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄ ν•™μµ λ°μ΄ν„° μ„ΈνΈλ¥Ό ν•™μµ λ°μ΄ν„°μ™€ κ²€μ¦ λ°μ΄ν„°λ΅ κµ¬μ„±λ μ—¬λ¬ κ°μ fold μ„ΈνΈλ΅ λ¶„λ¦¬ν•΄ κµμ°¨ κ²€μ¦μ„ μν–‰ν•  μ μμ
<br>μ‚¬μ΄ν‚·λ°μ€ μ΄λ¬ν• κµμ°¨ κ²€μ¦μ„ μ§€μ›ν•κΈ° μ„ν•΄ KFold, StratifiedKFold, cross_val_score()λ“±μ λ‹¤μ–‘ν• ν΄λμ¤μ™€ ν•¨μλ¥Ό μ κ³µ
<br>λ¨Έμ‹ λ¬λ‹ λ¨λΈμ μµμ μ ν•μ΄νΌ νλΌλ―Έν„°λ¥Ό κµμ°¨ κ²€μ¦μ„ ν†µν•΄ μ¶”μ¶ν•κΈ° μ„ν•΄ GridSearchCVλ¥Ό μ κ³µ


<br>
λ‹¤μ μ¥μ—μ„λ” λ³Έκ²©μ μΌλ΅ λ¨Έμ‹ λ¬λ‹ μ§€λ„ν•™μµμ λ€ν‘μ μΈ ν• μ¶•μΈ λ¶„λ¥(Classification)λ¥Ό ν•™μµν•κΈ° μ „μ— λ¨Όμ € λ¶„λ¥μ μμΈ΅ μ„±λ¥μ„ ν‰κ°€ν•λ” λ‹¤μ–‘ν• λ°©λ²•μ„ ν™•μΈ
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\(7)-μ •λ¦¬.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/(7) μ •λ¦¬.md</guid><pubDate>Fri, 14 Feb 2025 09:41:26 GMT</pubDate></item><item><title><![CDATA[02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹]]></title><description><![CDATA[ 
 μ‚¬μ΄ν‚·λ°μ„ ν™μ©ν• λ¨Έμ‹ λ¬λ‹ κΈ°μ΄λ¥Ό λ°°μ›λ‹λ‹¤.]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\02.-μ‚¬μ΄ν‚·λ°μΌλ΅-μ‹μ‘ν•λ”-λ¨Έμ‹ λ¬λ‹\index.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/02. μ‚¬μ΄ν‚·λ°μΌλ΅ μ‹μ‘ν•λ” λ¨Έμ‹ λ¬λ‹/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[(1) μ •ν™•λ„ (Accuracy)]]></title><description><![CDATA[ 
 <br>Note
λ¨Έμ‹ λ¬λ‹μ€ λ°μ΄ν„° κ°€κ³µ/λ³€ν™, λ¨λΈ ν•™μµ/μμΈ΅, κ·Έλ¦¬κ³  ν‰κ°€(Evaluation)μ ν”„λ΅μ„Έμ¤λ΅ κµ¬μ„±
λ¨Έμ‹ λ¬λ‹ λ¨λΈμ€ μ—¬λ¬ κ°€μ§€ λ°©λ²•μΌλ΅ μμΈ΅ μ„±λ¥μ„ ν‰κ°€ν•  μ μμ
μ„±λ¥ ν‰κ°€ μ§€ν‘(Evaluation Metric)λ” μΌλ°μ μΌλ΅ λ¨λΈμ΄ λ¶„λ¥λƒ νκ·€λƒμ— λ”°λΌ μ—¬λ¬ μΆ…λ¥λ΅ λ‚λ‰¨
νκ·€μ κ²½μ° λ€λ¶€λ¶„ μ‹¤μ  κ°’κ³Ό μμΈ΅κ°’μ μ¤μ°¨ ν‰κ· κ°’μ— κΈ°λ°

<br>μλ¥Ό λ“¤μ–΄ μ¤μ°¨μ— μ λ“κ°’μ„ μ”μ΄ λ’¤ ν‰κ·  μ¤μ°¨λ¥Ό κµ¬ν•κ±°λ‚ μ¤μ°¨μ μ κ³± κ°’μ— λ£¨νΈλ¥Ό μ”μ΄ λ’¤ ν‰κ·  μ¤μ°¨λ¥Ό κµ¬ν•λ” λ°©λ²• λ“±
<br>κΈ°λ³Έμ μΌλ΅ μμΈ΅ μ¤μ°¨λ¥Ό κ°€μ§€κ³  μ •κ·ν™” μμ¤€μ„ μ¬κ°€κ³µ ν•λ” λ°©λ²•μ΄ νκ·€μ μ„±λ¥ ν‰κ°€ μ§€ν‘ μ ν•

λ¶„λ¥μ ν‰κ°€ λ°©λ²•λ„ μΌλ°μ μΌλ΅λ” μ‹¤μ  κ²°κ³Ό λ°μ΄ν„°μ™€ μμΈ΅ κ²°κ³Ό λ°μ΄ν„°κ°€ μ–Όλ§λ‚ μ •ν™•ν•κ³  μ¤λ¥κ°€ μ κ² λ°μƒν•λ” κ°€μ— κΈ°λ°ν•μ§€λ§, λ‹¨μν μ΄λ¬ν• μ •ν™•λ„λ§ κ°€μ§€κ³  νλ‹¨ν–λ‹¤κ°€λ” μλ»λ ν‰κ°€ κ²°κ³Όμ— λΉ μ§ μ μμ
λ³Έ μ¥μ—μ„λ” λ¶„λ¥μ— μ‚¬μ©λλ” μ„±λ¥ ν‰κ°€ μ§€ν‘μ— λ€ν•΄μ„ μ§‘μ¤‘μ μΌλ΅ μ„¤λ…ν•¨

<br>νΉν 0κ³Ό 1λ΅ κ²°μ •κ°’μ΄ ν•μ •λλ” μ΄μ§„ λ¶„λ¥μ μ„±λ¥ ν‰κ°€ μ§€ν‘μ— λ€ν•΄μ„ μ§‘μ¤‘μ μΌλ΅ μ„¤λ…!

<br>*λ¶„λ¥μ μ„±λ¥ ν‰κ°€ μ§€ν‘**

<br>μ •ν™•λ„ (Accuracy)
<br>μ¤μ°¨ν–‰λ ¬ (Confusion Matrix)
<br>μ •λ°€λ„ (Precision)
<br>μ¬ν„μ¨ (Recall)
<br>F1 μ¤μ½”μ–΄
<br>ROCκ³΅μ„ κ³Ό AUC

<br>
<br>λ¶„λ¥λ” κ²°μ • ν΄λμ¤ κ°’ μΆ…λ¥μ μ ν•μ— λ”°λΌ κΈμ •/λ¶€μ •κ³Ό κ°™μ€ 2κ°μ κ²°κ΄κ°’λ§μ„ κ°€μ§€λ” μ΄μ§„ λ¶„λ¥μ™€ μ—¬λ¬ κ°μ κ²°μ • ν΄λμ¤ κ°’μ„ κ°€μ§€λ” λ©€ν‹° λ¶„λ¥λ΅ λ‚λ‰  μ μμ

<br>κ²°μ • ν΄λμ¤λ” "κ°€λ¥ν• μ •λ‹µμ μ§‘ν•©"μ΄κ³  labelμ€ "κ° λ°μ΄ν„°μ μ‹¤μ  μ •λ‹µ"


<br>μ„μ—μ„ μ–ΈκΈ‰ν• λ¶„λ¥μ μ„±λ¥ μ§€ν‘λ” μ΄μ§„/λ©€ν‹° λ¶„λ¥ λ¨λ‘μ— μ μ©λλ” μ§€ν‘μ΄μ§€λ§, νΉν μ΄μ§„ λ¶„λ¥μ—μ„ λ”μ± μ¤‘μ”ν•κ² κ°•μ΅°ν•λ” μ§€ν‘!
<br><br>
μ™ μ„ μ§€ν‘κ°€ λ¨λ‘ μ΄μ§„ λ¶„λ¥μ—μ„ μ¤‘μ”ν• κΉ?
<br>μ •ν™•λ„ (Accuracy)
μ‹¤μ  λ°μ΄ν„°μ—μ„ μμΈ΅ λ°μ΄ν„°κ°€ μ–Όλ§λ‚ κ°™μ€μ§€λ¥Ό νλ‹¨ν•λ” μ§€ν‘<br>
μ¦‰, μμΈ΅ κ²°κ³Όκ°€ λ™μΌν• λ°μ΄ν„° κ±΄μλ¥Ό μ „μ²΄ μμΈ΅ λ°μ΄ν„° κ±΄μλ΅ λ‚λ κ°’
<br>
<br>μ •ν™•λ„λ” μ§κ΄€μ μΌλ΅ λ¨λΈ μμΈ΅ μ„±λ¥μ„ λ‚νƒ€λ‚΄λ” ν‰κ°€ μ§€ν‘
<br>ν•μ§€λ§ μ΄μ§„ λ¶„λ¥μ κ²½μ° λ°μ΄ν„°μ κµ¬μ„±μ— λ”°λΌ ML λ¨λΈμ μ„±λ¥μ„ μ™κ³΅ν•  μ μκΈ° λ•λ¬Έμ— μ •ν™•λ„ μμΉ ν•λ‚λ§ κ°€μ§€κ³  μ„±λ¥μ„ ν‰κ°€ν•μ§€ μ•μ
<br>
κ·Έλ ‡λ‹¤λ©΄ μ •ν™•λ„ μ§€ν‘κ°€ μ–΄λ–»κ² ML λ¨λΈμ μ„±λ¥μ„ μ™κ³΅ν• κΉ?
<br>
<br>μ•μ νƒ€μ΄νƒ€λ‹‰ μμ  μν–‰ κ²°κ³Όλ¥Ό λ³΄λ©΄ ν• κ°€μ§€ μκµ¬μ‹¬μ΄ μƒκΈΈ μ μμ
<br>ML μ•κ³ λ¦¬μ¦μ„ μ μ©ν• ν›„ μμΈ΅ μ •ν™•λ„μ κ²°κ³Όκ°€ λ³΄ν†µ 80%λ€μ€μ§€λ§, νƒ‘μΉκ°μ΄ λ‚¨μμΈ κ²½μ°λ³΄λ‹¤ μ—¬μμΈ κ²½μ°μ— μƒμ΅΄ ν™•λ¥ μ΄ λ†’μ•κΈ° λ•λ¬Έμ— λ³„λ‹¤λ¥Έ μ•κ³ λ¦¬μ¦μ μ μ© μ—†μ΄ λ¬΄μ΅°κ±΄ μ„±λ³„μ΄ μ—¬μμΈ κ²½μ° μƒμ΅΄μΌλ΅, λ‚¨μμΈ κ²½μ° μ‚¬λ§μΌλ΅ μμΈ΅ κ²°κ³Όλ¥Ό μμΈ΅ν•΄λ„ μ΄μ™€ λΉ„μ·ν• μμΉκ°€ λ‚μ¬ μ μμ

<br>λ‹¨μ§€ μ„±λ³„ μ΅°κ±΄ ν•λ‚λ§μ„ κ°€μ§€κ³  κ²°μ •ν•λ” λ³„κ±° μ•„λ‹ μ•κ³ λ¦¬μ¦λ„ λ†’μ€ μ •ν™•λ„λ¥Ό λ‚νƒ€λ‚΄λ” μƒν™© λ°μƒ!


<br>
λ‹¤μ μμ μ—μ„λ” μ‚¬μ΄ν‚·λ°μ BaseEstimator ν΄λμ¤λ¥Ό μƒμ†λ°›μ•„ μ•„λ¬΄λ° ν•™μµμ„ ν•μ§€ μ•κ³ , μ„±λ³„μ— λ”°λΌ μƒμ΅΄μλ¥Ό μμΈ΅ν•λ” λ‹¨μν• Classifier μƒμ„±

<br>μ‚¬μ΄ν‚·λ°μ€ BaseEstimatorλ¥Ό μƒμ†λ°›μΌλ©΄ Customized ν•νƒμ Estimatorλ¥Ό κ°λ°μκ°€ μƒμ„± κ°€λ¥
<br>μ—¬κΈ°μ„ fit()λ©”μ„λ“λ” μ•„λ¬΄κ²ƒλ„ μν–‰ X, predict() λ©”μ„λ“λ” λ‹¨μν Sex featureκ°€ 1μ΄λ©΄ 0, κ·Έλ ‡μ§€ μ•μΌλ©΄ 1λ΅ μμΈ΅

<br>import numpy as np
from sklearn.base import BaseEstimator

class MyDummyClassifier(BaseEstimator):
    # fit( ) λ©”μ†λ“λ” μ•„λ¬΄κ²ƒλ„ ν•™μµν•μ§€ μ•μ.
    def fit(self, X , y=None):
        pass

    # predict( ) λ©”μ†λ“λ” λ‹¨μν Sex featureκ°€ 1 μ΄λ©΄ 0 , κ·Έλ ‡μ§€ μ•μΌλ©΄ 1 λ΅ μμΈ΅ν•¨.
    def predict(self, X):
        pred = np.zeros((X.shape[0], 1))
        for i in range (X.shape[0]) :
            if X['Sex'].iloc[i] == 1:
                pred[i] = 0
            else :
                pred[i] = 1

        return pred
<br>
MyDummyClassifierλ¥Ό μ΄μ©ν•΄ μ• μ¥μ νƒ€μ΄νƒ€λ‹‰ μƒμ΅΄μ μμΈ΅ μν–‰
<br>
<br>μ•μ—μ„ μ„¤μ •ν• λ°μ΄ν„° μ „μ²λ¦¬ ν•¨μ
<br>from sklearn.preprocessing import LabelEncoder

# Null μ²λ¦¬ ν•¨μ
def fillna(df):
    df['Age'] = df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'] = df['Cabin'].fillna('N', inplace=True)
    df['Embarked'] = df['Embarked'].fillna('N', inplace=True)
    df['Fare'] = df['Fare'].fillna(0, inplace=True)
    return df

# λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ— λ¶ν•„μ”ν• ν”Όμ² μ κ±°
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

# λ μ΄λΈ” μΈμ½”λ”© μν–‰.
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# μ•μ—μ„ μ„¤μ •ν• λ°μ΄ν„° μ „μ²λ¦¬ ν•¨μ νΈμ¶
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
<br>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# μ›λ³Έ λ°μ΄ν„°λ¥Ό μ¬λ΅λ”©, λ°μ΄ν„° κ°€κ³µ, ν•™μµ λ°μ΄ν„°/ν…μ¤νΈ λ°μ΄ν„° λ¶„ν• .
titanic_df = pd.read_csv('train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived', axis=1)
X_titanic_df = transform_features(X_titanic_df)
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df,
                                                  test_size=0.2, random_state=0)

# μ„μ—μ„ μƒμ„±ν• Dummy Classifierλ¥Ό μ΄μ©ν•΄ ν•™μµ/μμΈ΅/ν‰κ°€ μν–‰.
myclf = MyDummyClassifier()
myclf.fit(X_train, y_train)

mypredictions = myclf.predict(X_test)
print('Dummy Classifierμ μ •ν™•λ„λ”: {0:.4f}'.format(accuracy_score(y_test, mypredictions)))

&gt;&gt;&gt; Dummy Classifierμ μ •ν™•λ„λ”: 0.7877
<br>
<br>μ΄λ ‡κ² λ‹¨μν• μ•κ³ λ¦¬μ¦μΌλ΅ μμΈ΅μ„ ν•λ”λΌλ„ λ°μ΄ν„°μ κµ¬μ„±μ— λ”°λΌ μ •ν™•λ„ κ²°κ³Όλ” μ•½ 78.77%λ΅ κ½¤ λ†’μ€ μμΉκ°€ λ‚μ¬ μ μκΈ°μ— μ •ν™•λ„λ¥Ό ν‰κ°€ μ§€ν‘λ΅ μ‚¬μ©ν•  λ•λ” λ§¤μ° μ‹ μ¤‘ν•΄μ•Ό ν•¨!
<br>νΉν μ •ν™•λ„λ” λ¶κ· ν•ν•(imbalanced) label κ°’ λ¶„ν¬μ—μ„ ML λ¨λΈμ μ„±λ¥μ„ νλ‹¨ν•  κ²½μ°, μ ν•©ν• ν‰κ°€ μ§€ν‘κ°€ μ•„λ‹

<br>μλ¥Ό λ“¤μ–΄ 100κ°μ λ°μ΄ν„°κ°€ μκ³  μ΄ μ¤‘μ— 90κ°μ λ°μ΄ν„° labelμ΄ 0, λ‹¨ 10κ°μ λ°μ΄ν„° labelμ΄ 1μ΄λΌκ³  ν•λ‹¤λ©΄ λ¬΄μ΅°κ±΄ 0μΌλ΅ μμΈ΅ κ²°κ³Όλ¥Ό λ°ν™ν•λ” ML λ¨λΈμ κ²½μ°λΌλ„ μ •ν™•λ„κ°€ 90%λ‚ λ¨


<br>
MNIST λ°μ΄ν„° μ„ΈνΈλ¥Ό λ³€ν™ν•΄ λ¶κ· ν•ν• λ°μ΄ν„° μ„ΈνΈλ΅ λ§λ“  λ’¤μ— μ •ν™•λ„ μ§€ν‘ μ μ©μ‹ λ¬Έμ μ  ν™•μΈ
<br>
<br>MNIST λ°μ΄ν„° μ„ΈνΈνΈ 0λ¶€ν„° 9κΉμ§€μ μ«μ μ΄λ―Έμ§€μ ν”½μ…€ μ •λ³΄λ¥Ό κ°€μ§€κ³  μμΌλ©°, μ΄λ¥Ό κΈ°λ°μΌλ΅ μ«μ Digitλ¥Ό μμΈ΅ν•λ” λ° μ‚¬μ©
<br>μ‚¬μ΄ν‚·λ°μ€ load_digits() APIλ¥Ό ν†µν•΄ MNIST λ°μ΄ν„° μ„ΈνΈ μ κ³µ
<br>μ›λ MNIST λ°μ΄ν„° μ„ΈνΈλ” label κ°’μ΄ 0λ¶€ν„° 9κΉμ§€ μλ” λ©€ν‹° label λ¶„λ¥λ¥Ό μ„ν• κ²ƒμ΄μ§€λ§, μ΄κ²ƒμ„ label=7μΈ κ²ƒλ§ True, λ‚λ¨Έμ§€λ” λ¨λ‘ Falseλ΅ λ³€ν™ν•΄ μ΄μ§„ λ¶„λ¥ λ¬Έμ λ΅ λ³€κ²½<br>
<img alt="Pasted image 20250214205614.png" src="lib\media\pasted-image-20250214205614.png">
<br>Attention
μ•„λ¬΄κ²ƒλ„ ν•μ§€ μ•κ³  λ¬΄μ΅°κ±΄ νΉμ •ν• κ²°κ³Όλ΅ μμΈ΅ κ²°κ³Όλ¥Ό λ°ν™ν•΄λ„ λ°μ΄ν„° λ¶„ν¬λ„κ°€ κ· μΌν•μ§€ μ•μ€ κ²½μ° λ†’μ€ μμΉκ°€ λ‚νƒ€λ‚  μ μλ” κ²ƒμ΄ μ •ν™•λ„ ν‰κ°€ μ§€ν‘μ λ§Ήμ !
<br>
<br>λ¶κ· ν•ν• λ°μ΄ν„° μ„ΈνΈμ™€ Dummy Classifier μƒμ„±
<br>from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

class MyFakeClassifier(BaseEstimator):
    def fit(self,X,y):
        pass

    # μ…λ ¥κ°’μΌλ΅ λ“¤μ–΄μ¤λ” X λ°μ΄ν„° μ…‹μ ν¬κΈ°λ§νΌ λ¨λ‘ 0κ°’μΌλ΅ λ§λ“¤μ–΄μ„ λ°ν™
    def predict(self,X):
        return np.zeros( (len(X), 1) , dtype=bool)

# μ‚¬μ΄ν‚·λ°μ λ‚΄μ¥ λ°μ΄ν„° μ…‹μΈ load_digits( )λ¥Ό μ΄μ©ν•μ—¬ MNIST λ°μ΄ν„° λ΅λ”©
digits = load_digits()

print(digits.data)
print("### digits.data.shape:", digits.data.shape)
print(digits.target)
print("### digits.target.shape:", digits.target.shape)
]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\03.-ν‰κ°€\(1)-μ •ν™•λ„-(accuracy).html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/03. ν‰κ°€/(1) μ •ν™•λ„ (Accuracy).md</guid><pubDate>Fri, 14 Feb 2025 11:59:27 GMT</pubDate><enclosure url="lib\media\pasted-image-20250214205614.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250214205614.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[03. ν‰κ°€]]></title><description><![CDATA[ 
 λ¶„λ¥μ μμΈ΅ μ„±λ¥μ„ ν‰κ°€ν•λ” λ‹¤μ–‘ν• λ°©λ²•μ„ λ‹¤λ£¬λ‹¤.]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\03.-ν‰κ°€\index.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/03. ν‰κ°€/index.md</guid><pubDate>Fri, 14 Feb 2025 09:43:44 GMT</pubDate></item><item><title><![CDATA[π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“]]></title><description><![CDATA[ 
 μ΄ λ¬Έμ„λ” λ¨Έμ‹ λ¬λ‹μ„ νμ΄μ¬ κΈ°λ°μΌλ΅ λ°°μ°κΈ° μ„ν• κ°€μ΄λ“μ…λ‹λ‹¤.]]></description><link>content\π“-νμ΄μ¬-λ¨Έμ‹ λ¬λ‹-μ™„λ²½-κ°€μ΄λ“\index.html</link><guid isPermaLink="false">content/π“ νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½ κ°€μ΄λ“/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[ν™]]></title><description><![CDATA[ 
 λ°μ΄ν„° μ‚¬μ΄μ–Έμ¤μ™€ λ¨Έμ‹ λ¬λ‹μ„ νƒκµ¬ν•λ” λΈ”λ΅κ·Έ]]></description><link>content\index.html</link><guid isPermaLink="false">content/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item></channel></rss>