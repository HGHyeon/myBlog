<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ObsidianBlog]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>ObsidianBlog</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sat, 15 Feb 2025 05:46:18 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sat, 15 Feb 2025 05:46:12 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[(1) 머신러닝의 개념]]></title><description><![CDATA[ 
 <br>머신러닝 (Machine Learning)
일반적으로는 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭<br>
규칙을 사람이 직접 코딩하지 않고, 알고리즘이 학습을 통해 자동으로 찾아냄
<br>
<br>머신러닝 알고리즘은 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용

<br>데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출


<br><br><img alt="Pasted image 20250207005218.png" src="lib\media\pasted-image-20250207005218.png"><br>Note
일반적으로 머신러닝은 지도학습(Supervised Learning)과 비지도학습(Un-supervised Learning), 강화학습(Reinforcement Learning)으로 나뉨
<br>
<br>지도학습 (Supervised Learning)

<br>정답(label)이 있는 데이터를 기반으로 학습하며, 목표는 주어진 입력에 대한 출력을 예측하는 것
<br>분류(Classification), 회귀(Regression), 추천 시스템(Recommendation), 시각/음성 감지/인지(Computer Vision, Speech Recognition), 텍스트 분석 및 자연어 처리(NLP)


<br>비지도학습 (Un-supervised Learning)

<br>정답(label)이 없는 데이터를 기반으로 패턴이나 구조를 학습
<br>클러스터링(Clustering), 차원 축소(Dimensionality Reduction), 연관 규칙 학습(Association Rule Learning), 이상 탐지(Anomaly Detection)


<br>강화학습 (Reinforcement Learning)

<br>행동에 대한 보상을 받으면서 학습하여, 어떤 환경 안에서 선택 가능한 행동들 중 보상을 최대화하는 행동 또는 행동 순서를 선택하는 방법
<br>게임 플레이(예 : 체스, 바둑), 로봇 제어, 자율 주리


<br><br>
머신러닝에서 데이터와 머신러닝 알고리즘 중 어느 것이 더 중요한 요소인가?
<br>
<br>사실 데이터와 머신러닝 알고리즘 모두 머신러닝에서는 중요한 요소

<br>하지만 일단 머신러닝 세상이 본격적으로 펼쳐진다면 데이터의 중요성이 무엇보다 커짐!


<br>머신러닝의 가장 큰 단점은 데이터에 매우 의존적

<br>Garbage In, Garbage Out → 좋은 품질의 데이터를 갖추지 못한다면 머신러닝의 수행 결과도 좋을 수 없음


<br>따라서, 최적의 머신러닝 알고리즘과 모델 파라미터를 구축하는 능력도 중요하지만 데이터를 이해하고 효율적으로 가공, 처리, 추출해 최적의 데이터를 기반으로 알고리즘을 구동할 수 있도록 준비하는 능력이 더 중요할 수 있음!
<br><br>
<br>머신러닝 프로그램을 작성할 수 있는 대표적인 오픈 소스 프로그램 언어는 파이썬과 R
<br>R은 통계 전용 프로그램 언어이고 파이썬은 다양한 영역에서 사용되는 개발 전문 프로그램 언어
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\01.-파이썬-기반의-머신러닝과-생태계-이해\(1)-머신러닝의-개념.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/01. 파이썬 기반의 머신러닝과 생태계 이해/(1) 머신러닝의 개념.md</guid><pubDate>Thu, 13 Feb 2025 10:25:23 GMT</pubDate><enclosure url="lib\media\pasted-image-20250207005218.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250207005218.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(2) 파이썬 머신러닝 생태계를 구성하는 주요 패키지]]></title><description><![CDATA[ 
 ]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\01.-파이썬-기반의-머신러닝과-생태계-이해\(2)-파이썬-머신러닝-생태계를-구성하는-주요-패키지.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/01. 파이썬 기반의 머신러닝과 생태계 이해/(2) 파이썬 머신러닝 생태계를 구성하는 주요 패키지.md</guid><pubDate>Thu, 13 Feb 2025 09:56:59 GMT</pubDate></item><item><title><![CDATA[01. 파이썬 기반의 머신러닝과 생태계 이해]]></title><description><![CDATA[ 
 이 섹션에서는 머신러닝의 개념과 파이썬 기반 생태계를 다룹니다.]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\01.-파이썬-기반의-머신러닝과-생태계-이해\index.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/01. 파이썬 기반의 머신러닝과 생태계 이해/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[(1) 사이킷런 소개와 특징]]></title><description><![CDATA[ 
 <br>사이킷런 (scikit-learn)
파이썬 머신러닝 라이브러리

<br>머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API 제공

]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(1)-사이킷런-소개와-특징.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(1) 사이킷런 소개와 특징.md</guid><pubDate>Thu, 13 Feb 2025 10:25:27 GMT</pubDate></item><item><title><![CDATA[(2) 첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기]]></title><description><![CDATA[ 
 <br>scikit-learn을 통한 첫 번째 머신러닝 모델
붓꽃 데이터 세트 -&gt; 붓꽃의 품종을 분류(Classification)하는 것<br>
붓꽃 데이터 세트 : 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 feature를 기반으로 꽃의 품종 예측<br>
<img alt="Pasted image 20250207141842.png" src="lib\media\pasted-image-20250207141842.png">
<br>
<br>분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법
<br>지도학습이란?

<br>학습을 위한 다양한 feature(데이터를 설명하는 입력변수)와 분류 결정값인 label(정답) 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 label을 예측하는 방법
<br>즉, 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식
<br>이때 학습을 위해 주어진 데이터 세트 -&gt; 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트 -&gt; 테스트 데이터 세트로 지칭<br>
<img alt="Pasted image 20250209201811.png" src="lib\media\pasted-image-20250209201811.png">


<br><br><br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
<br>
<br>사이킷런 패키지 내의 모듈명은 sklearn으로 시작하는 명명규칙 존재

<br>sklearn.datasets : 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임

<br>붓꽃 데이터 세트를 생성하는 데는 load_iris()를 이용


<br>sklearn.tree : tree 기반 ML 알고리즘을 구현한 클래스의 모임

<br>ML 알고리즘은 의사 결정 트리(Decision Tree) 알고리즘으로, 이를 구현한 DecisionTreeClassifier를 적용


<br>sklearn.model_selection : 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임

<br>하이퍼 파라미터(Hyperparameter) : 머신러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터 (머신러닝 알고리즘의 성능을 튜닝할 수 있음)
<br>데이터 세트를 학습 데이터와 테스트 데이터로 분리하는 데는 train_test_split()함수를 사용




<br><br><br>import pandas as pd

# 붓꽃 데이터 세트를 로딩합니다.
iris = load_iris()

# iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터를 numpy로 가지고 있습니다.
iris_data = iris.data

# iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 가지고 있습니다.
iris_label = iris.target
print('iris target값:', iris_label)
print('iris target명:', iris.target_names)

# 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다.
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target
iris_df.head(3)
<br><img alt="Pasted image 20250209172443.png" src="lib\media\pasted-image-20250209172443.png"><br>
<br>feature : sepal length, sepal width, petal length, petal width
<br>label : 0 (setosa 품종), 1 (versicolort 품종), 2 (virginica 품종)
<br><br><br>X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label,
                                                    test_size=0.2, random_state=11)
<br>
<br>학습용 데이터와 테스트용 데이터는 반드시 분리해야 함

<br>학습 데이터로 학습된 모델이 얼마나 뛰어난 성능을 가지는지 평가하려면 테스트 데이터 세트가 필요하기 때문!
<br>이를 위해서 사이킷런은 train_test_split() API를 제공
<br>train_test_split() : 사이킷런에서 제공하는 데이터 분할 함수, train set와 test set를 나눌 때 사용

<br>test_size 파라미터 : 입력값의 비율로 쉽게 분할

<br>test_size = 0.2 : 전체 데이터 중 테스트 데이터가 20%, 학습 데이터가 80%로 데이터 분할




<br>첫 번째 파라미터인 iris_data는 feature 데이터 세트, 두 번째 파라미터인 iris_label은 label 데이터 세트


<br><br><br># DecisionTreeClassifier 객체 생성
dt_clf = DecisionTreeClassifier(random_state=11)

# 학습 수행
dt_clf.fit(X_train, y_train)
<br>
<br>먼저 사이킷런의 의사 결정 트리 클래스인 DecisionTreeClassifier 를 객체로 생성

<br>random_state=11 은 항상 동일한 모델이 생성되도록 보장해주는 것


<br>생성된 DecisionTreeClassifier 객체의 fit() 메서드에 학습용 feature 데이터 속성과 label 값 데이터 세트를 입력해 호출하면 학습을 수행
<br># 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행.
pred = dt_clf.predict(X_test)
<br>
<br>예측은 반드시 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용
<br>DecisionTreeclassifier 객체의 predict() 메서드에 테스트용 feature 데이터 세트를 입력해 호출 -&gt; 학습된 모델 기반에서 테스트 데이터 세트에 대한 예측값을 반환
<br><br><br>from sklearn.metrics import accuracy_score

print(f'예측 정확도: {accuracy_score(y_test, pred):.4f}')

&gt;&gt;&gt; 예측 정확도: 0.9333
<br>
<br>일반적으로 머신러닝 모델의 성능 평가 방법은 여러 가지가 있으나, 여기서는 정확도를 측정해봄
<br>정확도 (accuracy) : 예측 결과가 실제 label 값과 얼마나 정확하게 맞는지를 평가하는 지표<br>
- 사이킷런은 정확도 측정을 위해 accuracy_score() 함수 제공<br>
- 첫 번째 파라미터로 실제 label 데이터 세트, 두 번째 파라미터로 예측 label 데이터 세트 입력
붓꽃 데이터 세트로 분류를 예측한 process

<br>데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리
<br>모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습
<br>예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류 (즉, 붓꽃 종류)를 예측
<br>평가 : 예측된 결괏값과 테스트 데이터의 실제 결괏값을 비교해 ML 모델 성능을 평가<br>
<img alt="Pasted image 20250209202115.png" src="lib\media\pasted-image-20250209202115.png">



]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(2)-첫-번째-머신러닝-만들어-보기-붓꽃-품종-예측하기.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(2) 첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기.md</guid><pubDate>Thu, 13 Feb 2025 10:25:31 GMT</pubDate><enclosure url="lib\media\pasted-image-20250207141842.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250207141842.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(3) 사이킷런의 기반 프레임워크 익히기]]></title><description><![CDATA[ 
 <br><br>Note

<br>사이킷런은 ML 모델 학습을 위해서 fit() , 예측을 위해 predict() 메서드를 제공
<br>지도학습의 주요 유형인 분류(Classification)와 회귀(Regression)를 위한 다양한 알고리즘을 지원
<br>분류 알고리즘을 구현한 클래스를는 Classifier로, 회귀 알고리즘을 구현한 클래스는 Regressor 로 지칭
<br>이러한 Classifier와 Regressor 를 포함한 모든 모델 클래스는 Estimator 클래스로 통칭됨
<br>모든 Estimator는 fit()을 통해 학습하고, predict()를 통해 예측을 수행

<br>
<br>cross_val_score()와 같은 evaluation 함수, GridSearchCV와 같은 하이퍼 파라미터 튜닝을 지원하는 클래스의 경우 이 Estimator를 인자로 받음
<br>인자로 받은 Estimator에 대해서 cross_val_score(), GridSearchCV.fit() 함수 내에서 이 Estimator의 fit()과 predict()를 호출해서 평가를 하거나 하이퍼 파라미터 튜닝을 수행하는 것<br>
<img alt="Pasted image 20250209212336.png" src="lib\media\pasted-image-20250209212336.png">
<br><br><br><img alt="Pasted image 20250209221724.png" src="lib\media\pasted-image-20250209221724.png"><br>
<img alt="Pasted image 20250209221754.png" src="lib\media\pasted-image-20250209221754.png"><br>
<img alt="Pasted image 20250209221813.png" src="lib\media\pasted-image-20250209221813.png"><br>Check
일반적으로 머신러닝 모델을 구축하는 주요 프로세스는 feature의 가공, 변경, 추출을 수행하는 feature processing, ML 알고리즘 학습/예측 수행, 그리고 모델 평가의 단계를 반복적으로 수행하는 것
<br><br><br>Note
사이킷런에는 별도의 외부 웹사이트에서 데이터 세트를 내려받을 필요 없이 예제로 활용할 수 있는 간단하면서도 좋은 데이터 세트가 내장돼 있음
<br>
<br>사이킷런에 내장 되어 있는 데이터 세트는 분류나 회귀를 연습하기 위한 예제 용도의 데이터 세트와 분류나 클러스터링을 위해 표본 데이터로 생성될 수 있는 데이터 세트로 나뉘어 짐
<br><br><br>
<br>fetch 계열의 명령은 데이터의 크기가 커서 패키지에 처음부터 저장돼 있지 않고 인터넷에서 내려받아 홈 디렉터리 아래의 scikit_learn_data라는 서브 디렉터리에 저장한 후 추후 불러들이는 데이터 (최초 사용 시에 인터넷에 연결돼 있지 않으면 사용 불가)

<br>fetch_covtype() : 회귀 분석용 토지 조사 자료
<br>fetch_20newsgroups() : 뉴스 그룹 텍스트 자료
<br>fetch_olivetti_faces() : 얼굴 이미지 자료
<br>fetch_lfw_people() : 얼굴 이미지 자료
<br>fetch_lfw_pairs() : 얼굴 이미지 자료
<br>fetch_rcv1() : 로이터 뉴스 말뭉치
<br>fetch_mldata() : ML 웹사이트에서 다운로드


<br><br><br><br>
<br>사이킷런에 내장된 이 데이터 세트는 일반적으로 딕셔너리 형태로 되어 있음

<br>key는 보통 data, target, target_name, feature_names, DESCR로 구성돼 있음
<br>data(ndarray) : feature의 데이터 세트
<br>target(ndarray) : 분류 시 label 값, 회귀일 때는 숫자 결괏값 데이터 세트
<br>target_names(ndarray or list) : 개별 label의 이름
<br>feature_names(ndarray or list) : feature의 이름
<br>DESCR(string) : 데이터 세트에 대한 설명과 각 feature의 설명


<br><br>from sklearn.datasets import load_iris

iris_data = load_iris()

print(type(iris_data))
<br>
<br>load_iris()의 API 반환 결과는 sklearn.utils.Bunch클래스로 파이썬 딕셔너리 자료형과 유사
<br>keys = iris_data.keys()
print('붓꽃 데이터 세트의 키들:', keys)

&gt;&gt;&gt; 붓꽃 데이터 세트의 키들: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
<br>
<br>
다음 그림은 load_iris()가 반환하는 붓꽃 데이터 세트의 각 키가 의미하는 값<br>
<img alt="Pasted image 20250210191359.png" src="lib\media\pasted-image-20250210191359.png">

<br>
feature_names, target_name, data, target의 values 확인

<br>print('\n feature_names 의 type:',type(iris_data.feature_names))
print(' feature_names 의 shape:',len(iris_data.feature_names))
print(iris_data.feature_names)

print('\n target_names 의 type:',type(iris_data.target_names))
print(' feature_names 의 shape:',len(iris_data.target_names))
print(iris_data.target_names)

print('\n data 의 type:',type(iris_data.data))
print(' data 의 shape:',iris_data.data.shape)
print(iris_data['data'])

print('\n target 의 type:',type(iris_data.target))
print(' target 의 shape:',iris_data.target.shape)
print(iris_data.target)
<br><img alt="Pasted image 20250210193511.png" src="lib\media\pasted-image-20250210193511.png">]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(3)-사이킷런의-기반-프레임워크-익히기.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(3) 사이킷런의 기반 프레임워크 익히기.md</guid><pubDate>Thu, 13 Feb 2025 10:25:34 GMT</pubDate><enclosure url="lib\media\pasted-image-20250209212336.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250209212336.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(4) Model Selection 모듈 소개]]></title><description><![CDATA[ 
 <br>Note
사이킷런의 model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공
<br><br>
학습 데이터 세트로만 학습하고 예측하면 무엇이 문제일까?
<br>
<br>다음 예제는 학습과 예측을 동일한 데이터 세트로 수행한 결과
<br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
dt_clf = DecisionTreeClassifier()
train_data = iris.data
train_label = iris.target
dt_clf.fit(train_data, train_label)

# 학습 데이터 셋으로 예측 수행
pred = dt_clf.predict(train_data)
print('예측 정확도:',accuracy_score(train_label,pred))

&gt;&gt;&gt; 예측 정확도: 1.0
<br>
<br>예측 정확도가 1.0이라는 뜻은 정확도가 100%
<br>즉, 문제의 정답을 알고 있는 상태에서 같은 문제를 테스트 한 것!
<br>따라서, 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야 함
<br>사이킷런의 train_test_split()

<br>원본 데이터 세트에서 학습 및 테스트 데이터 세트를 쉽게 분리 가능
<br>train_test_split()는 첫 번째 파라미터로 feature 데이터 세트, 두 번째 파라미터로 label 데이터 세트를 입력받고, 선택적으로 다음 파라미터를 입력 받음

<br>test_size : 전체 데이터에서 test 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정 (default : 0.25, 즉 25%)
<br>train_size : 전체 데이터에서 train 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정 (test_size parameter를 통상적으로 사용하기 때문에 train_size는 잘 사용되지 않음)
<br>shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정 (default : True), 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를 만드는 데 사용
<br>random_state : 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값 (train_test_split()는 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 생성)
<br>train_test_split()의 반환값은 tuple 형태로, 순차적으로 train-feature, test-feature, train-label, test-label 데이터 세트 반환



<br>
<br>붓꽃 데이터 세트를 train_test_split()을 이용해 test 데이터 세트를 전체의 30%, train 데이터 세트를 70%로 분리
<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target,
                 test_size=0.3, random_state=121)
<br>
<br>train 데이터를 기반으로 DecisionTreeClassfier를 학습하고 예측 정확도 측정
<br>dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))

&gt;&gt;&gt; 예측 정확도: 0.9556
<br>
붓꽃 데이터는 150개의 데이터로 데이터의 양이 크지 않아 전체의 30% 정도인 테스트 데이터는 45개 정도밖에 되지 않으므로 알고리즘의 예측 성능을 판단하기에는 그리 적절하지 않음<br>
-&gt; 학습을 위한 데이터의 양을 일정 수준 이상으로 보장하는 것도 중요하지만, 학습된 모델에 대해 다양한 데이터를 기반으로 예측 성능을 평가해 보는 것도 매우 중요!
<br><br><br>과적합 (Overfitting)
모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 것
<br>
<br>고정된 학습 데이터와 테스트 데이터로 평가를 하다 보면 테스트 데이터에만 최적의 성능을 발휘할 수 있도록 편향되게 모델을 유도하는 경향이 생길 수 있음
<br>결국은 해당 테스트 데이터에만 과적합되는 학습 모델이 만들어져 다른 테스트용 데이터가 들어올 경우에는 성능이 저하됨<br>
-&gt; 이러한 문제점을 개선하기 위해 교차 검증을 이용해 더 다양한 학습과 평가를 수행!
<br>교차 검증 (Cross-Validation, CV)
주어진 데이터를 훈련 데이터와 검증 데이터로 나누어 모델의 일반화 성능을 평가하는 방법

<br>데이터 편중을 막기 위해서 별도의 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것
<br>각 세트에서 수행한 평가 결과에 따라 하이퍼 파라미터 튜닝 등의 모델 최적화를 더욱 손쉽게 할 수 있음
<br>대부분의 ML 모델의 성능 평가는 교차 검증 기반으로 1차 평가를 한 뒤에 최종적으로 테스트 데이터 세트에 적용해 평가하는 프로세스<br>
<img alt="Pasted image 20250210210414.png" src="lib\media\pasted-image-20250210210414.png">

<br><br>k 폴드 교차 검증 (K-Fold Cross-Validation)
k개의 데이터 fold(조각) 세트를 만들어서 k번만큼 각 fold 세트에 학습과 검증 평가를 반복적으로 수행하는 방법
<br>
<br>
다음 그림은 5 fold 교차 검증 수행 (k=5)<br>
<img alt="Pasted image 20250210211232.png" src="lib\media\pasted-image-20250210211232.png">

<br>5개의 fold된 데이터 세트를 학습과 검증을 위한 데이터 세트로 변경하면서 5번 평가를 수행한 뒤, 이 5개의 평가를 평균한 결과를 가지고 예측 성능을 평가
<br>이렇게 학습 데이터 세트와 검증 데이터 세트를 점진적으로 변경하면서 마지막 5번째(k번째)까지 학습과 검증을 수행하는 것이 바로 k fold 교차 검증


<br>
사이킷런에서는 k fold 교차 검증 프로세스를 구현하기 위해 KFold와 StratifiedKFold를 제공

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)
cv_accuracy = []
print('붓꽃 데이터 세트 크기:',features.shape[0])

&gt;&gt;&gt; 붓꽃 데이터 세트 크기: 150
<br>
<br>KFold(n_splits=5)로 KFold 객체를 생성했으니, 이제 생성된 KFold 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 fold 데이터 세트로 분리
<br>n_iter = 0

# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  
for train_index, test_index  in kfold.split(features):
    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    #학습 및 예측
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1

    # 반복 시 마다 정확도 측정
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]

    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)

# 개별 iteration별 정확도를 합하여 평균 정확도 계산
print('\n## 평균 검증 정확도:', np.mean(cv_accuracy))
<br><img alt="Pasted image 20250211162001.png" src="lib\media\pasted-image-20250211162001.png"><br>
<br>5번 교차 검증 결과 평균 검증 정확도는 0.9이고, 교차 검증 시마다 검증 세트의 인덱스가 달라짐을 알 수 있음!
<br><br>Stratified K Fold
불균형한(imbalanced) 분포도를 가진 label(결정 클래스) 데이터 집합을 위한 K fold 방식

<br>불균형한 분포도를 가진 label 데이터 집합은 특정 label 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것을 의미
<br>K Fold가 label 데이터 집합이 원본 데이터 집합의 label 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결!

<br>이를 위해 원본 데이터의 label 분포를 먼저 고려한 뒤 이 분포와 동일하게 학습과 검증 데이터 세트를 분배



<br>
대출 사기 데이터를 예측한다고 가정!
<br>
<br>이 데이터 세트는 1억 건이고, 수십 개의 feature와 대출 사기 여부를 뜻하는 label(사기:1, 정상:0)로 구성되어 있음
<br>그런데 대부분의 데이터는 정상 대출일 것!
<br>대출 사기가 약 1000건이 있다고 한다면 전체의 0.0001%의 아주 작은 확률로 대출 사기 label이 존재
<br>이렇게 된다면 K Fold로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라고 label 값인 0과 1의 비율을 제대로 반영하지 못하는 경우가 쉽게 발생!
<br>따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요!
<br>Note
먼저 K Fold가 어떤 문제를 가지고 있는지 확인해 보고 이를 사이킷런의 StratifiedKFold 클래스를 이용해 개선!
<br>
<br>붓꽃 데이터 세트를 간단하게 DataFrame으로 생성하고 label 값의 분포도 확인
<br>import pandas as pd

iris = load_iris()

iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label']=iris.target
iris_df['label'].value_counts()
<br><img alt="Pasted image 20250211164900.png" src="lib\media\pasted-image-20250211164900.png"><br>
<br>각 교차 검증 시마다 생성되는 학습/검증 label 데이터 값의 분포도 확인
<br>kfold = KFold(n_splits=3)
# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환.
n_iter =0
for train_index, test_index  in kfold.split(iris_df):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]

    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())
<br><img alt="0211170424950648.jpg" src="lib\media\0211170424950648.jpg"><br>
<br>교차 검증 시마다 3개의 fold 세트로 만들어지는 학습 label과 검증 label이 완전히 다른 값으로 추출됨
<br>이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없음
<br>
<br>동일한 데이터 분할을 StratifiedKFold로 수행하고 학습/검증 label 데이터의 분포도 확인
<br>
StratifiedKFold를 사용하는 방법은 KFold를 사용하는 방법과 거의 비슷하지만, 단 하나 큰 차이는 StratifiedKFold는 label 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에 split()메서드에 인자로 feature 데이터 세트뿐만 아니라 label 데이터 세트도 반드시 필요!
<br>from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
n_iter=0

for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]

    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())
<br><img alt="021117243817062.jpg" src="lib\media\021117243817062.jpg"><br>
<br>출력 결과를 보면 학습 label과 검증 label 데이터 값의 분포도가 거의 동일하게 할당됐음을 알 수 있음
<br>이렇게 분할이 되어야 label 값 0, 1, 2를 모두 학습할 수 있고, 이에 기반해 검증을 수행할 수 있음
<br>
<br>StratifiedKFold를 이용해 붓꽃 데이터 교차 검증
<br>dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  
for train_index, test_index  in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    #학습 및 예측
    dt_clf.fit(X_train, y_train)    
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]

    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)

# 교차 검증별 정확도 및 평균 정확도 계산
print('\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))
print('## 평균 검증 정확도:', np.round(np.mean(cv_accuracy), 4))
<br><img alt="Pasted image 20250211172934.png" src="lib\media\pasted-image-20250211172934.png"><br>Note
Stratified K Fold 의 경우 원본 데이터의 label 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로 왜곡된 label 데이터 세트에서는 반드시 Stratified K Fold를 이용해 교차 검증해야 함!
<br><br>
<br>사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API 제공
<br>KFold로 데이터를 학습하고 예측 하는 코드 순서

<br>

<br>fold 세트를 설정


<br>

<br>for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출


<br>

<br>반복적으로 학습과 예측을 수행하고 예측 성능 반환




<br>cross_val_score()는 이런 일련의 과정을 한꺼번에 수행해주는 API
<br>cross_val_score()
cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs') 

<br>주요 parameter : estimator, X, y, scoring, cv

<br>estimator : Classifier 클래스 or Regressor 클래스를 의미
<br>X : feature 데이터 세트
<br>y : label 데이터 세트
<br>scoring : 예측 성능 평가 지표
<br>cv : 교차 검증 fold 수

<br>KFold 객체나 StratifiedKFold 객체를 입력할 수도 있음




<br>반환 값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환
<br>즉, classifier가 입력되면 Stratified K fold 방식으로 label 값의 분포에 따라 학습/테스트 세트 분할 (회귀인 경우는 Stratified K fold 방식으로 분할 할 수 없으므로 K fold 방식으로 분할)

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score , cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개
scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)
print('교차 검증별 정확도:',np.round(scores, 4))
print('평균 검증 정확도:', np.round(np.mean(scores), 4))

&gt;&gt;&gt; 교차 검증별 정확도: [0.98 0.94 0.98]
    평균 검증 정확도: 0.9667
<br>
<br>cross_val_score() API는 내부에서 Estimator를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차 검증을 수행할 수 있음!
<br>cv 파라미터에 정수값(fold 수)를 입력하면 내부적으로 StratifiedKFold를 이용
<br>비슷한 API로 cross_validate() 존재

<br>여러 개의 평가 지표 반환 가능
<br>학습 데이터에 대한 성능 평가 지표와 수행 시간도 같이 제공


<br><br>Note
하이퍼 파라미터 (Hyperparameter)

<br>모델을 학습하기 전에 사용자가 직접 설정하는 값으로, 이 값을 조정해 알고리즘의 예측 성능 개선

GridSearchCV

<br>사이킷런에서 제공하는 하이퍼 파라미터 최적화 API
<br>Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안 제공
<br>Grid는 격자라는 뜻으로, 촘촘하게 파라미터를 입력하면서 테스트를 하는 방식

<br>
<br>결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 
<br>다음과 같이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화 수행 가능
<br>grid_parameters = {'max_depth': [1, 2, 3],
				   'min_samples_split': [2, 3]}
<br>
<br>GridSearchCV는 교차 검증을 기반으로 이 하이퍼 파라미터의 최적값을 찾게 해줌!

<br>

<br>데이터 세트를 cross-validation을 위한 학습/테스트 세트로 자동으로 분할


<br>

<br>하이퍼 파라미터 grid에 기술된 모든 파라미터를 순차적으로 적용해 최적의 파라미터를 찾을 수 있게 해줌




<br>단, 동시에 순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸리는 단점 존재!

<br>위의 경우 CV가 3회라면 CV 3회 x 6개 파라미터 조합 = 18회의 학습/평가 이루어짐


<br>GridSearchCV 클래스

<br>estimator : classifier, regressor, pipeline이 사용될 수 있음
<br>param_grid : key + 리스트 값을 갖는 딕셔너리가 주어짐 (estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값 지정)
<br>scoring : 예측 성능을 측정할 평가 방법 지정, 보통은 사이킷런의 성능 평가 지표를 지정하는 문자열(ex:'accuracy')로 지정하나 별도의 성능 평가 지표 함수도 지정할 수 있음
<br>cv : 교차 검증을 위해 분할되는 학습/테스트 세트의 개수 지정
<br>refit : default=True이며 True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 estimator 객체를 해당 하이퍼 파라미터로 재학습

<br>
&lt;예제&gt;<br>
결정 트리 알고리즘의 여러 가지 최적화 파라미터를 순차적으로 적용해 붓꽃 데이터를 예측 분석하는 데 GridSearchCV 이용
<br>from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,
                                                    test_size=0.2, random_state=121)
dtree = DecisionTreeClassifier()

### parameter 들을 dictionary 형태로 설정
parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}
<br>
<br>train_test_split()을 이용해 학습 데이터와 테스트 데이터를 먼저 분리
<br>테스트할 하이퍼 파라미터들을 dictionary 형태로 설정
<br>import pandas as pd

# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  
### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과 추출하여 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score',
           'split0_test_score', 'split1_test_score', 'split2_test_score']]
<br>
<br>학습 데이터 세트를 GridSearchCV 객체의 fit() 메서드에 인자로 입력
<br>학습 데이터를 cv에 기술된 폴딩 세트로 분할해 param_grid에 기술된 하이퍼 파라미터를 순차적으로 변경하면서 학습/평가를 수행하고 그 결과를 cv_results_ 속성에 기록
<br>
&lt;결과&gt;<img alt="Pasted image 20250211213803.png" src="lib\media\pasted-image-20250211213803.png">

<br>params 컬럼에는 수행할 때마다 적용된 개별 하이퍼 파라미터값을 나타냄
<br>rank_test_score는 하이퍼 파라미터별로d 성능이 좋은 score 순위를 나타냄 (1이 가장 뛰어난 순위이며 이때의 파라미터가 최적의 하이퍼 파라미터)
<br>mean_test_score는 개별 하이퍼 파라미터별로 CV의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값

<br>print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))
<br><img alt="Pasted image 20250211214549.png" src="lib\media\pasted-image-20250211214549.png"><br>
<br>GridSearchCV 객체의 fit()을 수행하면 최고 성능을 나타낸 하이퍼 파라미터의 값과 그때의 평가 결과 값이 각각 best_params_, best_score_ 속성에 기록
<br># GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
estimator = grid_dtree.best_estimator_

# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))

&gt;&gt;&gt; 테스트 데이터 세트 정확도: 0.9667
<br>
<br>refit=True이면 GridSearchCV가 최적 성능을 나타내는 하이퍼 파라미터로 Estimator를 다시 학습해 best_estimator_로 저장

<br>refit=False인 경우 최적의 모델(best_estimator_)을 자동으로 다시 학습하지 않음!
<br>즉, best_estimator_ 속성이 존재하지 않으며, 오직 교차 검증 결과(cv_results_)만 제공됨


<br>이미 학습된 best_estimator_를 이용해 테스트 데이터 세트로 정확도를 측정한 결과 약 96.67%의 결과 도출
<br>Tip
학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신러닝 모델 적용 방법
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(4)-model-selection-모듈-소개.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(4) Model Selection 모듈 소개.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate><enclosure url="lib\media\pasted-image-20250210210414.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250210210414.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(5) 데이터 전처리]]></title><description><![CDATA[ 
 <br>데이터 전처리 (Data Preprocessing)
"Garbage In, Garbage Out"<br>
분석이나 머신러닝 모델링 전에 데이터를 정제하고 변형하여 최적의 상태로 만드는 과정
<br>
<br>머신러닝 모델링에서 결손값, 즉 NaN, Null 값은 허용되지 않음

<br>따라서 이러한 Null 값은 고정된 다른 값으로 변환해야 함!


<br>Null 값을 어떻게 처리해야 할지는 경우에 따라 다름

<br>feature 값 중 Null 값이 얼마 되지 않는다면 feature의 평균값 등으로 간단히 대체할 수 있음
<br>하지만, Null 값이 대부분이라면 오히려 해당 feature는 drop 하는 것이 더 좋음!


<br>가장 결정이 힘든 부분이 Null 값이 일정 수준 이상 되는 경우!

<br>정확히 몇 퍼센트까지를 일정 수준 이상이라고 한다는 기준은 없지만, 해당 feature가 중요도가 높은 feature이고 Null을 단순히 feature의 평균값으로 대체할 경우 예측 왜곡이 심할 수 있다면 업무 로직 등을 상세히 검토해 더 정밀한 대체 값을 선정해야 함


<br>사이킷런의 머신러닝 알고리즘은 문자열 값을 입력값으로 허용하지 않음

<br>모든 문자열 값은 인코딩돼서 숫자형으로 변환해야 함!
<br>문자열 feature는 일반적으로 카테고리형 feature와 텍스트형 feature를 의미


<br><br><br>데이터 인코딩 (Data Encoding)
머신러닝 모델이 이해할 수 있도록 범주형(category) 데이터를 숫자로 변환하는 과정<br>
머신러닝을 위한 대표적인 인코딩 방식은 레이블 인코딩(Label encoding)과 원-핫 인코딩(One-Hot encoding)
<br><br>레이블 인코딩 (Label Encoding)
각 범주(category)를 정수(0, 1, 2...)로 변환하는 방식

<br>장점 : 간단하게 문자열 값을 숫자형 카테고리 값으로 변환
<br>단점 : 숫자 값의 경우 크고 작음에 대한 특성이 작용하기 때문에 몇몇 ML 알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생할 수 있음

<br>숫자 변환 값은 단순 코드이지 숫자 값에 따른 순서나 중요도로 인식돼서는 안 됨
<br>이러한 특성 때문에 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용하지 않아야 함
<br>tree 계열의 ML 알고리즘은 적용 가능



<br>
<br>사이킷런의 레이블 인코딩은 LabelEncoder 클래스로 구현
<br>LabelEncoder를 객체로 생성한 후 fit()과 transform()을 호출해 레이블 인코딩 수행
<br>from sklearn.preprocessing import LabelEncoder

items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']

# LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행.
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
print('인코딩 변환값:',labels)

&gt;&gt;&gt; 인코딩 변환값: [0 1 4 5 3 3 2 2]
<br>
<br>위 예제는 데이터가 작아서 문자열 값이 어떤 숫자 값으로 인코딩됐는지 직관적으로 알 수 있지만, 많은 경우에 이를 알지 못함
<br>이 경우에는 LabelEncoder 객체의 classes_ 속성값으로 확인!<br>
print('인코딩 클래스:',encoder.classes_)<br>
인코딩 클래스: ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']

<br>classes_ 속성은 0번부터 순서대로 변환된 인코딩 값에 대한 원본값을 가지고 있음


<br>inverse_transform()을 통해 인코딩된 값을 다시 디코딩할 수 있음<br>
print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))<br>
디코딩 원본 값: ['전자레인지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기']
<br><br>원-핫 인코딩 (One-Hot Encoding)
feature 값의 유형에 따라 새로운 feature를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식

<br>즉, row 형태로 되어 있는 feature의 고유 값을 column 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 column에는 0을 표시

<br><img alt="Pasted image 20250212151043.png" src="lib\media\pasted-image-20250212151043.png"><br>
<br>원-핫 인코딩은 사이킷런에서 OneHotEncoder 클래스로 변환 가능
<br>단, LabelEncoder와 다르게 약간 주의할 점은 입력값으로 2차원 데이터가 필요하다는 것과, OneHotEncoder를 이용해 변환한 값이 희소 행렬(Sparse Matrix) 형태이므로 이를 다시 toarray() 메서드를 이용해 밀집 행렬(Dense Matrix)로 변환해야 한다는 것

<br>여기서의 밀집 행렬은 모든 값을 명시적으로 메모리에 저장하는 행렬을 의미


<br>from sklearn.preprocessing import OneHotEncoder
import numpy as np

items=['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서']

# 2차원 ndarray로 변환합니다.
items = np.array(items).reshape(-1, 1)

# 원-핫 인코딩을 적용합니다.
oh_encoder = OneHotEncoder()
oh_encoder.fit(items)
oh_labels = oh_encoder.transform(items)

# OneHotEncoder로 변환한 결과는 희소행렬이므로 toarray()를 이용해 밀집 행렬로 변환.
print('원-핫 인코딩 데이터')
print(oh_labels.toarray())
print('원-핫 인코딩 데이터 차원')
print(oh_labels.shape)
<br><img alt="Pasted image 20250212151601.png" src="lib\media\pasted-image-20250212151601.png"><br>
<br>
위 예제 코드의 변환 절차<br>
<img alt="Pasted image 20250212153523.png" src="lib\media\pasted-image-20250212153523.png">

<br>
판다스에는 원-핫 인코딩을 더 쉽게 지원하는 API 존재!

<br>pd.get_dummies(df)
<br>사이킷런의 OneHotEncoder와 다르게 문자열 카테고리 값을 숫자형으로 변환할 필요 없이 바로 변환 가능!


<br>import pandas as pd
from tabulate import tabulate

df = pd.DataFrame({'item':['TV','냉장고','전자레인지','컴퓨터','선풍기','선풍기','믹서','믹서'] })
print(tabulate(pd.get_dummies(df).astype(int), headers='keys', tablefmt='fancy_outline'))
<br><img alt="Pasted image 20250212154912.png" src="lib\media\pasted-image-20250212154912.png"><br><br><br>피처 스케일링 (feature scaling)
서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업

<br>머신러닝 알고리즘은 입력 값의 크기에 영향을 받을 수 있기 때문에, 특성이 서로 다른 단위나 범위를 가지면 모델 성능이 저하될 수 있음!
<br>대표적인 방법으로 표준화(Standardization)와 정규화(Normalization)

<br>표준화 (Standardization)
데이터의 feature 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것
<br><br>정규화 (Normalization)
서로 다른 feature 크기를 통일하기 위해 크기를 변환해주는 개념<br>
데이터의 값을 모두 최소 0 ~ 최대 1의 값으로 변환하는 것
<br><br>
<br>단, 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 존재!
<br>사이킷런의 Normalizer 모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미!
<br>즉, 개별 벡터를 모든 feature 벡터의 크기로 나눠줌


<br>Note
혼선을 방지하기 위해 일반적인 의미의 표준화와 정규화를 피처 스케일링으로 통칭하고 선형대수 개념의 정규화를 벡터 정규화로 지칭
<br><br><br>사이킷런에서 제공하는 대표적인 Feature Scaling 클래스
StandardScaler와 MinMaxScaler
<br>StandardScaler

<br>표준화를 쉽게 지원하기 위한 클래스
<br>즉, 개별 feature를 평균이 0이고, 분산이 1인 값으로 변환
<br>가우시안 정규 분포를 가질 수 있도록 데이터를 변환하는 것은 몇몇 알고리즘에서 매우 중요!

<br>SVM, 선형회귀, 로지스틱 회귀는 데이터가 가우시안 분포를 가지고 있다고 가정하고 구현됐기 때문에 사전에 표준화를 적용하는 것은 예측 성능 향상에 중요한 요소가 될 수 있음



<br>from sklearn.datasets import load_iris
import pandas as pd

# 붓꽃 데이터 셋을 로딩하고 DataFrame으로 변환합니다.
iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print('feature 들의 평균 값')
print(iris_df.mean())
print('\nfeature 들의 분산 값')
print(iris_df.var())
<br><img alt="0212171840210174.jpg" src="lib\media\0212171840210174.jpg"><br>
<br>이제 StandardScaler를 이용해 각 feature를 한 번에 표준화
<br>StandardScaler 객체를 생성한 후에 fit()과 transform() 메서드에 변환 대상 feature 데이터 세트를 입력하고 호출하면 간단하게 변환
<br>transform()을 호출할 때 스케일 변환된 데이터 세트는 넘파이의 ndarray이므로 이를 DataFrame으로 변환해 평균값과 분산 값을 다시 확인
<br>from sklearn.preprocessing import StandardScaler

# StandardScaler객체 생성
scaler = StandardScaler()

# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

print('feature 들의 평균 값')
print(iris_df_scaled.mean())
print('\nfeature 들의 분산 값')
print(iris_df_scaled.var())
<br><img alt="0212172238464823.jpg" src="lib\media\0212172238464823.jpg"><br>
<br>모든 column 값의 평균이 0에 아주 가까운 값으로, 분산은 1에 아주 가까운 값으로 변환됨
<br><br><br>MinMaxScaler

<br>정규화를 쉽게 지원하기 위한 클래스
<br>데이터값을 0과 1 사이의 범위 값으로 변환

<br>음수 값이 있으면 -1에서 1값으로 변환


<br>데이터의 분포가 가우시안 분포가 아닐 경우에 Min, Max Scale을 적용해 볼 수 있음

<br>from sklearn.preprocessing import MinMaxScaler

# MinMaxScaler객체 생성
scaler = MinMaxScaler()

# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)

# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

print('feature들의 최솟값')
print(iris_df_scaled.min())
print('\nfeature들의 최댓값')
print(iris_df_scaled.max())
<br><img alt="0212173412428698.jpg" src="lib\media\0212173412428698.jpg"><br>
<br>모든 feature에 0에서 1 사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있음
<br>왜 fit()과 transform()을 따로 실행할까?
훈련 데이터를 기준으로 변환 기준(scaling parameter)을 학습하고, 이를 나중에 테스트 데이터에도 적용하기 위해서!

<br>훈련 데이터에서 fit -&gt; 학습된 기준을 훈련 데이터와 테스트 데이터에 적용
<br>훈련 데이터에 대해서만 변환할 경우, fit_transform()을 사용할 순 있지만, 학습 데이터와 테스트 데이터가 분리되어 있을 때는 반드시 fit()과 transform()을 따로 사용해야 함!

<br><br><br>
<br>Scaler 객체를 이용해 데이터의 스케일링 변환 과정

<br>fit()은 데이터 변환을 위한 기준 정보 설정을 적용
<br>transform()은 이렇게 설정된 정보를 이용해 데이터 변환
<br>그리고 fit_transform()은 fit()과 transform()을 한 번에 적용하는 기능 수행


<br>fit()과 transform()을 적용할 때 주의할 점!
학습 데이터 세트로 fit()을 적용하면 테스트 데이터 세트로는 다시 fit()을 수행하지 않고 학습 데이터 세트로 fit()을 수행한 결과를 이용해 transform() 변환을 적용해야 함!<br>
즉, 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용
<br>
<br>모델을 학습할 때는 훈련 데이터에서 패턴을 학습한 후, 테스트 데이터에서 그 패턴이 잘 적용되는 지를 평가해야 함
<br>따라서 훈련 데이터에서 학습한 기준을 그대로 테스트 데이터에도 적용해야 일관성이 유지됨!
<br>
테스트 데이터에 fit()을 적용할 때 어떠한 문제 발생?
<br>from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 학습 데이터는 0 부터 10까지, 테스트 데이터는 0 부터 5까지 값을 가지는 데이터 세트로 생성
# Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경

train_array = np.arange(0, 11).reshape(-1, 1)
test_array =  np.arange(0, 6).reshape(-1, 1)
<br>
<br>학습 데이터를 0부터 10까지, 테스트 데이터를 0부터 5까지 값을 가지는 ndarray
<br># MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1 값으로 변환
scaler = MinMaxScaler()

# fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정.
scaler.fit(train_array)

# 1/10 scale로 train_array 데이터 변환함. 원본 10-&gt; 1로 변환됨.
train_scaled = scaler.transform(train_array)

print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

&gt;&gt;&gt; 원본 train_array 데이터: [ 0 1 2 3 4 5 6 7 8 9 10]
    Scale된 train_array 데이터: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]
<br>
<br>학습 데이터인 train_array부터 MinMaxScaler를 이용해 변환
<br># MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨
scaler.fit(test_array)

# 1/5 scale로 test_array 데이터 변환함. 원본 5-&gt;1로 변환.
test_scaled = scaler.transform(test_array)

# test_array의 scale 변환 출력.
print('원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))

&gt;&gt;&gt; 원본 test_array 데이터: [0 1 2 3 4 5]
    Scale된 test_array 데이터: [0. 0.2 0.4 0.6 0.8 1. ]
<br>
<br>테스트 데이터 세트 변환
<br>fit()을 호출해 스케일링 기준 정보를 다시 적용한 뒤 transform()을 수행한 결과 확인

<br>출력 결과를 확인하면 학습 데이터와 테스트 데이터의 스케일링이 맞지 않음!
<br>이렇게 되면 학습 데이터와 테스트 데이터의 서로 다른 원본값이 동일한 값으로 반환되는 결과 초래


<br>Tip
머신러닝 모델은 학습 데이터를 기반으로 학습되기 때문에 반드시 테스트 데이터는 학습 데이터의 스케일링 기준에 따라야 하며, 테스트 데이터의 1 값은 학습 데이터와 동일하게 0.1 값으로 변환되어야 함!

<br>따라서, 테스트 데이터에 다시 fit()을 적용해서는 안 되며 학습 데이터로 이미 fit()이 적용된 Scaler 객체를 이용해 transform()으로 변환해야 함

<br>
<br>다음 코드는 테스트 데이터에 fit()을 호출하지 않고 학습 데이터로 fit()을 수행한 MinMaxScaler 객체의 transform()을 이용해 데이터 변환
<br>scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)

print('원본 train_array 데이터:', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1), 2))

# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform() 만으로 변환해야 함.
test_scaled = scaler.transform(test_array)

print('\n원본 test_array 데이터:', np.round(test_array.reshape(-1), 2))
print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1), 2))
<br><img alt="Pasted image 20250212204025.png" src="lib\media\pasted-image-20250212204025.png"><br>
<br>fit_transform()은 fit()과 transform()을 순차적으로 수행하는 메서드이므로 학습 데이터에서는 상관없지만 테스트 데이터에서는 절대 사용해서는 안됨!
<br>Tip
학습과 테스트 데이터에 fit()과 transform()을 적용할 때 주의 사항이 발생하므로 학습과 테스트 데이터 세트로 분리하기 전에 먼저 전체 데이터 세트에 스케일링을 적용한 뒤 학습과 테스트 데이터 세트로 분리하는 것이 더 바람직!
<br>Summary
1. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리
2. 1이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 Scaler 객체를 이용해 transform()으로 변환
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(5)-데이터-전처리.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(5) 데이터 전처리.md</guid><pubDate>Thu, 13 Feb 2025 10:26:00 GMT</pubDate><enclosure url="lib\media\pasted-image-20250212151043.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250212151043.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(6) 사이킷런으로 수행하는 타이타닉 생존자 예측]]></title><description><![CDATA[ 
 <br>캐글 타이타닉 탑승자 데이터를 기반으로 생존자 예측

<br>Passengerid : 탑승자 데이터 일련번호
<br>survived : 생존 여부 (0:사망, 1:생존)
<br>pclass : 티켓의 선실 등급 (1:일등석, 2:이등석, 3:삼등석)
<br>sex : 탑승자 성별
<br>name : 탑승자 이름
<br>Age : 탑승자 나이
<br>sibsp : 같이 탑승한 형제자매 또는 배우자 인원수
<br>parch : 같이 탑승한 부모님 또는 어린이 인원수
<br>ticket : 티켓 번호
<br>fare : 요금
<br>cabin : 선실 번호
<br>embarked : 중간 정착 항구 (C:Cherbourg, Q:Queenstown, S:Southhampton)

<br><br><br>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

titanic_df = pd.read_csv('train.csv')
titanic_df.head(3)
<br><img alt="Pasted image 20250213151504.png" src="lib\media\pasted-image-20250213151504.png"><br><br>print('\n ### train 데이터 정보 ###  \n')
print(titanic_df.info())
<br><img alt="Pasted image 20250213151614.png" src="lib\media\pasted-image-20250213151614.png"><br>
<br>RangeIndex는 DataFrame 인덱스 범위를 나타내므로 전체 row 수를 알 수 있음

<br>현재 데이터는 891개 row로 구성되어 있음
<br>column 수는 12개
<br>2개의 column이 float64
<br>5개의 column이 int64
<br>5개의 column이 object (판다스의 object 타입은 string타입으로 봐도 무방)


<br>Age, Cabin, Embarked 칼럼은 각각 714개, 204개, 889개의 Not Null 값을 가지고 있으므로 각각 117개, 608개, 2개의 Null 값(NaN)을 가지고 있음
<br><br><br>Note
사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않으므로 Null 값을 어떻게 처리할지 결정해야 함
<br>
<br>여기서는 DataFrame의 fillna() 함수를 사용해 간단하게 Null 값을 평균 또는 고정 값으로 변경
<br>titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].mean())
titanic_df['Cabin'] = titanic_df['Cabin'].fillna('N')
titanic_df['Embarked'] = titanic_df['Embarked'].fillna('N')
print('데이터 세트 Null 값 갯수 ',titanic_df.isnull().sum().sum())

&gt;&gt;&gt; 데이터 세트 Null 값 갯수 0
<br>
<br>Cabin과 Embarded는 범주형 데이터로 결측치를 단순히 제거하거나 평균으로 대체할 수 없기 때문에 해당 값이 원래 없었다는 것을 명확히 하기 위하여 N으로 대체
<br><br><br>print(' Sex 값 분포 :\n',titanic_df['Sex'].value_counts())
print('\n Cabin 값 분포 :\n',titanic_df['Cabin'].value_counts())
print('\n Embarked 값 분포 :\n',titanic_df['Embarked'].value_counts())
<br><img alt="Pasted image 20250213153150.png" src="lib\media\pasted-image-20250213153150.png"><br>
<br>Sex, Embarked 값은 별 문제가 없으나, Cabin(선실)의 경우 N이 687건으로 가장 많은 것도 특이하지만, 속성 값이 제대로 정리가 되지 않은 것처럼 보임<br>
- C23 C25 C27과 같이 여러 Cabin이 한꺼번에 표기된 값이 4건이나 되고, Cabin의 경우 선실 번호 중 선실 등급을 나타내는 첫 번째 알파벳이 중요해 보임
Tip
왜냐하면 이 시절에는 지금보다도 부자와 가난한 사람에 대한 차별이 더 있던 시절이었기에 일등실에 투숙한 사람이 삼등실에 투숙한 사람보다 더 살아날 확률이 높았을 것!


<br>따라서 Cabin의 경우 앞 문자만 추출!
<br>titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
print(titanic_df['Cabin'].head(3))

&gt;&gt;&gt; 0  N
    1  C
    2  N
    Name: Cabin, dtype: object
<br><br><br><br>Tip
바다에서 사고가 날 경우 여성과 아이들 그리고 노약자가 제일 먼저 구조 대상!<br>
그리고 아마도 부자나 유명인이 다음 구조 대상이었을 것<br>
예상컨데 삼등실에 탄 많은 가난한 이는 생존하지 못할 확률이 높을 것
<br><br>titanic_df.groupby(['Sex','Survived'])['Survived'].count()
<br><img alt="Pasted image 20250213155100.png" src="lib\media\pasted-image-20250213155100.png"><br>
<br>Survived 칼럼은 label로서 결정 클래스 값
<br>탑승객은 남자가 577명, 여자가 314명으로 남자가 더 많았음
<br>여자는 314명 중 233명으로 약 74.2%가 생존했지만, 남자의 경우에는 577명 중 468명이 죽고 109명만 살아남아 약 18.8%가 생존
<br>그래프로 확인<br>
sns.barplot(x='Sex', y='Survived', data=titanic_df)
<br>기본적인 작동 방식

<br>x='Sex' : Sex 별로 그룹화 (male, female)
<br>y='Survived' : Survived 값의 평균 계산
<br>막대 높이 : 그룹별 Survived 평균<br>
- Survived 컬럼은 0 또는 1이므로, 평균값이 곧 생존 확률<br>
<img alt="Pasted image 20250213163711.png" src="lib\media\pasted-image-20250213163711.png">


<br><br>
<br>부를 측정할 수 있는 속성으로 적당한 것은 객실 등급
<br>객실 등급 별 성별에 따른 생존 확률 비교
<br>sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)<br>
<img alt="Pasted image 20250213164010.png" src="lib\media\pasted-image-20250213164010.png">

<br>hue 파라미터 : 데이터를 또 다른 기준으로 그룹화하여 색상별로 나누어 표현

<br>즉, 같은 x값에 대해 서브 그룹을 만들어서 여러 개의 막대를 나란히 표시하는 기능




<br>여성의 경우 일, 이등실에 따른 생존 확률의 차이는 크지 않으나, 삼등실의 경우 생존 확률이 상대적으로 많이 떨어짐
<br>남성의 경우 일등실의 생존 확률이 이, 삼등실의 생존 확률보다 월등히 높음
<br><br>Tip
Age의 경우 값 종류가 많기 때문에 범위별로 분류해 카테고리 값 할당

<br>0~5세 : Baby
<br>6~12세 : Child
<br>13~18세 : Teenager
<br>19~25세 : Student
<br>26~35세 : Yiung Adult
<br>36~60세 : Adult
<br>61세 이상 : Elderly
<br>-1 이하의 오류 값은 Unknown으로 분류

<br># 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame의 apply lambda식에 사용.
def get_category(age):
    cat = ''
    if age &lt;= -1: cat = 'Unknown'
    elif age &lt;= 5: cat = 'Baby'
    elif age &lt;= 12: cat = 'Child'
    elif age &lt;= 18: cat = 'Teenager'
    elif age &lt;= 25: cat = 'Student'
    elif age &lt;= 35: cat = 'Young Adult'
    elif age &lt;= 60: cat = 'Adult'
    else : cat = 'Elderly'
    return cat

# 막대그래프의 크기 figure를 더 크게 설정
plt.figure(figsize=(10,6))

# X축의 값을 순차적으로 표시하기 위한 설정
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']

# lambda 식에 위에서 생성한 get_category( ) 함수를 반환값으로 지정.
# get_category(X)는 입력값으로 'Age' 컬럼값을 받아서 해당하는 cat 반환
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
sns.barplot(x='Age_cat', y = 'Survived', hue='Sex', data=titanic_df, order=group_names)
titanic_df.drop('Age_cat', axis=1, inplace=True)
<br><img alt="Pasted image 20250213165046.png" src="lib\media\pasted-image-20250213165046.png"><br>
<br>여자 Baby의 경우 비교적 생존 확률이 높았지만, 여자 Child의 경우 다른 연령대에 비해 생존 확률이 낮음
<br>여자 Elderly의 경우는 매우 생존 확률이 높았음
<br>Summary
이제까지  분석한 결과 Sex, Age, PClass 등이 중요하게 생존을 좌우하는 feature임을 어느 정도 확인 가능
<br><br><br>Note

<br>인코딩은 사이킷런의 LabelEncoder 클래스를 이요해 레이블 인코딩 적용

<br>레이블 인코딩은 각 범주를 정수형으로 변환하는 방식


<br>LabelEncoder 객체는 카테고리 값의 유형 수에 따라 0 ~ (카테고리 유형 수 -1)까지의 숫자 값으로 변환
<br>사이킷런의 전처리 모듈의 대부분 인코딩 API는 사이킷런의 기본 프레임워크 API인 fit(), transform()으로 데이터 변환

<br>
<br>여러 칼럼을 encode_features() 함수를 새로 생성해 한 번에 변환
<br>from sklearn.preprocessing import LabelEncoder

def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
    return dataDF

titanic_df = encode_features(titanic_df)
titanic_df.head()
<br><img alt="Pasted image 20250213171432.png" src="lib\media\pasted-image-20250213171432.png"><br>
<br>Sex, Cabin, Embarked 속성이 숫자형으로 바뀜
<br><br><br>Note
지금까지 feature를 가공한 내역을 정리하고 이를 함수로 만들어 쉽게 재사용할 수 있도록 구성<br>
transform_features() 

<br>Null 처리, 불필요한 feature 제거, 인코딩을 수행하는 내부 함수로 구성
<br>불필요한 feature 제거는 drop_features(df)로 수행하며 머신러닝 알고리즘에 불필요한, 단순한 식별자 수준의 feature인 PassengerId, Name, Ticket feature 제거

<br>from sklearn.preprocessing import LabelEncoder

# Null 처리 함수
def fillna(df):
    df['Age'] = df['Age'].fillna(df['Age'].mean())
    df['Cabin'] = df['Cabin'].fillna('N')
    df['Embarked'] = df['Embarked'].fillna('N')
    df['Fare'] = df['Fare'].fillna(0)
    return df

# 머신러닝 알고리즘에 불필요한 피처 제거
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

# 레이블 인코딩 수행.
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 앞에서 설정한 데이터 전처리 함수 호출
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
<br><br><br>Todo

<br>원본 CSV 파일을 다시 로딩하고 label 값인 Survived 속성만 별도 분리해 클래스 결정값 데이터 세트로 만들기
<br>그리고 Survived 속성을 drop 해 feature 데이터 세트 만들기
<br>이렇게 생성된 feature 데이터 세트에 transform_features()를 적용해 데이터 가공

<br># 원본 데이터를 재로딩 하고, feature데이터 셋과 Label 데이터 셋 추출.
titanic_df = pd.read_csv('train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived',axis=1)

X_titanic_df = transform_features(X_titanic_df)
<br>
<br>내려받은 학습 데이터 세트를 기반으로 train_test_split()를 이용해 별도의 테스트 데이터 세트 추출
<br>테스트 데이터 세트 크기는 전체의 20%
<br>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df,
                                                  test_size=0.2, random_state=11)
<br><br><br>Note
ML 알고리즘인 결정 트리, 랜덤 포레스트, 로지스틱 회귀를 이용해 타이타닉 생존자 예측<br>
여기서는 사이킷런 기반의 머신러닝 코드에 익숙해지는 것을 목표!

<br>결정 트리 : DecisionTreeClassifier
<br>랜덤 포레스트 : RandomForestClassifier
<br>로지스틱 회귀 : LogisticRegression

핵심 과정

<br>사이킷런 클래스를 이용해 train_test_split()으로 분리한 학습 데이터와 테스트 데이터를 기반으로 머신러닝 모델을 학습하고(fit), 에측(predict)
<br>예측 성능 평가는 정확도로 할 것이며 이를 위해 accuracy_score() API 사용

<br>from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression(solver='liblinear')

# DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train , y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

# RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train , y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))

# LogisticRegression 학습/예측/평가
lr_clf.fit(X_train , y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))
<br><img alt="Pasted image 20250213175649.png" src="lib\media\pasted-image-20250213175649.png"><br>
<br>3개의 알고리즘 중 LogisticRegression이 타 알고리즘에 비해 높은 정확도를 나타내고 있음
<br>하지만 아직 최적화 작업을 수행하지 않았고, 데이터 양도 충분하지 않기 때문에 어떤 알고리즘이 가장 성능이 좋다고 평가할 수는 없음
<br><br>Note
사이킷런 model_selection 패키지의 KFold 클래스, cross_val_score(), GridSearchCV 클래스를 모두 사용!
<br><br>
<br>fold 개수는 5개로 설정
<br>from sklearn.model_selection import KFold

def exec_kfold(clf, folds=5):
    # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한  리스트 객체 생성.
    kfold = KFold(n_splits=folds)
    scores = []

    # KFold 교차 검증 수행.
    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성
        X_train, X_test = X_titanic_df.values[train_index],          
                          X_titanic_df.values[test_index]

        y_train, y_test = y_titanic_df.values[train_index],  
			              y_titanic_df.values[test_index]

        # Classifier 학습, 예측, 정확도 계산
        clf.fit(X_train, y_train)
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        scores.append(accuracy)
        print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))    

    # 5개 fold에서의 평균 정확도 계산.
    mean_score = np.mean(scores)
    print("평균 정확도: {0:.4f}".format(mean_score))

# exec_kfold 호출
exec_kfold(dt_clf , folds=5)
<br>“Pasted image 20250214163848.png” 을 찾지 못했습니다.<br>enumerate()
반복(iterable) 객체를 순회할 때, 현재 몇 번째 반복인지(index)를 함께 반환하는 내장 함수
<br>
<br>평균 정확도는 약 78.23%
<br><br>from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)

for iter_count, accuracy in enumerate(scores):
	print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))

print("평균 정확도: {0:.4f}".format(np.mean(scores)))
<br>“Pasted image 20250214165844.png” 을 찾지 못했습니다.<br>cross_val_score()와 방금 전 k fold의 평균 정확도는 왜 다를까?
cross_val_score()가 StratifiedKFold를 이용해 fold 세트를 분할하기 때문!
<br><br>from sklearn.model_selection import GridSearchCV

parameters = {'max_depth':[2,3,5,10],
             'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}

grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_dclf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))
best_dclf = grid_dclf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행.
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy))
<br>“Pasted image 20250214165142.png” 을 찾지 못했습니다.<br>
<br>최적화된 하이퍼 파라미터인 max_depth=3, min_samples_leaf=5, min_samples_split=2로 DecisionTreeClassifier를 학습시킨 뒤 예측 정확도가 약 87.15%로 향상됨
<br>하지만, 일반적으로 하이퍼 파라미터를 튜닝하더라도 이 정도 수준으로 증가하기는 매우 어려움
<br>테스트용 데이터 세트가 작기 때문에 수치상으로 예측 성능이 많이 증가한 것처럼 보임
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(6)-사이킷런으로-수행하는-타이타닉-생존자-예측.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(6) 사이킷런으로 수행하는 타이타닉 생존자 예측.md</guid><pubDate>Fri, 14 Feb 2025 09:40:45 GMT</pubDate><enclosure url="lib\media\pasted-image-20250213151504.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250213151504.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(7) 정리]]></title><description><![CDATA[ 
 <br>
<br>
사이킷런은 매우 많은 머신러닝 알고리즘을 제공할 뿐만 아니라, 쉽고 직관적인 API 프레임워크, 편리하고 다양한 모듈 지원 등으로 파이썬 계열의 대표적인 머신러닝 패키지

<br>
머신러닝 애플리케이션은 데이터의 가공 및 변환 과정의 전처리 작업, 데이터를 학습 데이터와 테스트 데이터로 분리하는 데이터 세트 분리 작업을 거친 후에 학습 데이터를 기반으로 머신러닝 알고리즘을 적용해 모델을 학습

<br>그리고 학습된 모델을 기반으로 테스트 데이터에 대한 예측을 수행하고, 이렇게 예측된 결괏값을 실제 결괏값과 비교해 머신러닝 모델에 대한 평가를 수행하는 방식으로 구성


<br>
데이터 전처리는 오류 데이터의 보정이나 결손값(Null) 처리 등의 다양한 데이터 클렌징 작업, 레이블 인코딩이나 원-핫 인코딩과 같은 인코딩 작업, 그리고 데이터의 스케일링/정규화 작업 등으로 머신러닝 알고리즘이 최적으로 수행될 수 있게 데이터를 사전 처리하는 것

<br>
머신러닝 모델은 학습 데이터 세트로 학습한 뒤 반드시 별도의 테스트 데이터 세트로 평가되어야 함

<br>또한 테스트 데이터의 건수 부족이나 고정된 테스트 데이터 세트를 이용한 반복적인 모델으 학습과 평가는 해당 테스트 데이터 세트에만 치우친 빈약한 머신러닝 모델을 만들 가능성이 높음
<br>이를 해결하기 위해 학습 데이터 세트를 학습 데이터와 검증 데이터로 구성된 여러 개의 fold 세트로 분리해 교차 검증을 수행할 수 있음
<br>사이킷런은 이러한 교차 검증을 지원하기 위해 KFold, StratifiedKFold, cross_val_score()등의 다양한 클래스와 함수를 제공
<br>머신러닝 모델의 최적의 하이퍼 파라미터를 교차 검증을 통해 추출하기 위해 GridSearchCV를 제공


<br>
다음 장에서는 본격적으로 머신러닝 지도학습의 대표적인 한 축인 분류(Classification)를 학습하기 전에 먼저 분류의 예측 성능을 평가하는 다양한 방법을 확인
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\(7)-정리.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/(7) 정리.md</guid><pubDate>Fri, 14 Feb 2025 09:41:26 GMT</pubDate></item><item><title><![CDATA[02. 사이킷런으로 시작하는 머신러닝]]></title><description><![CDATA[ 
 사이킷런을 활용한 머신러닝 기초를 배웁니다.]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\02.-사이킷런으로-시작하는-머신러닝\index.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/02. 사이킷런으로 시작하는 머신러닝/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[(1) 정확도 (Accuracy)]]></title><description><![CDATA[ 
 <br>Note
머신러닝은 데이터 가공/변환, 모델 학습/예측, 그리고 평가(Evaluation)의 프로세스로 구성
머신러닝 모델은 여러 가지 방법으로 예측 성능을 평가할 수 있음
성능 평가 지표(Evaluation Metric)는 일반적으로 모델이 분류냐 회귀냐에 따라 여러 종류로 나뉨
회귀의 경우 대부분 실제 값과 예측값의 오차 평균값에 기반

<br>예를 들어 오차에 절댓값을 씌운 뒤 평균 오차를 구하거나 오차의 제곱 값에 루트를 씌운 뒤 평균 오차를 구하는 방법 등
<br>기본적으로 예측 오차를 가지고 정규화 수준을 재가공 하는 방법이 휘귀의 성능 평가 지표 유형

분류의 평가 방법도 일반적으로는 실제 결과 데이터와 예측 결과 데이터가 얼마나 정확하고 오류가 적게 발생하는 가에 기반하지만, 단순히 이러한 정확도만 가지고 판단했다가는 잘못된 평가 결과에 빠질 수 있음
본 장에서는 분류에 사용되는 성능 평가 지표에 대해서 집중적으로 설명함

<br>특히 0과 1로 결정값이 한정되는 이진 분류의 성능 평가 지표에 대해서 집중적으로 설명!

<br>*분류의 성능 평가 지표**

<br>정확도 (Accuracy)
<br>오차행렬 (Confusion Matrix)
<br>정밀도 (Precision)
<br>재현율 (Recall)
<br>F1 스코어
<br>ROC곡선과 AUC

<br>
<br>분류는 결정 클래스 값 종류의 유형에 따라 긍정/부정과 같은 2개의 결괏값만을 가지는 이진 분류와 여러 개의 결정 클래스 값을 가지는 멀티 분류로 나뉠 수 있음

<br>결정 클래스는 "가능한 정답의 집합"이고 label은 "각 데이터의 실제 정답"


<br>위에서 언급한 분류의 성능 지표는 이진/멀티 분류 모두에 적용되는 지표이지만, 특히 이진 분류에서 더욱 중요하게 강조하는 지표!
<br><br>
왜 위 지표가 모두 이진 분류에서 중요할까?
<br>정확도 (Accuracy)
실제 데이터에서 예측 데이터가 얼마나 같은지를 판단하는 지표<br>
즉, 예측 결과가 동일한 데이터 건수를 전체 예측 데이터 건수로 나눈 값
<br>
<br>정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표
<br>하지만 이진 분류의 경우 데이터의 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하지 않음
<br>
그렇다면 정확도 지표가 어떻게 ML 모델의 성능을 왜곡할까?
<br>
<br>앞의 타이타닉 예제 수행 결과를 보면 한 가지 의구심이 생길 수 있음
<br>ML 알고리즘을 적용한 후 예측 정확도의 결과가 보통 80%대였지만, 탑승객이 남자인 경우보다 여자인 경우에 생존 확률이 높았기 때문에 별다른 알고리즘의 적용 없이 무조건 성별이 여자인 경우 생존으로, 남자인 경우 사망으로 예측 결과를 예측해도 이와 비슷한 수치가 나올 수 있음

<br>단지 성별 조건 하나만을 가지고 결정하는 별거 아닌 알고리즘도 높은 정확도를 나타내는 상황 발생!


<br>
다음 예제에서는 사이킷런의 BaseEstimator 클래스를 상속받아 아무런 학습을 하지 않고, 성별에 따라 생존자를 예측하는 단순한 Classifier 생성

<br>사이킷런은 BaseEstimator를 상속받으면 Customized 형태의 Estimator를 개발자가 생성 가능
<br>여기서 fit()메서드는 아무것도 수행 X, predict() 메서드는 단순히 Sex feature가 1이면 0, 그렇지 않으면 1로 예측

<br>import numpy as np
from sklearn.base import BaseEstimator

class MyDummyClassifier(BaseEstimator):
    # fit( ) 메소드는 아무것도 학습하지 않음.
    def fit(self, X , y=None):
        pass

    # predict( ) 메소드는 단순히 Sex feature가 1 이면 0 , 그렇지 않으면 1 로 예측함.
    def predict(self, X):
        pred = np.zeros((X.shape[0], 1))
        for i in range (X.shape[0]) :
            if X['Sex'].iloc[i] == 1:
                pred[i] = 0
            else :
                pred[i] = 1

        return pred
<br>
MyDummyClassifier를 이용해 앞 장의 타이타닉 생존자 예측 수행
<br>
<br>앞에서 설정한 데이터 전처리 함수
<br>from sklearn.preprocessing import LabelEncoder

# Null 처리 함수
def fillna(df):
    df['Age'] = df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'] = df['Cabin'].fillna('N', inplace=True)
    df['Embarked'] = df['Embarked'].fillna('N', inplace=True)
    df['Fare'] = df['Fare'].fillna(0, inplace=True)
    return df

# 머신러닝 알고리즘에 불필요한 피처 제거
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

# 레이블 인코딩 수행.
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 앞에서 설정한 데이터 전처리 함수 호출
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
<br>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할.
titanic_df = pd.read_csv('train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df= titanic_df.drop('Survived', axis=1)
X_titanic_df = transform_features(X_titanic_df)
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df,
                                                  test_size=0.2, random_state=0)

# 위에서 생성한 Dummy Classifier를 이용해 학습/예측/평가 수행.
myclf = MyDummyClassifier()
myclf.fit(X_train, y_train)

mypredictions = myclf.predict(X_test)
print('Dummy Classifier의 정확도는: {0:.4f}'.format(accuracy_score(y_test, mypredictions)))

&gt;&gt;&gt; Dummy Classifier의 정확도는: 0.7877
<br>
<br>이렇게 단순한 알고리즘으로 예측을 하더라도 데이터의 구성에 따라 정확도 결과는 약 78.77%로 꽤 높은 수치가 나올 수 있기에 정확도를 평가 지표로 사용할 때는 매우 신중해야 함!
<br>특히 정확도는 불균형한(imbalanced) label 값 분포에서 ML 모델의 성능을 판단할 경우, 적합한 평가 지표가 아님

<br>예를 들어 100개의 데이터가 있고 이 중에 90개의 데이터 label이 0, 단 10개의 데이터 label이 1이라고 한다면 무조건 0으로 예측 결과를 반환하는 ML 모델의 경우라도 정확도가 90%나 됨


<br>
MNIST 데이터 세트를 변환해 불균형한 데이터 세트로 만든 뒤에 정확도 지표 적용시 문제점 확인
<br>
<br>MNIST 데이터 세트트 0부터 9까지의 숫자 이미지의 픽셀 정보를 가지고 있으며, 이를 기반으로 숫자 Digit를 예측하는 데 사용
<br>사이킷런은 load_digits() API를 통해 MNIST 데이터 세트 제공
<br>원래 MNIST 데이터 세트는 label 값이 0부터 9까지 있는 멀티 label 분류를 위한 것이지만, 이것을 label=7인 것만 True, 나머지는 모두 False로 변환해 이진 분류 문제로 변경<br>
<img alt="Pasted image 20250214205614.png" src="lib\media\pasted-image-20250214205614.png">
<br>Attention
아무것도 하지 않고 무조건 특정한 결과로 예측 결과를 반환해도 데이터 분포도가 균일하지 않은 경우 높은 수치가 나타날 수 있는 것이 정확도 평가 지표의 맹점!
<br>
<br>불균형한 데이터 세트와 Dummy Classifier 생성
<br>from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

class MyFakeClassifier(BaseEstimator):
    def fit(self,X,y):
        pass

    # 입력값으로 들어오는 X 데이터 셋의 크기만큼 모두 0값으로 만들어서 반환
    def predict(self,X):
        return np.zeros( (len(X), 1) , dtype=bool)

# 사이킷런의 내장 데이터 셋인 load_digits( )를 이용하여 MNIST 데이터 로딩
digits = load_digits()

print(digits.data)
print("### digits.data.shape:", digits.data.shape)
print(digits.target)
print("### digits.target.shape:", digits.target.shape)
]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\03.-평가\(1)-정확도-(accuracy).html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/03. 평가/(1) 정확도 (Accuracy).md</guid><pubDate>Fri, 14 Feb 2025 11:59:27 GMT</pubDate><enclosure url="lib\media\pasted-image-20250214205614.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="lib\media\pasted-image-20250214205614.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[03. 평가]]></title><description><![CDATA[ 
 분류의 예측 성능을 평가하는 다양한 방법을 다룬다.]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\03.-평가\index.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/03. 평가/index.md</guid><pubDate>Fri, 14 Feb 2025 09:43:44 GMT</pubDate></item><item><title><![CDATA[📘 파이썬 머신러닝 완벽 가이드]]></title><description><![CDATA[ 
 이 문서는 머신러닝을 파이썬 기반으로 배우기 위한 가이드입니다.]]></description><link>content\📘-파이썬-머신러닝-완벽-가이드\index.html</link><guid isPermaLink="false">content/📘 파이썬 머신러닝 완벽 가이드/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item><item><title><![CDATA[홈]]></title><description><![CDATA[ 
 데이터 사이언스와 머신러닝을 탐구하는 블로그]]></description><link>content\index.html</link><guid isPermaLink="false">content/index.md</guid><pubDate>Fri, 14 Feb 2025 09:36:15 GMT</pubDate></item></channel></rss>